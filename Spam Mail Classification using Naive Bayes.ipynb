{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# Do not change the random seed, or else the tests will fail!\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "For this lab, we'll be working with the Spambase Dataset from UC Irvine's Machine Learning Repository.\n",
    "\n",
    "This dataset contains emails that have already been vectorized, as well as summary statistics about each email that can also be useful in classification. In this case, the Data Dictionary containing the names and descriptions of each column is stored in a separate file from the dataset itself. For ease of use, we have included the spambase.csv file in this repo. However, we have not included the Data Dictionary and column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spambase.csv', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding in column names\n",
    "\n",
    "column_names = ['word_freq_make',\n",
    "    'word_freq_address',\n",
    "    'word_freq_all',     \n",
    "    'word_freq_3d',           \n",
    "    'word_freq_our',          \n",
    "    'word_freq_over',         \n",
    "    'word_freq_remove',       \n",
    "    'word_freq_internet',     \n",
    "    'word_freq_order',        \n",
    "    'word_freq_mail',         \n",
    "    'word_freq_receive',      \n",
    "    'word_freq_will',         \n",
    "    'word_freq_people',       \n",
    "    'word_freq_report',       \n",
    "    'word_freq_addresses',    \n",
    "    'word_freq_free',         \n",
    "    'word_freq_business',     \n",
    "    'word_freq_email',        \n",
    "    'word_freq_you',          \n",
    "    'word_freq_credit',       \n",
    "    'word_freq_your',         \n",
    "    'word_freq_font',         \n",
    "    'word_freq_000',          \n",
    "    'word_freq_money',        \n",
    "    'word_freq_hp',           \n",
    "    'word_freq_hpl',          \n",
    "    'word_freq_george',       \n",
    "    'word_freq_650',          \n",
    "    'word_freq_lab',          \n",
    "    'word_freq_labs',         \n",
    "    'word_freq_telnet',       \n",
    "    'word_freq_857',          \n",
    "    'word_freq_data',         \n",
    "    'word_freq_415',          \n",
    "    'word_freq_85',           \n",
    "    'word_freq_technology',   \n",
    "    'word_freq_1999',         \n",
    "    'word_freq_parts',        \n",
    "    'word_freq_pm',           \n",
    "    'word_freq_direct',       \n",
    "    'word_freq_cs',           \n",
    "    'word_freq_meeting',      \n",
    "    'word_freq_original',     \n",
    "    'word_freq_project',      \n",
    "    'word_freq_re',           \n",
    "    'word_freq_edu',          \n",
    "    'word_freq_table',        \n",
    "    'word_freq_conference',   \n",
    "    'char_freq_;',            \n",
    "    'char_freq_(',            \n",
    "    'char_freq_[',            \n",
    "    'char_freq_!',            \n",
    "    'char_freq_$',            \n",
    "    'char_freq_#',            \n",
    "    'capital_run_length_average', \n",
    "    'capital_run_length_longest', \n",
    "    'capital_run_length_total',\n",
    "     'is_spam'              \n",
    "    ]   \n",
    "\n",
    "df.columns = column_names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Null Values\n",
    "df.isna().sum().unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get basic statistical measures\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking target information\n",
    "df['is_spam'].sum() / len(df['is_spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Heatmap\n",
    "\n",
    "sns.set(style='white')\n",
    "\n",
    "corr = df.corr()\n",
    "\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "f, ax = plt.subplots(figsize=(22,18))\n",
    "\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.5, center=0, \n",
    "            square=True, linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create train and test data\n",
    "target = df['is_spam']\n",
    "clean_df = df.drop('is_spam', axis=1, inplace=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_df, target, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "GaussianNB(priors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "preds = clf.predict(X_test)\n",
    "#check accuracy score\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "#check f1 score\n",
    "f1 = f1_score(y_test, preds)\n",
    "print(\"Accuracy Score for model: {:.4}%\".format(accuracy * 100))\n",
    "print(\"F1 Score for model: {:.4}%\".format(f1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix to interpret results\n",
    "def confusion_matrix(predictions, labels):\n",
    "    labels = list(labels)\n",
    "    cm = {'TP': 0, 'TN': 0, 'FP':0, 'FN':0}\n",
    "    for i in range(len(predictions)):\n",
    "        pred = predictions[i]\n",
    "        label = labels[i]\n",
    "        if pred == label:\n",
    "            if pred == 1:\n",
    "                cm['TP'] += 1\n",
    "            else:\n",
    "                cm['TN'] += 1\n",
    "        else:\n",
    "            if pred == 1:\n",
    "                cm['FP'] += 1\n",
    "            else:\n",
    "                cm['FN'] += 1\n",
    "    \n",
    "    return cm\n",
    "\n",
    "training_preds = clf.predict(X_train)\n",
    "training_cm = confusion_matrix(training_preds, y_train)\n",
    "testing_cm = confusion_matrix(preds, y_test)\n",
    "\n",
    "print(\"Training Confusion Matrix: {}\".format(training_cm))\n",
    "print(\"Testing Confusion Matrix: {}\".format(testing_cm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
