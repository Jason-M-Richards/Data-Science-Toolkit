{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science with Azure ML Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logging into ML service resource and creating a Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Run\n",
    "\n",
    "ws = Workspace.get(name='junior_ds',\n",
    "                      subscription_id='idontthinkso', \n",
    "                      resource_group='Jason' \n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt will appear - To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code FXKD2FUZQ to authenticate. - copy the code, go to link and paste code where prompted. When connected - will prompt - Interactive authentication successfully completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to create a workspace from scratch\n",
    "**only run if workspace not created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(azureml.core.VERSION)\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.create(name='myworkspace',\n",
    "                      subscription_id='<azure-subscription-id>', \n",
    "                      resource_group='myresourcegroup',\n",
    "                      create_resource_group=True,\n",
    "                      location='eastus2' # Or other supported Azure region   \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get workspace details\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Details of your workspace need to be saved to a configuration Json file to the current directory.\n",
    "# Create the configuration file.\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging information to your workspace\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# create an experiment\n",
    "exp = Experiment(workspace=ws, name='trial_exp')\n",
    "\n",
    "# start a run\n",
    "run = exp.start_logging()\n",
    "\n",
    "# log a number\n",
    "run.log('trial', 30)\n",
    "\n",
    "# log a list (Fibonacci numbers)\n",
    "run.log_list('my list', [1, 1, 2, 3, 5, 8, 13, 21, 34, 55]) \n",
    "\n",
    "# finish the run\n",
    "run.complete()\n",
    "\n",
    "print('Execution complete')\n",
    "\n",
    "#print results\n",
    "print(run.get_portal_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete resources\n",
    "ws.delete(delete_dependent_resources=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "#create a fold for the dataset\n",
    "os.makedirs('./data', exist_ok = True)\n",
    "# load dataset to the directory, as you can see, you need to load train sets and test sets seperately \n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', filename='./data/train-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', filename='./data/train-labels.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename='./data/test-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename='./data/test-labels.gz')\n",
    "\n",
    "print('Code executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions needed by downstream code...\n",
    "\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "# load compressed MNIST gz files and return numpy arrays\n",
    "def load_data(filename, label=False):\n",
    "    with gzip.open(filename) as gz:\n",
    "        struct.unpack('I', gz.read(4))\n",
    "        n_items = struct.unpack('>I', gz.read(4))\n",
    "        if not label:\n",
    "            n_rows = struct.unpack('>I', gz.read(4))[0]\n",
    "            n_cols = struct.unpack('>I', gz.read(4))[0]\n",
    "            res = np.frombuffer(gz.read(n_items[0] * n_rows * n_cols), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], n_rows * n_cols)\n",
    "        else:\n",
    "            res = np.frombuffer(gz.read(n_items[0]), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], 1)\n",
    "    return res\n",
    "\n",
    "\n",
    "# one-hot encode a 1-D array\n",
    "def one_hot_encode(array, num_of_classes):\n",
    "    return np.eye(num_of_classes)[array.reshape(-1)]\n",
    "\n",
    "# load the data...\n",
    "\n",
    "# To help the model to converge faster , you shrink the intensity values (X) from 0-255 to 0-1\n",
    "X_train = load_data('./data/train-images.gz', False) / 255.0\n",
    "y_train = load_data('./data/train-labels.gz', True).reshape(-1)\n",
    "\n",
    "X_test = load_data('./data/test-images.gz', False) / 255.0\n",
    "y_test = load_data('./data/test-labels.gz', True).reshape(-1)\n",
    "\n",
    "print('Code executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# now let's show some randomly chosen images from the training set.\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running model from local machine\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#load model\n",
    "clf = LogisticRegression()\n",
    "#fit model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#evaluate model using test set\n",
    "y_hat = clf.predict(X_test)\n",
    "#print the accuracy\n",
    "print(np.average(y_hat == y_test))\n",
    "\n",
    "print('Code executed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that, depending on your local machine capacity, it takes a long time to fit models with parameters of different values, or to find k that returns best results. In this case, you should try to run the model on a remote cluster.\n",
    "\n",
    "RUN AND EXPERIMENT OVERVIEW\n",
    "\n",
    "Before you begin, it is imperative to familiarize ourselves with two concepts: run and experiment.\n",
    "\n",
    "Run, within the context of the Azure Machine Learning service, refers to Python code for a specific task, for example, training a model or tuning hyperparameters (we'll define this in a moment). Run does the job of logging metrics and upload result to Azure platform, it's a more natural way to keep track of jobs in your Workspace.\n",
    "\n",
    "Experiment is a term referring to a composition of a series of runs. In the example, you have one run for the logistic regression model and another for the KNN model, and together they make up an experiment for you to compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experiment\n",
    "# ensure that first you are connected to a workspace \n",
    "# Add this to top of code...\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import azureml\n",
    "from azureml.core import Workspace, Run\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an experiment\n",
    "experiment = Experiment(workspace = ws, name = \"my-first-experiment\")\n",
    "#Create a run\n",
    "run = experiment.start_logging()\n",
    "run.log(\"trial\",1)\n",
    "run.complete()\n",
    "print('Code executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view logged results - go to link provided\n",
    "print(run.get_portal_url())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AML Compute as our compute resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# Step 1: name the cluster, set minimal and maximal number of nodes \n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\n",
    "min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 3)\n",
    "\n",
    "# Step 2: choose environment variables \n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = min_nodes, \n",
    "                                                                max_nodes = max_nodes)\n",
    "\n",
    "# create the cluster\n",
    "compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "print('Code executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload data using get_default_datastore()\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "ds.upload(src_dir='./data', target_path='mnist', overwrite=True, show_progress=True)\n",
    "print('Code executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folder\n",
    "folder_training_script= './trial_model_mnist'\n",
    "os.makedirs(folder_training_script, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $folder_training_script/train.py\n",
    "\n",
    "# functions needed by downstream code...\n",
    "\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "# load compressed MNIST gz files and return numpy arrays\n",
    "def load_data(filename, label=False):\n",
    "    with gzip.open(filename) as gz:\n",
    "        struct.unpack('I', gz.read(4))\n",
    "        n_items = struct.unpack('>I', gz.read(4))\n",
    "        if not label:\n",
    "            n_rows = struct.unpack('>I', gz.read(4))[0]\n",
    "            n_cols = struct.unpack('>I', gz.read(4))[0]\n",
    "            res = np.frombuffer(gz.read(n_items[0] * n_rows * n_cols), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], n_rows * n_cols)\n",
    "        else:\n",
    "            res = np.frombuffer(gz.read(n_items[0]), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], 1)\n",
    "    return res\n",
    "\n",
    "\n",
    "# one-hot encode a 1-D array\n",
    "def one_hot_encode(array, num_of_classes):\n",
    "    return np.eye(num_of_classes)[array.reshape(-1)]\n",
    "#\n",
    "\n",
    "# end of functions needed by downstream code...\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from azureml.core import Run\n",
    "# from utils import load_data\n",
    "\n",
    "# let user feed in 2 parameters, the location of the data files (from datastore), and the regularization rate of the logistic regression model\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = os.path.join(args.data_folder, 'mnist')\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "# load train and test set into numpy arrays\n",
    "# note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\n",
    "X_train = load_data(os.path.join(data_folder, 'train-images.gz'), False) / 255.0\n",
    "X_test = load_data(os.path.join(data_folder, 'test-images.gz'), False) / 255.0\n",
    "y_train = load_data(os.path.join(data_folder, 'train-labels.gz'), True).reshape(-1)\n",
    "y_test = load_data(os.path.join(data_folder, 'test-labels.gz'), True).reshape(-1)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep = '\\n')\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "print('Train a logistic regression model with regularizaion rate of', args.reg)\n",
    "clf = LogisticRegression(C=1.0/args.reg, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Predict the test set')\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# calculate accuracy on the prediction\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy is', acc)\n",
    "\n",
    "run.log('regularization rate', np.float(args.reg))\n",
    "run.log('accuracy', np.float(acc))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')\n",
    "\n",
    "print('Code executed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you must add a utils script as shown below for loading data and to create an estimator so that it's easier to scale our work in the future. An estimator object is used to submit the run. Create your estimator by running the following code to define these items:\n",
    "\n",
    "- The name of the estimator object, est.\n",
    "- The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution.\n",
    "- The compute target. In this case, you use the Azure Machine Learning compute cluster you created.\n",
    "- The training script name, train.py.\n",
    "- Parameters required from the training script.\n",
    "- Python packages needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "    '--regularization': 0.8\n",
    "}\n",
    "\n",
    "#import scikit-learn package \n",
    "est = Estimator(source_directory=folder_training_script,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                conda_packages=['scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "run = experiment.submit(config=est)\n",
    "run\n",
    "\n",
    "print('Code executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor the run\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get result\n",
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#link for Azure Container Instances and Kubernetes\n",
    "https://docs.microsoft.com/azure/machine-learning/service/how-to-deploy-and-where#aci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#link for Azure IoT Edge\n",
    "https://docs.microsoft.com/azure/machine-learning/service/how-to-deploy-and-where#iotedge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#link for Field Programmable Gate Array\n",
    "https://docs.microsoft.com/azure/machine-learning/service/how-to-deploy-and-where#fpga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a model\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "X, y = load_diabetes(return_X_y = True)\n",
    "columns = ['age', 'gender', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "data = {\n",
    "    \"train\":{\"X\": X_train, \"y\": y_train},        \n",
    "    \"test\":{\"X\": X_test, \"y\": y_test}\n",
    "}\n",
    "\n",
    "print (\"Data contains\", len(data['train']['X']), \"training samples and\",len(data['test']['X']), \"test samples\")\n",
    "\n",
    "# Create, fit, and test the scikit-learn Ridge regression model\n",
    "regression_model = Ridge(alpha=0.03)\n",
    "regression_model.fit(data['train']['X'], data['train']['y'])\n",
    "preds = regression_model.predict(data['test']['X'])\n",
    "\n",
    "# Output the Mean Squared Error to the notebook and to the run\n",
    "print('Mean Squared Error is', mean_squared_error(data['test']['y'], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save work as a pickle file\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(value=regression_model, filename='sklearn_regression_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register the model\n",
    "from azureml.core.model import Model\n",
    "model = Model.register(model_path = \"sklearn_regression_model.pkl\",\n",
    "                       model_name = \"sklearn_regression_model.pkl\",\n",
    "                       tags = {'area': \"diabetes\", 'type': \"regression\"},\n",
    "                       description = \"Ridge regression model to predict diabetes\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After registering the models, you can reference the Azure portal so that you can see your registered models under Model tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Scoring Script\n",
    "\n",
    "Container images allow users to deploy models reliably since the machine learning model always depends on some other dependencies such as PyTorch. Using containers to deploy machine learning models can avoid dependency issues.\n",
    "\n",
    "A container image has the following items packaged, which you need to prepare:\n",
    "\n",
    "- The model itself\n",
    "- The inference engine, such as PyTorch\n",
    "- The scoring file (score.py) or other application consuming the model\n",
    "- Any dependencies needed\n",
    "\n",
    "The first step is to create the score.py file that consumes the model, like below. You only need to define two functions: init, which loads the model and run, which does the inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import pickle\n",
    "import json\n",
    "import numpy\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import Ridge\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path('sklearn_regression_model.pkl')\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "# note you can pass in multiple rows for scoring\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = numpy.array(data)\n",
    "        result = model.predict(data)\n",
    "        # you can return any datatype if it is JSON-serializable\n",
    "        return result.tolist()\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to make sure the dependencies are included in the image. Azure Machine Learning does that by creating a conda dependency file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "myenv = CondaDependencies.create(conda_packages=['numpy','scikit-learn'])\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is preparing the container image by running the code below. This may take a few minutes. You should see several output messages display as it creates the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.image import Image, ContainerImage\n",
    "\n",
    "image_config = ContainerImage.image_configuration(runtime= \"python\",\n",
    "                                 execution_script=\"score.py\",\n",
    "                                 conda_file=\"myenv.yml\",\n",
    "                                 tags = {'area': \"diabetes\", 'type': \"regression\"},\n",
    "                                 description = \"Image with ridge regression model\")\n",
    "\n",
    "image = Image.create(name = \"myimage1\",\n",
    "                     # this is the model object \n",
    "                     models = [model],\n",
    "                     image_config = image_config, \n",
    "                     workspace = ws)\n",
    "\n",
    "image.wait_for_creation(show_output = True)\n",
    "\n",
    "print('Execution complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the code is executed, you can view the images in the Azure Machine Learning Service portal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Model\n",
    "\n",
    "Define the deployment configuration. For example, the following code defines a container that uses 1 CPU and 1 GB of memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={'sample name': 'AML 101'}, \n",
    "                                               description='This is a great example.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the image created in the previous unit, you can use code similar to the code below. This may take a few minutes to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "# Create the webservice \n",
    "service = Webservice.deploy_from_model(name='my-aci-svc3',\n",
    "                                       deployment_config=aciconfig,\n",
    "                                       models=[model],\n",
    "                                       image_config=image_config,\n",
    "                                       workspace=ws)\n",
    "\n",
    "# Wait for the service deployment to complete while displaying log output.  This can take several minutes.\n",
    "service.wait_for_deployment(show_output=True)\n",
    "\n",
    "print('Execution complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Data w/ Deployed Model\n",
    "\n",
    "Since the model is deployed as a web service that exposes a REST API, it can be tested with many tools. Azure Machine Learning SDK has a built-in testing tool that can work with the deployed web service as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# scrape the first row from the test set.\n",
    "test_samples = json.dumps({\"data\": X_test[0:1, :].tolist()})\n",
    "\n",
    "print(test_samples) # here is what we are sending to the service.\n",
    "\n",
    "#score on our service\n",
    "service.run(input_data = test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "You can use the Azure Machine Learning SDK for Python to create ML pipelines, as well as to submit and track individual pipeline runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto ML\n",
    "\n",
    "AutoML provides an automated solution, let Azure Machine Learning Services run all the models concurrently, compare the results, and recommend the best model for the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an experiment\n",
    "experiment = Experiment(workspace = ws, name = \"my-third-experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert automl parameters\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "import logging\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             iteration_timeout_minutes = 6,\n",
    "                             iterations = 3,\n",
    "                             primary_metric = 'spearman_correlation',\n",
    "                             n_cross_validations = 5,\n",
    "                             debug_log = 'automl.log',\n",
    "                             verbosity = logging.INFO,\n",
    "                             X = X_train, \n",
    "                             y = y_train)\n",
    "\n",
    "print('Execution Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run automl\n",
    "local_run = experiment.submit(automl_config, show_output = True)\n",
    "\n",
    "print('Execution Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve best model\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(local_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way to retrieve best model w/o widget\n",
    "best_run, fitted_model = local_run.get_output()\n",
    "\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using HyperDrive to auto-tune parameters\n",
    "\n",
    "HyperDrive is a built-in service that automatically launches multiple experiments in parallel each with different parameter configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage and Monitor Models\n",
    "\n",
    "In the Azure Machine Learning service, you can track experiments and monitor metrics with a few lines of code.\n",
    "\n",
    "- how to add logs into your scripts so Azure Machine Learning can track the key metrics\n",
    "- how to submit the experiments with model monitoring enabled\n",
    "- how to monitor the progress of running jobs\n",
    "- how to view the model's results\n",
    "\n",
    "To explore the Azure Machine Learning service further with in-depth code samples, follow the link: https://github.com/Azure/MachineLearningNotebooks\n",
    "\n",
    "For full details and documentation follow the link: https://docs.microsoft.com/azure/machine-learning/service/how-to-enable-data-collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query models\n",
    "regression_models = Model.list(workspace=ws, tags=['area'])\n",
    "for m in regression_models:\n",
    "    print(\"Name:\", m.name,\"\\tVersion:\", m.version, \"\\tDescription:\", m.description, m.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SDK allows data scientists to code within any Python environment including Jupyter Notebooks, PyCharm, and Visual Studio Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azureml-monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable data collection\n",
    "#The first step is enabling data collection. This can either be done using the SDK or the portal. In the SDK, \n",
    "#add the line below to your score.py script. The code also enables application insights.\n",
    "\n",
    "aks_config = AciWebservice.deploy_configuration(collect_model_data=True, enable_app_insights=True)\n",
    "\n",
    "\n",
    "#sample of where to insert\n",
    "'''from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={'sample name': 'AML 101'}, \n",
    "                                               description='This is a great example.')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### collecting model data\n",
    "\n",
    "There are four steps to use the SDK to collect model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#step 1: import package\n",
    "from azureml.monitoring import ModelDataCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2: add the following code to the init() function\n",
    "global inputs_dc, prediction_d\n",
    "inputs_dc = ModelDataCollector(\"best_model\", identifier=\"inputs\", feature_names=[\"feat1\", \"feat2\", \"feat3\", \"feat4\", \"feat5\", \"Feat6\"])\n",
    "prediction_dc = ModelDataCollector(\"best_model\", identifier=\"predictions\", feature_names=[\"prediction1\", \"prediction2\"])\n",
    "\n",
    "#sample of code to input\n",
    "'''%%writefile score.py\n",
    "import pickle\n",
    "import json\n",
    "import numpy\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import Ridge\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path('sklearn_regression_model.pkl')\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "    #inserted code\n",
    "    global inputs_dc, prediction_d\n",
    "inputs_dc = ModelDataCollector(\"best_model\", identifier=\"inputs\", feature_names=[\"feat1\", \"feat2\", \"feat3\", \"feat4\", \"feat5\", \"Feat6\"])\n",
    "prediction_dc = ModelDataCollector(\"best_model\", identifier=\"predictions\", feature_names=[\"prediction1\", \"prediction2\"])\n",
    "    \n",
    "# note you can pass in multiple rows for scoring\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = numpy.array(data)\n",
    "        result = model.predict(data)\n",
    "        # you can return any datatype if it is JSON-serializable\n",
    "        return result.tolist()\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identifier is later used for building the folder structure in your Blob, it can be used to divide “raw” versus \"processed\" data.\n",
    "- CorrelationId is an optional parameter. You do not need to set it up if your model doesn't require it. Having a correlationId in place helps you simplify mapping of other data. (Examples include: LoanNumber, CustomerId, etc.)\n",
    "- Feature Names need to be identified in the order of your features for them to have column names when the .csv is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3: add the collection function calls, lines below, to your run function.\n",
    "inputs_dc.collect(data)\n",
    "prediction_dc.collect(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 4: update the myenv.yml file with the required modules needed to train the model.\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "myenv = CondaDependencies.create(conda_packages=['numpy','scikit-learn'])\n",
    "myenv.add_pip_package(\"azureml-monitoring\")\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WHERE CAN I VIEW THE COLLECTED DATA?\n",
    "\n",
    "The output of the monitoring service gets saved in the Azure BLOB storage account associated with your Azure Machine Learning service workspace. The path of the file follows the following pattern.\n",
    "\n",
    "/modeldata/<subscriptionid>/<resourcegroup>/<workspace>/<webservice>/<model>/<version>/<identifier>/<year>/<month>/<day>/data.csv\n",
    "\n",
    "An example would be:\n",
    "\n",
    "/modeldata/1a2b3c4d-5e6f-7g8h-9i10-j11k12l13m14/myresourcegrp/myWorkspace/aks-w-collv9/best_model/10/inputs/2018/12/31/data.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
