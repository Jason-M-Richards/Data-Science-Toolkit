{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Design\n",
    "\n",
    "### 1. Make an Observation\n",
    "\n",
    "The first step of the scientific method is to observe something that we want to test. During this step, we observe phenomena to help us refine the question that we want to answer.  This might be anything from \"does this drug have an affect on headaches?\" to \"does the color of this button affect the number of sales our website makes in a day?\".  Before we can test these things, we need to notice they exist and then come up with a specific question to answer. \n",
    "\n",
    "### 2. Examine the Research\n",
    "\n",
    "Good data scientists work smart before they work hard.  In the case of scientific method, this means seeing what research already exists that may help us answer our question, directly or indirectly.  It could be that someone else has already done an experiment that answers our question--if that's case, we should be aware of that experiment before starting our own, as it could inform our approach to structuring our experiment, or maybe even answer our question outright!  \n",
    "\n",
    "### 3. Form a Hypothesis\n",
    "\n",
    "This is the stage that most people remember from learning the scientific method in grade school. In school, we learned that a hypothesis is just an educated guess that we will try to prove by structuring our experiment. In reality, its a bit more complicated than that.  During this stage, we'll formulate 2 hypotheses to test--our educated guess about the outcome is called our **_Alternative Hypothesis_**, while the opposite of it is called the **_Null Hypothesis_**.  This is where the language behind experimental design (and the idea of what we can actually **_prove_** using an experiment) gets a bit complicated--more on this below. \n",
    "\n",
    "### 4. Conduct an Experiment\n",
    "\n",
    "This step is the part of the scientific method that we're most concerned with for this section. We can only test our hypothesis by gathering data from a well-structured experiment.  A well-structured experiment means is one that accounts for all of the mistakes and randomness that could give us mistaken signals as to the effect an intervention has.  Just because we're running an experiment doesn't prove that A causes B, or that there's even a relationship between them! A poorly designed experiment will lead to false conclusions that we haven't considered or controlled for--a well-designed experiment leaves us no choice but to acknowledge that the effects seen in a dependent variable are related to our independent variable.  The world is messy and random--we have to account for this messiness and randomness in experiments, so that we can filter it out and be left only with the things we're actively trying to measure. \n",
    "\n",
    "### 5. Analyze Experimental Results\n",
    "\n",
    "Whether you realize it or not, you've already gotten pretty good at this step! All the work we've done with statistics is usually in service of this goal--looking at the data and understanding what happened. During this step, we tease out relationships, filter out noise, and try to determine if something that happened is **_statistically significant_** or not. \n",
    "\n",
    "### 6. Draw Conclusions\n",
    "\n",
    "This step is the logical end point for our experiment.  We've asked a question, looked at experimental results from others that could be related to our question, made an educated guess, designed an experiment, collected data and analyzed the results.  All that is left is to use the results of our analysis step to evaluate whether we believe our hypothesis was correct or not! While the public generally oversimplifies this step to determining causal relationships (e.g. \"my experiment showed that {x} causes {y}\"), true scientists rarely make claims so bold.  The reality of this step is that we use our analysis of the data to do one of two things: either **_reject the null hypothesis, or fail to reject the null hypothesis_**.  This is a tricky concept, so we'll explore it in much more detail in a future lesson. \n",
    "\n",
    "### The Foundations of a Sound Experiment\n",
    "\n",
    "All experiments are not created equal--simply following the steps outlined above does not guarantee that the results of any experiment will be meaningful. For instance, there's nothing stopping a person from testing the hypothesis that \"wearing a green shirt will make it rain tomorrow!\", seeing rain the next day, and rejecting the null hypothesis, thereby incorrectly \"proving\" that their choice of wardrobe affected the weather.  Good experiments show us that our independent variable {X} has an affect on our dependent variable {Y} because we control for all the other things that could be affecting {Y}, until we are forced to conclude that the only thing that explains what happened to {Y} is {X}!\n",
    "\n",
    "Although there are many different kinds of experiments, there are some fundamental aspects of experimental design that all experiments have:\n",
    "\n",
    "#### 1. A Control Group/Random Controlled Trials\n",
    "\n",
    "One of the most important aspects of a sound experiment is the use of a **_Control Group_**. A Control Group is a cohort that receives no treatment or intervention--for them, it's just business as usual.  In a medical test, this might be a **_placebo_**, such as a sugar pill. In the example of testing the color of a button on a website, this would be customers that are shown the a version of the website with the button color unchanged.  Using a control group allows us to compare the results of doing nothing (our control) with the effects  of doing something (our **_intervention_**).  Without a control group, we have no way of knowing how much of the results we see can be attributed to our intervention, and how much would have happened anyways. \n",
    "\n",
    "To make this more obvious, let's consider what we can actually know with confidence after an experiment that doesn't use a control. Let's say that a pharmaceutical company decides to test a new drug that is supposed to reduce the amount of time someone has the flu.  The company gives the drug to all participants in the study.  After analyzing the data, we find that the average length of time a person had the flu was 12 days.  Was the drug effective, or not? Without a control, we don't know how long this flu would have lasted if these people were never given a drug.  It could be that our drug reduced the time of infection down to 12 days.  Then again, it could be that the these people would have gotten better on their own after 12 days, and our drug didn't really do anything--or maybe they would have gotten better in 10 days, and our drug made it worse! By using a control group that gets no drugs and recovers naturally, we can compare the results of our treatment (people that received the experimental flu drug) to our control group (people that recovered naturally).\n",
    "\n",
    "Note that a control group is only a control group if they are sampled from the same population as our treatment groups! If they aren't the same, then we have no way of knowing how much of the difference between in results should be attributed to our flu drug, and how much should be attributed to the way(s) in which the control group is different.  For instance, our experiment would not be very effective if the average age of one group was much higher or lower than another--if that was the case, how do we know the age difference isn't actually causing the difference in results (or lack thereof) between our control and our treatment groups, instead of our drug?\n",
    "\n",
    "The main way scientists deal with this is through **_Random Controlled Trials_**.  In a Random Controlled Trial, we have a control group and an intervention (also called treatment) group, where subjects are **_randomly assigned to each_**.  You may have heard the term **_Single-Blind_** and **_Double-Blind_**--these refer to people knowing which groups they are in. In a sound experiment, people should not know if they are in the treatment group or the control group, as that could potentially affect the outcome of the trial! \n",
    "\n",
    "A **_Single-Blind_** or **_Blind Trial_** is one where the participant does not know if they are receiving the treatment or a placebo. \n",
    "\n",
    "A **_Double-Blind Trial_** is one where the participant does not know if they are receiving the treatment or a placebo, and neither does the person administering the experiment (because their bias could affect the outcomes, too!).  Instead, knowing whether someone received the treatment or a placebo is kept hidden from everyone until after the experiment is over (obviously, _someone_ has to know for recordkeeping purposes, but that person stays away from the actual experiment to avoid contaminating it with bias from that knowledge). \n",
    "\n",
    "#### 2. Appropriate Sample Sizes\n",
    "\n",
    "Randomness is a big problem in experiments--it can lead us to false conclusions by making us think that something doesn't matter when it does, or vice versa. Small sample sizes make us susceptible to the problem of randomness. Large sample sizes protect us from it.  The following scenario illustrates this point:\n",
    "\n",
    "A person tells you that they can predict the outcome of a fair coin flip. You flip a coin, they call \"tails\", and they are correct.  Is this enough evidence to accept or reject this person's statement?  What if they got it right 2 times in a row? 5 times in a row? 55 times out of 100?  \n",
    "\n",
    "This situation illustrates two things that are important for us to understand and acknowledge:\n",
    "\n",
    "1. No matter how large your sample size, there's always a chance that our results can be attributed to randomness or luck.\n",
    "\n",
    "1. At some point, we cross a threshold where random chance is small enough that we say \"this probably isn't random\", and are okay with accepting the results as the result of something other than randomness or luck.\n",
    "\n",
    "With the situation above, we probably wouldn't assume that this person can predict coin flips after only seeing them get 1 correct.  However, if this person got 970 out of 1000 correct, we probably believe very strongly that this person _can_ predict coin flips, because the odds of guessing randomly and getting 970/1000 correct are very, very small--but not 0!  \n",
    "\n",
    "Large sample sizes protect us from randomness and variance. A more realistic example would be testing a treatment for HIV.  Less than 1% of the global population carry a protective mutation that makes them resistant to HIV infection.  If our sample size is only 1 person randomly selected from the population, there is a ~1% chance that we may mistakenly attribute successful prevention to the drug we're testing, when the results really happened because we randomly selected a person with this mutation.  However, if our sample size is 100 people per sample, our odds of randomly selecting 100 people with that mutation are $.01^100$. The larger our sample size, the more unlikely it is that we randomly draw people that happen to affect our study.\n",
    "\n",
    "#### 3. Reproducibility\n",
    "\n",
    "This one is a big one, and is a bit of a crisis in some parts of the scientific community right now.  Good scientific experiments have **_Reproducible Results_**! This means that if someone else follows the steps you outline for your experiment and performs it themselves, they should get pretty much the same results as you did (allowing for natural variance and randomness). If many different people try reproducing your experiment and don't get the same results, this might suggest that your results may be due to randomness, or to a **_lurking variable_** that was present in your samples that wasn't present in others. Either way, lack of reproducibility often casts serious doubts on the results of a study or experiment. \n",
    "\n",
    "This is less of a problem for data scientists, since reproducibility for us usually just means providing the dataset we worked with and the corresponding jupyter notebook.  However, this isn't always the case!   Luckily, we can use code to easily run our experiments multiple times and show reproducibility. When planning experiments, consider running them multiple times to ensure to really help show that your results are sound, and not due to randomness!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Types\n",
    "\n",
    "**_Null Hypothesis_**: There is no relationship between A and B \n",
    "Example: \"There is no relationship between this flu medication and a reduced recovery time from the flu\".\n",
    "\n",
    "The _Null Hypothesis_ is usually denoted as $H_O$\n",
    "\n",
    "**_Alternative Hypothesis_**: The hypothesis we traditionally think of when thikning of a hypothesis for an experiment\n",
    "Example: \"This flu medication reduces recovery time for the flu.\"\n",
    "\n",
    "The _Alternative Hypothesis_ is usually denoted as $H_a$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## P-Values and Alpha Values\n",
    "\n",
    "No matter what you're experimenting on, good experiments come down down to one question: Is our p-value less than our alpha value? Let's dive into what each of these values represents, and why they're so important to experimental design. \n",
    "\n",
    "**_P-value_**: The calculated probability of arriving at this data randomly. \n",
    "\n",
    "If we calculate a p-value and it comes out to 0.03, we can interpret this as saying \"There is a 3% chance that the results I'm seeing are actually due to randomness or pure luck\".  \n",
    "\n",
    "$\\alpha$ **_(alpha value)_**: The marginal threshold at which we're okay with with rejecting the null hypothesis. \n",
    "\n",
    "An alpha value can be any value we set between 0 and 1. However, the most common alpha value in science is 0.05 (although this is somewhat of a controversial topic in the scientific community, currently).  \n",
    "\n",
    "If we set an alpha value of $\\alpha = 0.05$, we're essentially saying \"I'm okay with accepting my alternative hypothesis as true if there is less than a 5% chance that the results that I'm seeing are actually due to randomness\".  \n",
    "\n",
    "When we conduct an experiment, our goal is calculate a p-value and compare it to our alpha value. If $p < \\alpha$, then we **_reject the null hypothesis_** and accept that there is not \"no relationship\" between our dependent variables.  Note that any good scientist will admit that this doesn't prove that there is a _direct relationship_ between our dependent and independent variables--just that we have enough evidence to the contrary to show that we can no longer believe that there is no relationship between them. \n",
    "\n",
    "In simple terms:\n",
    "\n",
    "$p < \\alpha$: Reject the _Null Hypothesis_ and accept the _Alternative Hypothesis_\n",
    "\n",
    "$p >= \\alpha$: Fail to reject the _Null Hypothesis_.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring Hypothesis statement\n",
    "\n",
    "There are many different ways that we can structure a hypothesis statement, but they always come down to this comparison in the end.  In normally distributed data, we calculate p-values from z-scores. This is done a bit differently with discrete data. We may also have **_One-Tail_** and **_Two-Tail_** tests.  \n",
    "\n",
    "A **_One-Tail Test_** is when we want to know if a parameter from our treatment group is greater than (or less than) a corresponding parameter from our control group.\n",
    "\n",
    "**_Example One-Tail Hypothesis_**\n",
    "\n",
    "\"$H_a = \\mu_1 < \\mu_2 $ The treatment group given this weight loss drug will lost more weight on average than the control group that was given a competitor's weight loss drug \n",
    "\n",
    "$ H_o = \\mu1 >= \\mu_2$  The treatment group given this weight loss drug will not lose more weight on average than the control group that was given a competitor's weight loss drug\". \n",
    "\n",
    "A **_Two-Tail Test_** is for when we want to test if a parameter falls between (or outside of) a range of two given values. \n",
    "\n",
    "**_Example Two-Tail Hypothesis_**\n",
    "\n",
    "$H_a = \\mu_1 != \\mu_2$ \"People in the experimental group that are administered this drug will not lose the same amount of weight as the people in the control group.  They will be heavier or lighter\". \n",
    "\n",
    "$H_o = \\mu_1 = \\mu_2$ \"People in the experimental group that are administered this drug will lose the same amount of weight as the people in the control group.\"\n",
    "\n",
    "#### Steps for a one-sample t-test\n",
    "\n",
    "##### Step 1: Write your null hypothesis statement\n",
    "##### Step 2: Write your alternate hypothesis.\n",
    "##### Step 3: Import necessary libraries and calculate sample statistics:\n",
    "- The population mean ($\\mu$). \n",
    "- The sample mean ($\\bar{x}$). Calculate from the sample data\n",
    "- The sample standard deviation ($s$). Calculate from sample data\n",
    "- Number of observations($n$). This can be calculated from the sample data.\n",
    "- Degrees of Freedom($df$). Calculate from the sample as df = total no. of observations - 1\n",
    "\n",
    "##### Step 4: Calculate the t value from given data\n",
    "    #Calculate Sigma\n",
    "    t = (x_bar -  mu)/(sigma/np.sqrt(n))\n",
    "    t\n",
    "##### Step 5: Find the critical t value.\n",
    "##### Step 6: Compare t-value with critical t-value to accept or reject the Null hypothesis.\n",
    "\n",
    "#### code for one sample t-test\n",
    "\n",
    "    def one_sample_ttest(sample, popmean, alpha):\n",
    "\n",
    "        # Visualize sample distribution for normality \n",
    "        sns.set(color_codes=True)\n",
    "        sns.set(rc={'figure.figsize':(12,10)})\n",
    "        sns.distplot(sample)\n",
    "\n",
    "        # Population mean \n",
    "        mu = popmean\n",
    "\n",
    "        # Sample mean (x̄) using NumPy mean()\n",
    "        x_bar= sample.mean()\n",
    "\n",
    "        # Sample Standard Deviation (sigma) using Numpy\n",
    "        sigma = np.std(sample)\n",
    "\n",
    "        # Degrees of freedom\n",
    "        df = len(sample) - 1\n",
    "    \n",
    "        #Calculate the critical t-value\n",
    "        t_crit = stats.t.ppf(1 - alpha, df=df)\n",
    "\n",
    "        #Calculate the t-value and p-value\n",
    "        results = stats.ttest_1samp(a= sample, popmean= mu)         \n",
    "\n",
    "        if (results[0]>t_crit) and (results[1]<alpha):\n",
    "            print (\"Null hypothesis rejected. Results are statistically significant with t-value =\", \n",
    "                    round(results[0], 2), \"critical t-value =\", t_crit, \"and p-value =\", np.round((results[1]), 10))\n",
    "        else:\n",
    "            print (\"Null hypothesis is True with t-value =\", \n",
    "                    round(results[0], 2), \", critical t-value =\", t_crit, \"and p-value =\", np.round((results[1]), 10))\n",
    "                    \n",
    "#### two sample t-test\n",
    "\n",
    "    '''\n",
    "    Calculates the T-test for the means of *two independent* samples of scores.\n",
    "\n",
    "    This is a two-sided test for the null hypothesis that 2 independent samples\n",
    "    have identical average (expected) values. This test assumes that the\n",
    "    populations have identical variances by default.\n",
    "    '''\n",
    "\n",
    "    stats.ttest_ind(experimental, control)\n",
    "    \n",
    "**scipy library has several options for t-test single line code**\n",
    "https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "\n",
    "#### Welch's t-test (for unequal variances)\n",
    "    def welch_t(a, b):\n",
    "\n",
    "        \"\"\" Calculate Welch's t statistic for two samples. \"\"\"\n",
    "\n",
    "        numerator = a.mean() - b.mean()\n",
    "\n",
    "        # “ddof = Delta Degrees of Freedom”: the divisor used in the calculation is N - ddof, \n",
    "        #  where N represents the number of elements. By default ddof is zero.\n",
    "\n",
    "        denominator = np.sqrt(a.var(ddof=1)/a.size + b.var(ddof=1)/b.size)\n",
    "\n",
    "        return numerator/denominator\n",
    "\n",
    "    welch_t(a,b)\n",
    "    \n",
    "#### Permutation test for population similarity (null hypothesis - populations not similar)\n",
    "calculate the mean of both samples and wish to perform a hypothesis test with a 5% confidence interval for whether the two samples belong to the same overall population. In our previous work, we would use a t-test to perform this comparison. The permutation test alternative would be to compare the difference in these sample means to the difference in sample means of all possible combinations of 37-45 splits between our 82 data points. In other words, we compare the difference between our actual sample means to the difference in sample means between all variations of all those 82 points in order to calculate our p-values and determine whether we accept or reject the null-hypothesis.\n",
    "\n",
    "    diff_mu_a_b = np.mean(a) - np.mean(b)\n",
    "    combos = permT(a, b)\n",
    "    print(\"There are {} possible sample variations.\".format(len(combos)))\n",
    "    num = 0 #Initialize numerator\n",
    "    for ai, bi in combos:\n",
    "        diff_mu_ai_bi = np.mean(ai) - np.mean(bi)\n",
    "        if diff_mu_ai_bi >= diff_mu_a_b:\n",
    "            num +=1\n",
    "    p_val = num / len(combos)\n",
    "    print('P-value: {}'.format(p_val))\n",
    "\n",
    "#### Permutation Tests and Exploding Combination Sizes - Using Monte Carlo Simulations¶\n",
    "When conducting permutation tests, the size of potential combination sizes quickly explodes as our original sample sizes grow. As a result, even with modern computers, it is often infeasible or aggregiously resource expensive to attempt to generate these permutation spaces. To cope with this, monte carlo simulations are often used in practice in order to simulate samples from the permutation space.\n",
    "\n",
    "    diff_mu_a_b = np.mean(a) - np.mean(b)\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    union = a + b\n",
    "    for i in range(5*10**6):\n",
    "        #Generate an a\n",
    "        ai = np.random.choice(union, size=len(a), replace=False)\n",
    "        #Generate its compliment as b\n",
    "        bi = union.copy()\n",
    "        for item in ai:\n",
    "            bi.remove(item)\n",
    "        diff_mu_ai_bi = np.mean(ai) - np.mean(bi)\n",
    "        if diff_mu_ai_bi >= diff_mu_a_b:\n",
    "            num +=1\n",
    "        denom += 1\n",
    "        #Compute difference in means\n",
    "        if i in [10,100,500,1000, 10**4, 10**5, 10**6, 2*10**6, 5*10**6]:\n",
    "            print(\"After {} iterations p-value is: {}\".format(i, num/denom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect Size\n",
    "\n",
    "In a data analytics domain, effect size calculation serves three primary goals:\n",
    "\n",
    "* Communicate **practical significance** of results. An effect might be statistically significant, but does it matter in practical scenarios?\n",
    "\n",
    "* Effect size calculation and interpretation allows you to draw **Meta-Analytical** conclusions. This allows you to group together a number of existing studies, calculate the meta-analytic effect size and get the best estimate of the tur effect size of the population. \n",
    "\n",
    "* Perform **Power Analysis**, which help determine the number of participants (sample size) that a study requires to achieve a certain probability of finding a true effect - if there is one. \n",
    "\n",
    "#### Un-standardized or Simple Effect Size Calculation\n",
    "An unstandardized effect size simply tries to find the difference between two groups by calculating the difference between distribution means. Here is how you can do it in python. \n",
    "\n",
    "    mean1, std1 = sample1.mean(), sample1.std()\n",
    "    mean1, std1\n",
    "    mean2, std2 = sample2.mean(), sample2.std()\n",
    "    mean2, std2\n",
    "    difference_in_means = sample1.mean() - sample2.mean()\n",
    "    difference_in_means\n",
    "    \n",
    "#### overlap threshold to determine possibility of misclassification (how many samples overlap)\n",
    "    simple_thresh = (mean1 + mean2) / 2\n",
    "    simple_thresh\n",
    "    sample1_below_thresh = sum(sample1 < thresh)\n",
    "    sample1_below_thresh\n",
    "    sample2_above_thresh = sum(sample2 > thresh)\n",
    "    sample2_above_thresh\n",
    "    overlap = sample1_below_thresh / len(sample1) + sample2_above_thresh / len(sample2)\n",
    "    overlap\n",
    "    misclassification_rate = overlap / 2\n",
    "    misclassification_rate\n",
    "   \n",
    "#### pobability of superiority (likelyhood of x>y)\n",
    "    sum(x > y for x, y in zip(sample1, sample2)) / len(sample1)\n",
    "    \n",
    "#### Cohen's $d$\n",
    "\n",
    "Cohen’s D is one of the most common ways to measure effect size.  As an effect size, Cohen's d is typically used to represent the magnitude of differences between two (or more) groups on a given variable, with larger values representing a greater differentiation between the two groups on that variable. \n",
    "\n",
    "The basic formula to calculate Cohen’s $d$ is:\n",
    "\n",
    "> ** $d$ = effect size (difference of means) / pooled standard deviation **\n",
    "\n",
    "    def Cohen_d(group1, group2):\n",
    "\n",
    "        # Compute Cohen's d.\n",
    "\n",
    "        # group1: Series or NumPy array\n",
    "        # group2: Series or NumPy array\n",
    "\n",
    "        # returns a floating point number \n",
    "\n",
    "        diff = group1.mean() - group2.mean()\n",
    "\n",
    "        n1, n2 = len(group1), len(group2)\n",
    "        var1 = group1.var()\n",
    "        var2 = group2.var()\n",
    "\n",
    "        # Calculate the pooled threshold as shown earlier\n",
    "        pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n",
    "\n",
    "        # Calculate Cohen's d statistic\n",
    "        d = diff / np.sqrt(pooled_var)\n",
    "\n",
    "        return d\n",
    "\n",
    "#### Interpreting $d$\n",
    "Most people don't have a good sense of how big $d=2.0$ is. If you are having trouble visualizing what the result of Cohen’s D means, use these general “rule of thumb” guidelines (which Cohen said should be used cautiously):\n",
    "\n",
    ">**Small effect = 0.2**\n",
    "\n",
    ">**Medium Effect = 0.5**\n",
    "\n",
    ">**Large Effect = 0.8**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type I and Type II errors\n",
    "\n",
    "#### Type I (alpha)\n",
    "When conducting hypothesis testing, you must choose a confidence level, alpha ($\\alpha$) which you will use as the threshold for accepting or rejecting the null hypothesis. This confidence level is also the probability that you reject the null hypothesis when it is actually true. This scenario is a Type 1 error, more commonly known as a **False Positive**. \n",
    "\n",
    "#### Type II (beta)\n",
    "Another type of error is beta ($\\beta$), which is the probability that you fail to reject the null hypothesis when it is actually false. Type 2 errors are also referred to as **False Negatives**.\n",
    "\n",
    "#### Balancing Type I and Type II Errors \n",
    "Different scenarios call for scientists to minimize one type of error over another. The two error types are inversely related to one other; reducing type 1 errors will increase type 2 errors and vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Power\n",
    "\n",
    "The power of a statistical test is defined as the probability of rejecting the null hypothesis, given that it is indeed false. As with any probability, the power of a statistical test therefore ranges from 0 to 1, with 1 being a perfect test that gaurantees rejecting the null hypothesis when it is indeed false.\n",
    "\n",
    "#### code to visualize power as sample size increases\n",
    "\n",
    "    #What does the power increase as we increase sample size?\n",
    "    powers = []\n",
    "    cutoff = .99 #Set the p-value threshold for rejecting the null hypothesis\n",
    "    #Iterate through various sample sizes\n",
    "    unfair_coin_prob = .75\n",
    "    for n in range(1,50):\n",
    "        #Do multiple runs for that number of samples to compare\n",
    "        p_val = []\n",
    "        for i in range(200):\n",
    "            n_heads = np.random.binomial(n, unfair_coin_prob)\n",
    "            mu = n / 2\n",
    "            sigma = np.sqrt(n*.5*(1-.5))\n",
    "            z  = (n_heads - mu) / (sigma / np.sqrt(n))\n",
    "            p_val.append(st.norm.cdf(np.abs(z)))\n",
    "        cur_power = sum([1 if p >= cutoff else 0 for p in p_val])/200\n",
    "        powers.append(cur_power)\n",
    "    plt.plot(list(range(1,50)), powers)\n",
    "    plt.title('Power of Statistical Tests of a .75 Unfair Coin by Number of Trials using .99 threshold')\n",
    "    plt.ylabel('Power')\n",
    "    plt.xlabel('Number of Coin Flips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Testing to assist in Experiment Design\n",
    "\n",
    "### Step 1: State the Null Hypothesis, $H_0$\n",
    "### Step 2: State the Alternative Hypothesis, $H_1$\n",
    "### Step 3: Define Alpha and Beta\n",
    "- start at .01 or .05 for both and adjust as needed\n",
    "### Step 4: Calculate N (sample size)\n",
    "    import scipy.stats as st\n",
    "    def compute_n(alpha, beta, mu_0, mu_1, var):\n",
    "        z_alpha = st.norm.ppf(alpha)\n",
    "        z_beta = st.norm.ppf(beta)\n",
    "        num = ((z_alpha+z_beta)**2)*var\n",
    "        den = (mu_1 - mu_0)**2\n",
    "        return num/den\n",
    "\n",
    "    alpha = .01 #Part of A/B test design\n",
    "    beta = .01 #Part of A/B test design\n",
    "    mu_0 = .76 #Part of A/B test design\n",
    "    mu_1 = .8 #Part of A/B test design\n",
    "    var = .1 #sample variance\n",
    "\n",
    "    compute_n(alpha, beta, mu_0, mu_1, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA Testing\n",
    "\n",
    "ANOVA (Analysis of Variance) is a method for generalizing of previous discussion regarding statistical tests to multiple groups. As we will see, ANOVA then partitions our total sum of square of deviations (from the mean) into sum of squares for each of these groups and sum of squares for error.\n",
    "\n",
    "#### ANOVA table\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "formula = Control_Column ~ C(factor_col1) + factor_col2 + C(factor_col3) + ... + X #C() values are categorical values\n",
    "lm = ols(formula, df).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=2)\n",
    "print(table)\n",
    "\n",
    "**Higher values of the F-statistic indicate a higher probability of that factor being influential.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodharts Law and importance for Data Scientists?\n",
    "\n",
    "[Goodhart's Law](https://en.wikipedia.org/wiki/Goodhart%27s_law) is an observation made by the British Economist Charles Goodhart in 1975.  Charles Goodhart famously said:\n",
    "\n",
    "> \"Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.\"  --Charles Goodhart\n",
    "\n",
    "Goodhart's Law is something that matters much to Data Scientists because it is our findings and experiments that often drive the policies and decisions made by a company.  Data Science is complex, and often, project managers, CEOs, and other decision makers don't want to know about experimental methodologies or confidence intervals--they just want to know what the best decision they can make is, based on what the data says! It's quite common for decision makers to not realize that setting a target for one metric can negatively affect other metrics in ways that aren't immediately obvious--for instance, pushing employees at a call center to reduce call times could possibly reduce customer satisfaction, because of employees hustling to get off the phone based on the shorter call time \"target\" handed down from management.  \n",
    "\n",
    "As a data scientist, it is important to communicate your results clearly to stakeholders--but it is also important to be the voice of reason at times.  This is why communication with stakeholders is important throughout the process of any data science project.  The sooner you know how they plan on using your results, the more you can help them avoid ugly unforseen problems that come from Goodhart's Law--always remember that massive amounts of data are no substitute for _critical thinking_! At the very least, you should get a bit nervous when you see targets being set for certain metrics.  Note that this doesn't necessarily mean \"don't set targets\"--instead, seek to encourage decision makers to think critically about any unintended consequences these targets could have, and track changes in metrics early and often when new policies or targets are put in place to ensure that unintended consequences are caught early!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
