{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Programming Interface (API)\n",
    "\n",
    "**ALWAYS CHECK WEBSITE API PARAMETERS REGARDING LIMITS, AVAILABLE INFO., ETC.**\n",
    "\n",
    "> API stands for **Application Programming Interface**. \n",
    "\n",
    "An API is a communication protocol between 2 software systems. It describes the mechanism through which if one system **requests** some information using a predefined format, a remote system **responds** with an outcome that gets sent back to the first system. \n",
    "\n",
    "APIs are a way of allowing 2 applications to interact with each other. This is an incredibly common task in modern web-based programs. For instance, if you've ever connected your facebook profile to another service such as Spotify or Instagram, this is done through APIs. An API represents a way for 2 pieces of software to interact with one another. Under the hood, the actual request and response is done as a **_HTTP Request_**.\n",
    "\n",
    "### What is an API made of ?\n",
    "\n",
    "APIs are very common in tech world, which means that are many, many different kinds that you're going to run into. While each API you work with will be unique in some way, there are some common traits you can expect to see overall. An API has three main components as listed below:\n",
    "\n",
    "* **Access Permissions:** Is the user allowed to ask for data or services?\n",
    "* **Request:** The service being asked for (e.g., if I give you current location using GPS, tell me the map around that place - as we see in Pokemon Go).  A Request has two main parts:\n",
    "\n",
    "    * **Methods:** Once the access is permitted, what questions can be asked.\n",
    "    \n",
    "    * **Parameters:** Additional details that can be sent with requests or responses\n",
    "\n",
    "* **Response:** The data or service as a result of the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP Request using python requests\n",
    "\n",
    "To make these things simpler, one easy-to-use third-party library, known as Requests, is available and most developers prefer to use it instead or urllib/urllib2. It is an Apache2 licensed HTTP library powered by urllib3 and httplib. Requests is add-on library that allows you to send HTTP requests using Python. With this library, you can access content like web page headers, form data, files, and parameters via simple Python commands. It also allows you to access the response data in a simple way.\n",
    "\n",
    "    !pip install requests\n",
    "    #Import requests to working environment\n",
    "    import requests\n",
    "    \n",
    "    #Making a request\n",
    "    resp = requests.get('https://www.google.com')\n",
    "    \n",
    "    #Check the returned status code\n",
    "    resp.status_code == requests.codes.ok\n",
    "    #https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\n",
    "    \n",
    "    #Once we know that our request was successful and we have a valid response, we can check the returned information using `.text` property of the response object. \n",
    "    print (resp.text)\n",
    "   \n",
    "    #Read the header of the response - convert to dictionary for displaying k:v pairs neatly\n",
    "    dict(resp.headers)\n",
    "    \n",
    "    #parsing retrieved data\n",
    "    print(resp.headers['Content-Length'])  # length of the response\n",
    "    print(resp.headers['Date'])  # Date the response was sent\n",
    "    print(resp.headers['server'])   # Server type (google web service - GWS)\n",
    "    \n",
    "#### Passing Parameters in GET\n",
    "In some cases, you'll need to pass parameters along with your GET requests. These extra parameters usually take the the form of query strings added to the requested URL. To do this, we need to pass these values in the `params` parameter. Let's try to acces information from `httpbin` with some user information. \n",
    "\n",
    "Note: The user information is not getting authenticated at `httpbin` so any name/password will work fine. This is merely for practice. \n",
    "\n",
    "\n",
    "    credentials = {'user_name': 'FlatironSchool', 'password': 'learnlovecode'}  \n",
    "    r = requests.get('http://httpbin.org/get', params=credentials)\n",
    "\n",
    "    print(r.url)  \n",
    "    print(r.text)\n",
    "\n",
    "#### HTTP POST method \n",
    "\n",
    "Sometimes we need to send one or more files simultaneously to the server. For example, if a user is submitting a form and the form includes different fields for uploading files, like user profile picture, user resume, etc. Requests can handle multiple files on a single request. This can be achieved by putting the files to a list of tuples in the form (field_name, file_info).\n",
    "\n",
    "    import requests\n",
    "\n",
    "    url = 'http://httpbin.org/post'  \n",
    "    file_list = [  \n",
    "        ('image', ('fi.png', open('fi.png', 'rb'), 'image/png')),\n",
    "        ('image', ('fi2.jpeg', open('fi2.jpeg', 'rb'), 'image/png'))\n",
    "    ]\n",
    "\n",
    "    r = requests.post(url, files=file_list)  \n",
    "    print(r.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OAuth\n",
    "\n",
    "> **OAuth stands for Open Authorization.**\n",
    "\n",
    "**_OAuth_** is an open-source protocol created to allow the creators of APIs and other online services to easily let them share private data or assets with users. One of the biggest challenges of building multi-user applications is making sure that you only give people access to the data and functionality they're supposed to have. OAuth provides a framework for allowing authenticated access, but without the risk of having to share the original login credentials such as a password.  The OAuth protocol was created in 2010, and was the brainchild of major tech companies such as Google and Twitter. It's now the most popular open standard for user authentication, is is used by almost all of the the major players in the tech world, such as Netflix, Amazon, Facebook, and more!\n",
    "\n",
    "### The Steps of OAuth\n",
    "\n",
    "Prior to using OAuth, we must also register our application with the authorizer and get our **credentials** to use during the process. We need to set up some information about the application, like the app's name or website, and most importantly, **a redirect URI**. The authorizer later uses this to contact the requesting app and tell them that the user said yes. \n",
    "\n",
    "> A URI (Uniform Resource Identifier) is a string that refers to a resource. The most common are URLs, which identify the resource by giving its location on the Web.\n",
    "\n",
    "After registration, The first step is the **authorization**. Here, we send our users to the authorization server to ask for some permissions with our scope (permissions) that we would like to have. The user can see everything being requested on his behalf and confirm that they would like to grant our application access for those permissions.\n",
    "\n",
    " \n",
    "The second step is the **redirect**. Redirect URI are a critical part of the OAuth flow. After a user successfully authorizes their application, the authorization server then redirects the user back to app with an **authorization code** in the URL. Because the redirect URL will contain sensitive information, it is critical that the service doesn’t redirect the user to arbitrary locations. The authorization code is used by our application in the final act of getting the access token. \n",
    "\n",
    "\n",
    "The final step is **acquisition**. This is where we finally receive our **access token** from service provider so we can process API requests for our user. We use the authorization code we received in the redirect to our redirect url and our own application secret (which is acquired during initial registration) in order to get our user’s access token. The access token can then be used to make API calls on behalf of our user.\n",
    "\n",
    "#### OAuth requests and conversion to DataFrame\n",
    "**wont work unless proper credentials are entered (authorization, bearer)**\n",
    "\n",
    "    #As a general rule of thumb, don't store passwords in a main file like this!\n",
    "    #Instead, you would normally store those passwords under a sub file like passwords.py which you would then import.\n",
    "    #Or even better, as an environment variable that could then be imported!\n",
    "    #For now, we'll simply hardcode them into our notebook for simplicity.\n",
    "    client_id = #Your client ID goes here (as a string)\n",
    "    api_key = #Your api key goes here (as a string)\n",
    "    \n",
    "    term = 'Mexican'\n",
    "    location = 'Astoria NY'\n",
    "    SEARCH_LIMIT = 10\n",
    "\n",
    "    url = 'https://api.yelp.com/v3/businesses/search'\n",
    "\n",
    "    headers = {\n",
    "            'Authorization': 'Bearer {}'.format(api_key), #authorization information\n",
    "        }\n",
    "\n",
    "    url_params = {\n",
    "                    'term': term.replace(' ', '+'),\n",
    "                    'location': location.replace(' ', '+'),\n",
    "                    'limit': SEARCH_LIMIT\n",
    "                }\n",
    "    response = requests.get(url, headers=headers, params=url_params)\n",
    "    print(response)\n",
    "    print(type(response.text))\n",
    "    print(response.text[:1000])\n",
    "    \n",
    "    #get response\n",
    "    response.json().keys()\n",
    "    \n",
    "    #preview information in each key\n",
    "    for key in response.json().keys():\n",
    "        print(key)\n",
    "        value = response.json()[key] #Use standard dictionary formatting\n",
    "        print(type(value)) #What type is it?\n",
    "        print('\\n\\n') #Seperate out data\n",
    "        \n",
    "    #more detailed responses\n",
    "    response.json()['businesses'][:2]\n",
    "    response.json()['region']\n",
    "    response.json()['total']\n",
    "    \n",
    "    #convert information to pandas dataframe\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame.from_dict(response.json()['businesses'])\n",
    "    print(len(df)) #Print how many rows\n",
    "    print(df.columns) #Print column names\n",
    "    df.head() #Previews the first five rows. \n",
    "    #You could also write df.head(10) to preview 10 rows or df.tail() to see the bottom\n",
    "    \n",
    "#### Bonus: pagination to retrieve all results and convert to DF\n",
    "\n",
    "    #Your code here; use a function or loop to retrieve all the results from your original request\n",
    "    import pandas as pd\n",
    "    import time\n",
    "\n",
    "    def yelp_call(url_params, api_key):\n",
    "        url = 'https://api.yelp.com/v3/businesses/search'\n",
    "        headers = {'Authorization': 'Bearer {}'.format(api_key)}\n",
    "        response = requests.get(url, headers=headers, params=url_params)\n",
    "\n",
    "        df = pd.DataFrame(response.json()['businesses'])\n",
    "        return df\n",
    "\n",
    "    def all_results(url_params, api_key):\n",
    "        num = response.json()['total']\n",
    "        print('{} total matches found.'.format(num))\n",
    "        cur = 0\n",
    "        dfs = []\n",
    "        while cur < num and cur < 1000:\n",
    "            url_params['offset'] = cur\n",
    "            dfs.append(yelp_call(url_params, api_key))\n",
    "            time.sleep(1) #Wait a second\n",
    "            cur += 50\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    term = 'pizza'\n",
    "    location = 'Astoria NY'\n",
    "    url_params = {  'term': term.replace(' ', '+'),\n",
    "                    'location': location.replace(' ', '+'),\n",
    "                    'limit' : 50\n",
    "                 }\n",
    "    df = all_results(url_params, api_key)\n",
    "    print(len(df))\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping with Beautiful Soup\n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/   \n",
    "\n",
    "Beautiful Soup is a Python library designed for quick turnaround projects like screen-scraping. Three features make it powerful:\n",
    "\n",
    "* Beautiful Soup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree: a toolkit for dissecting a document and extracting what you need. It doesn't take much code to write an application\n",
    "\n",
    "* Beautiful Soup automatically converts incoming documents to Unicode and outgoing documents to UTF-8. You don't have to think about encodings, unless the document doesn't specify an encoding and Beautiful Soup can't detect one. Then you just have to specify the original encoding.\n",
    "\n",
    "* Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you to try out different parsing strategies or trade speed for flexibility.\n",
    "\n",
    "Beautiful Soup parses anything you give it, and does the tree traversal stuff for you. You can tell it \"Find all the links\", or \"Find all the links of class externalLink\", or \"Find all the links whose urls match \"foo.com\", or \"Find the table heading that's got bold text, then give me that text.\"\n",
    "\n",
    "#### Packages\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import re\n",
    "    import pandas as pd\n",
    "\n",
    "#### Collect song lyrics from an artists page\n",
    "\n",
    "    def grab_song_links(artist_page_url):\n",
    "\n",
    "        url = artist_page_url\n",
    "\n",
    "        html_page = requests.get(url) #Make a get request to retrieve the page\n",
    "        soup = BeautifulSoup(html_page.content, 'html.parser') #Pass the page contents to beautiful soup for parsing\n",
    "\n",
    "\n",
    "        #The example from our lecture/reading\n",
    "        data = [] #Create a storage container\n",
    "\n",
    "        #Get album divs\n",
    "        albums = soup.find_all(\"div\", class_=\"album\")\n",
    "        for album_n in range(len(albums)):\n",
    "            #On the last album, we won't be able to look forward\n",
    "            if album_n == len(albums)-1:\n",
    "                cur_album = albums[album_n]\n",
    "                album_songs = cur_album.findNextSiblings('a')\n",
    "                for song in album_songs:\n",
    "                    page = song.get('href')\n",
    "                    title = song.text\n",
    "                    album = cur_album.text\n",
    "                    data.append((title, page, album))\n",
    "            else:\n",
    "                cur_album = albums[album_n]\n",
    "                next_album = albums[album_n+1]\n",
    "                saca = cur_album.findNextSiblings('a') #songs after current album\n",
    "                sbna = next_album.findPreviousSiblings('a') #songs before next album\n",
    "                album_songs = [song for song in saca if song in sbna] #album songs are those listed after the current album but before the next one!\n",
    "                for song in album_songs:\n",
    "                    page = song.get('href')\n",
    "                    title = song.text\n",
    "                    album = cur_album.text\n",
    "                    data.append((title, page, album))\n",
    "        return data\n",
    "        \n",
    "#### function to scrape text (lyrics)\n",
    "\n",
    "    def scrape_lyrics(song_page_url):\n",
    "        html_page = requests.get(song_page_url)\n",
    "        soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "        main_page = soup.find('div', {\"class\": \"container main-page\"})\n",
    "        main_l2 = main_page.find('div', {\"class\" : \"row\"})\n",
    "        main_l3 = main_l2.find('div', {\"class\" : \"col-xs-12 col-lg-8 text-center\"})\n",
    "        lyrics = main_l3.findAll('div')[6].text\n",
    "        return lyrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
