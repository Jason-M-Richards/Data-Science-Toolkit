{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats Cheatsheet\n",
    "\n",
    "https://drive.google.com/open?id=1e_Nklf_stdNcXx2MXskey-XLH8CyWFsy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Modeling\n",
    "\n",
    "#### Statistical Learning\n",
    "\n",
    "> Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data. **The goal of statistical learning theory is to study, in a statistical framework, the properties of learning algorithms.**\n",
    "\n",
    "#### Types of data in statistical learning \n",
    "\n",
    "In the context of Statistical learning, there are two types of data:\n",
    "\n",
    "* **Data that can be controlled directly OR independent variables** \n",
    "* **Data that cannot be controlled directly OR dependent variables**\n",
    "\n",
    "#### Statistical Model\n",
    "\n",
    "> A statistical model can be thought as some kind of a transformation that helps us express dependent variables **as a function** of independent variables. \n",
    "\n",
    "SO a model essentially defines a **Relationship** between a dependent and an independent variable. For the plot we see above, the relationship between height and weight, in its simplest form, can be shown using a **straight line** connecting all the individual observations in the data. So this line here would be our **model** as shown. \n",
    "\n",
    "We can define and **fit** such a straight line to our data following a straight line equation: **y = m * x + b** . Such a simple model would simply describes, a person's height  has almost a linear relationship with weight i.e. weight increases with height. \n",
    "<img src=\"https://blogs.sas.com/content/iml/files/2013/02/RegSlopeInt.png\" width = 400>. \n",
    "\n",
    "So this is our simple model for the relationship. Of course we can use more sophisticated models like quadratic equations or polynomial equations for a **better fit**, and we shall see that with advanced modeling techniques. Let's get back to our plain old straight line for now. \n",
    "\n",
    "Looking at this line above, we can define is as **Weight = -143 + 3.9 * Height**, based on slope(m) and intercept(c) values for **y = mx+ b**.  \n",
    "\n",
    "This would be our **model**, which can help us work out a weight value for a given height OR in some cases you may put to change the orientation of data and try to predict height based on an individual's weight. That's all got to do with the question you are trying to ask. \n",
    "\n",
    "> A model is expressed as a mathematical equation showing the relationship between dependent and independent variables.\n",
    "\n",
    "#### Model Parameters\n",
    "\n",
    "Every model Parameters are the co-efficients of the model equation for estimating the output. Statistical Learning is all about learning these parameters. A statistical learning approach would help us **learn** these parameters so we have a clear description of their relationship which we can replicate and analyze under different circumstances. \n",
    "\n",
    "#### Model Validation\n",
    "\n",
    "> Data is finite. \n",
    "\n",
    "The available data needs to be used very efficiently to build and **validate** a model. \n",
    "\n",
    "Here is a brief introduction to validation, in its simplest form:\n",
    "\n",
    "* Split the data into two parts.\n",
    "* Use one part for training so the model learns from it. This set of data is normally called the **Training Data**\n",
    "\n",
    "* Use the other part for testing the model. This is data is kept away from the model during learning process and used only for testing the performance of a learned model. This dataset is called as the **Testing Data.**\n",
    "\n",
    "This setup looks like as shown below:\n",
    "![](https://francisbrochu.github.io/microbiome-summer-school-2017_mass-spec/sections/machine_learning/figures/train_test_sets.png)\n",
    "\n",
    "In statistical learning, if the model has learned well from the training data, it will perform well on the test data and that would be our measure of accuracy. It is assessed based on how close it has estimated the output to the actual value.\n",
    "\n",
    "#### Loss Functions\n",
    "\n",
    ">A loss function method of evaluating how well your model represents the relationship between data variables. \n",
    "\n",
    "If the model can not figure out the underlying relationship between independent and dependent variable(s), the loss function outputs a higher number. If the relationship is well modeled, the loss function will be a lower value. As you change parameters of your model to try and improve results, your loss function is your best friend, telling you if you are on the right track. Below is an example loss function which calculates a loss for fitting straight line to set of variables (as in our case above). The function tries to measure the distance between data points and line to measure the level of LOSS. \n",
    "![](https://blog.algorithmia.com/wp-content/uploads/2018/04/word-image-5.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance\n",
    "\n",
    "    data.var()\n",
    "\n",
    "a measure of dispersion for continuous random variables from its expected mean value. Let's quickly revisit this, as variance formula plays a key role while calculating covariance and correlation measures.\n",
    "\n",
    "The formula for calculating variance as shown below:\n",
    "\n",
    "$$\\sigma^2 = \\dfrac{1}{n}\\displaystyle\\sum_{i=1}^{n}(x_i-\\mu)^2$$\n",
    "\n",
    "- $x$ represents an individual data points\n",
    "- $\\mu $ is the sample mean of the data points\n",
    "- $n$ is the total number of data points \n",
    "\n",
    "## Covariance\n",
    "\n",
    "    data.cov()\n",
    "\n",
    "In some cases, you'll want to look at **two random variables** to get an idea on how they **change together**. In statistics, when trying to figure out how two random variables **vary together**, you can use the so-called **covariance** between these variables.\n",
    "\n",
    "Covariance calculation plays a major role in a number of advanced machine learning algorithms like dimensionality reduction, predictive analyses, etc.\n",
    "\n",
    "### Calculating Covariance\n",
    "If you have $X$ and $Y$, two random variables having $n$ elements each. You can calculate covariance ($\\sigma_{xy}$) between these two variables by using the formula:\n",
    "\n",
    "$$\\sigma_{XY} = \\dfrac{1}{n}\\displaystyle\\sum_{i=1}^{n}(x_i -\\mu_x)(y_i - \\mu_y)$$\n",
    "\n",
    "- $\\sigma_{XY}$ = Covariance between $X$ and $Y$\n",
    "- $x_i$ = ith element of variable $X$\n",
    "- $y_i$ = ith element of variable $Y$\n",
    "- $n$ = number of data points (__$n$ must be same for $X$ and $Y$__)\n",
    "- $\\mu_x$ = mean of the independent variable $X$\n",
    "- $\\mu_y$ = mean of the dependent variable $Y$\n",
    "\n",
    "### Interpreting covariance values \n",
    "\n",
    "Covariance values range from positive infinity to negative infinity. \n",
    "\n",
    "* A **positive covariance** indicates that two variables are **positively related**\n",
    "\n",
    "* A **negative covariance** indicates that two variables are **inversely related**\n",
    "\n",
    "* A **covariance equal or close to 0** indicates that there is **no linear relationship** between two variables\n",
    "\n",
    "## Correlation\n",
    "\n",
    "    data.corr()\n",
    "\n",
    "Covariance uses a formulation that only depends on the units of $X$ and $Y$ variables. When doing data science, it is often not appropriate to use covariance as such because different experiments may contain underlying data measured in different units. \n",
    "\n",
    "Because of this, it is important to normalize this degree of variation into a standard unit, with interpretable results, *independent of the units of data*. You can achieve this with a derived normalized measure, called **correlation**. \n",
    "\n",
    "The term \"correlation\" refers to a relationship or association between variables. In almost any business, it is useful to express one quantity in terms of its relationship with others. For example: \n",
    "- Sales might increase when the marketing department spends more on TV advertisements\n",
    "- Customer's average purchase amount on an e-commerce website might depend on a number of factors related to that customer, e.g. location, age group, gender etc.\n",
    "- Social media activity and website clicks might be associated with revenue that a digital publisher makes. etc.\n",
    "\n",
    "Correlation is the first step to understanding these relationships and subsequently building better business and statistical models.\n",
    "\n",
    "\n",
    "### Pearson's Correlation Coefficient\n",
    "\n",
    "__Pearson Correlation Coefficient__, $r$, also called the __linear correlation coefficient__, measures the strength and the direction of a __linear relationship__ between two variables. This coefficient quantifies the degree to which a relationship between two variables can be described by a line. \n",
    "\n",
    "**Note:** There are a [number other correlation coefficients](https://math.tutorvista.com/statistics/correlation.html),  but for now, we will focus on __Pearson correlation__ as it is the go-to correlation measure for most needs.\n",
    "\n",
    "### Calculating Correlation Coefficient\n",
    "\n",
    "Pearson Correlation ($r$) is calculated using following formula :\n",
    "\n",
    "$$ r = \\frac{\\sum_{i=1}^{n}(x_i -\\mu_x)(y_i - \\mu_y)} {\\sqrt{\\sum_{i=1}^{n}(x_i - \\mu_x)^2 \\sum_{i=1}^{n}(y_i-\\mu_y)^2}}$$\n",
    "\n",
    "So just like in the case of covariance,  $X$ and $Y$ are two random variables having n elements each. \n",
    "\n",
    "\n",
    "- $x_i$ = ith element of variable $X$\n",
    "- $y_i$ = ith element of variable $Y$\n",
    "- $n$ = number of data points (__$n$ must be same for $X$ and $Y$__)\n",
    "- $\\mu_x$ = mean of the independent variable $X$\n",
    "- $\\mu_y$ = mean of the dependent variable $Y$\n",
    "- $r$ = Calculated Pearson Correlation\n",
    "\n",
    "### Interpreting Correlation values\n",
    "\n",
    "> _The Pearson Correlation formula always gives values in a range between -1 and 1_\n",
    "\n",
    "### Correlation is not Causation\n",
    "\n",
    "You may have come across the saying: **“correlation is not causation”** or **“correlation does not imply causation”**. \n",
    "\n",
    "What do we mean by saying this?\n",
    "\n",
    "Causation takes a step further than correlation.\n",
    "> Any change in the value of one variable will cause a change in the value of another variable, which means one variable causes the other one to happen. It is also referred to as __cause and effect__.\n",
    "\n",
    "### Key Takeaways\n",
    "- Correlation is dimensionless, i.e. it is a unit-free measure of the relationship between variables. \n",
    "- Correlation is a normalized form of covariance and exists between [0,1]\n",
    "- Correlation is not causation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Distribution\n",
    "\n",
    "A statistical distribution is a representation of the frequencies of potential events or the percentage of time each event occurs.\n",
    "\n",
    "#### Examples of Discrete Distributions\n",
    "\n",
    "##### The Bernoulli (Binomial) Distribution \n",
    "\n",
    "The Bernoulli distribution represents the probability of success for a certain experiment (the outcome being \"success or not\", so there are two possible outcomes). A coin toss is a classic example of a Bernoulli experiment with a probability of success 0.5 or 50%, but a Bernoulli experiment can have any probability of success between 0 and 1.\n",
    "\n",
    "##### The Poisson Distribution\n",
    "\n",
    "The Poisson distribution represents the probability of $n$ events in a given time period when the overall rate of occurrence is constant. A typical example is pieces of mail. If your overall mail received is constant, the number of items received on a single day (or month) follows a Poisson distribution. Other examples might include visitors arriving on a website, or customers arrive at a store, or clients waiting to be served in a queue.\n",
    "\n",
    "##### The Uniform Distribution\n",
    "\n",
    "The uniform distribution occurs when all possible outcomes are equally likely. The dice example shown before follows a uniform distribution with equal probabilities for throwing values from 1 to 6. The dice example follows a discrete uniform distribution, but continuous uniform distributions exist as well.\n",
    "\n",
    "#### Examples of Continuous Distributions\n",
    "\n",
    "##### The Normal or Gaussian distribution\n",
    "\n",
    "A normal distribution is the single most important distribution, you'll basically come across it very often. The normal distribution follows a bell shape and is a foundational distribution for many models and theories in statistics and data science. A normal distribution turns up very often when dealing with real world data including heights, weights of different people, errors in some measurement or marks on a test.\n",
    "\n",
    "#### Discrete vs Continuous Distributions\n",
    "\n",
    "When dealing with **discrete** data you use a **Probability Mass Function (PMF)**. When dealing with **continuous** data, you use a **Probability Density Function (PDF)**.\n",
    "\n",
    "Based on the variation of their attributes, data distributions can take many shapes and forms. In the next few lessons, you'll learn how to describe data distributions. Very often, distributions are described using their statistical mean (or **expected value**) and variance of the data, but this is not always the case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative binomial distribution and Geometric distribution\n",
    "\n",
    "The Binomial Distribution describes the number of successes  k  achieved in  n  trials, where the probability of success is  p .\n",
    "\n",
    "The Negative Binomial Distribution describes the number of successes  k  until observing  r  failures (or successes--this is arbitrary, and depends on how you phrase the question; it doesn't particularly matter if we define heads or tails as a failure, as long as we pick one). Note that these failures do not need to be consecutive, just cumulative!\n",
    "\n",
    "If we know the parameters, we can calculate our Negative Binomial Probability by pulling them into the following formula:\n",
    "\n",
    "$b(x, r, P) =\\  _{x-1}C_{\\ r-1} * P^{\\ r} * (1-P)^{\\ x-r}  $ \n",
    "\n",
    "$ r = no. of failures $\n",
    "\n",
    "$ x =  no. of trials$\n",
    "\n",
    "$ P = 0.5 probability (success/failure)$\n",
    "\n",
    "#### Characteristics of the Negative Binomial Distribution\n",
    "\n",
    "The **_mean_** of the Negative Binomial Distribution is: \n",
    "\n",
    "$$\\mu = \\frac{r}{p}$$\n",
    "\n",
    "The **_variance_** of the Negative Binomial Distribution is:\n",
    "\n",
    "$$\\sigma^2 = \\frac{r\\ (1-p)}{p^{\\ 2}}  $$\n",
    "\n",
    "#### Calculating Negative Binomial Probability with Numpy\n",
    "\n",
    "[numpy documenation for negative binomial sampling function](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.negative_binomial.html):\n",
    "\n",
    "> A company drills wild-cat oil exploration wells, each with an estimated probability of success of 0.1. What is the probability of having one success for each successive well, that is what is the probability of a single success after drilling 5 wells, after 6 wells, etc.?\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    s = np.random.negative_binomial(1, 0.1, 100000)\n",
    "    for i in range(1, 11):\n",
    "        probability = sum(s<i) / 1000000\n",
    "        print(\"{} wells drilled, probability of success: {:.4f}%\".format(i, probability * 100))\n",
    "        \n",
    "## Geometric Distribution\n",
    "\n",
    "The Geometric Distribution is extremely similar to the negative binomial distribution. Whereas  r  is a parameter we choose ourselves in the Negative Binomial Distribution, in the Geometric Distribution r is always equal to 1! In this way, any questions that we can answer with the Geometric Distribution are questions that we can also answer with the Negative Binomial Distribution. The Geomtric Distribution, then, is just a subset of the Negative Binomial Distribution where  r always equals 1.\n",
    "\n",
    "Whereas our previous example for the Negative Binomial Distribution was about how many times we could flip a coin before tails comes up twice, an equivalent question we could solve with the Geometric Distribution would be \"What is the probability that I can flip a coin X times before it lands on tails?\"\n",
    "\n",
    "#### Equation for the Geometric Distribution\n",
    "\n",
    "$$P(X=x) = q^{(x\\ -\\ 1)}p$$\n",
    "\n",
    "Where $$q = 1 - p$$\n",
    "\n",
    "\n",
    "$p$: The probability of failure for a given trial.\n",
    "\n",
    "$q$: (1 - p), which is the probability of success for a given trial.\n",
    "\n",
    "Note that in cases where there is an equal chance of both outcomes such as our coin flip example, p and q are the same thing.  This means that for \"fair\" trials where there is an equal chance of success or failure, we can further simplify our equation to $$P(X=x) = q^x$$\n",
    "\n",
    "#### Python function for geo dist\n",
    "\n",
    "    def geometric_dist(y,p):\n",
    "        \"\"\"y is a discrete random variable. It should be an integer that is greater then zero.\n",
    "        p is the probability of a success for the Bernoulli experiment to be conducted.\n",
    "        This function should return the probability that the first successful Bernoulli experiment will occur on the yth trial.\"\"\"\n",
    "        #This outline could further help students.\n",
    "        probability_failures_all_previous = (1-p)**(y-1)\n",
    "        probability_success_this_trial = p\n",
    "        overall_prob = probability_failures_all_previous * probability_success_this_trial\n",
    "        prob = overall_prob#The probability that the first successful bernoulli experiment occurs on the yth trial.\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Distribution\n",
    "\n",
    "The Poisson Distribution is yet another statistical distribution we can use to answer questions about the probability of a given number of successes, the probability of success and a series of independent trials. Specifically, the Poisson Distribution allows us to calculate the probability of a given event happening by examining the mean number of events that happen in a given time period. Given a set time period, we can use the Poisson Distribution to predict how many times a given event will happen over that time period. We are not given this probability--however, we know how likely an event is to occur the mean number of times over a given time period, which means that we actually **do** know the probability--we just need to do some basic calculations to discover this probability.\n",
    "\n",
    "#### Poisson Distribution Formula\n",
    "\n",
    "$$p(x) = \\frac{\\lambda^xe^{-\\lambda}}{x!}$$\n",
    "\n",
    "$\\lambda = mean # of units per unit of time$\n",
    "$e$: Euler's Constant, which is $e \\approx 2.71828$\n",
    "$x!$: The factorial of x.  For example, $3! = 3 * 2 * 1 = 6$ \n",
    "\n",
    "#### Python poisson function\n",
    "\n",
    "    import numpy as np\n",
    "    from math import factorial\n",
    "    def poisson_probability(lambd, x):\n",
    "        return ((lambd)**x * (np.exp(-lambd))) / factorial(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential Distribution\n",
    "\n",
    "The Exponential Distribution lets us ask how likely the length of an interval of time is before an event occurs exactly once. As with the other distributions we've learned about, our goal is to discover the probability that our **_Random Variable, $X$_** will turn out to be a specific value, $x$. \n",
    "\n",
    "In order to figure this out, we need to know the **_Decay Parameter_**, $\\lambda$ (although you may also see this denoted by the letter $m$).  To calculate the decay parameter, we just divide 1 by the average length of time it takes for an event to occur (e.g. the average number of minutes a customer interaction takes, or the average number of days before a machine breaks down). The average interval length is usually labeled as $\\mu$.\n",
    "\n",
    "#### Decay Rate Formula\n",
    "\n",
    "$$\\lambda = \\frac{1}{\\mu}$$\n",
    "\n",
    "Once we know the decay rate, we can use the **_Probability Density Function_** to tell us the **exact point probability for any length $x$.**\n",
    "\n",
    "$$PDF(x) = \\lambda e^{-\\lambda x}$$\n",
    "\n",
    "Since we are talking about a Continuously-valued function, we'll also often want to make use of the **_Cumulative Density Function_**.  This allows us to answer questions such as **what is the probability that it will take less than 4 minutes ring up this customer?\"**\n",
    "\n",
    "$$CDF(x) = 1 - e^{-\\lambda x}$$\n",
    "\n",
    "#### Python functions for exp dist.\n",
    "\n",
    "    import numpy as np\n",
    "    #solve for exact time\n",
    "    def exp_pdf(mu, x):\n",
    "        decay_rate = 1 / mu\n",
    "        return decay_rate * np.exp(-decay_rate * x)\n",
    "\n",
    "    #solve for a period of time\n",
    "    def exp_cdf(mu, x):\n",
    "        decay_rate = 1 / 4\n",
    "        return 1 - np.exp(-decay_rate * x)\n",
    "\n",
    "    print(\"Point robability for exactly 3 minutes: {:.4f}%\".format(exp_pdf(4, 3) * 100))\n",
    "    print(\"Cumulative probability of 3 minutes or less: {:.4f}%\".format(exp_cdf(4, 3) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distributions\n",
    "\n",
    "The normal distribution is the most important and most widely used distribution in statistics and analytics. It is also called the \"bell curve,\" due to its shape or the \"Gaussian curve\" after the mathematician Karl Friedrich Gauss.\n",
    "\n",
    "#### Measures of Center and Spread\n",
    "If you remember skewness, you would recognize there is no skew in a perfectly normal distribution. It is centered around its mean.\n",
    "\n",
    "There could possibly be many normal distributions based on how they are defined. Normal distributions can differ in their means and in their standard deviations.\n",
    "\n",
    "#### Normal Characteristics\n",
    "For now , we will identify normal distributions with following key characteristics.\n",
    "\n",
    "- Normal distributions are symmetric around their mean.\n",
    "- The mean, median, and mode of a normal distribution are equal.\n",
    "- The area under the bell curve is equal to 1.0.\n",
    "- Normal distributions are denser in the center and less dense in the tails.\n",
    "- Normal distributions are defined by two parameters, the mean (μ) and the standard deviation (σ).\n",
    "- Around 68% of the area of a normal distribution is within one standard deviation of the mean (μ - σ to μ + σ)\n",
    "- Approximately 95% of the area of a normal distribution is within two standard deviations of the mean ((μ - 2σ to μ + 2σ).\n",
    "\n",
    "#### Central limit theorem\n",
    "\n",
    "When we add large number of independent random variables, irrespective of the original distribution of these variables, their normalized sum tends towards a Gaussian distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Normal Distribution\n",
    "\n",
    "The standard normal distribution is a special case of the normal distribution. The Standard Normal Distribution is a normal distribution with a mean of 0, and a standard deviation of 1.\n",
    "\n",
    "Thinking back to the standard deviation rule for normal distributions:\n",
    "\n",
    "* $68\\%$ of the area lies in the interval of 1 standard deviation from the mean, or mathematically speaking, $68\\%$ is in the interval  $[\\mu-\\sigma, \\mu+\\sigma]$\n",
    "*  $95\\%$ of the area lies in the interval of 2 standard deviations from the mean, or mathematically speaking, $95\\%$ is in the interval  $[(\\mu-2\\sigma), (\\mu+2\\sigma)]$\n",
    "* $99\\%$ of the area lies in the interval of 3 standard deviations from the mean, or mathematically speaking, $99\\%$ is in the interval  $[(\\mu-3\\sigma), (\\mu+3\\sigma)]$\n",
    "\n",
    "\n",
    "With a $\\mu = 0$ and $\\sigma=1$, this means that for the standard normal distribution:\n",
    "\n",
    "* $68\\%$ of the area lies between -1 and 1.\n",
    "* $95\\%$ of the area lies between -2 and 2.\n",
    "* $99\\%$ of the area lies between -3 and 3.\n",
    "\n",
    "#### Standard Score (z-score)\n",
    "\n",
    "The standard score (more commonly referred to as a z-score) is a very useful statistic because it allows us to:\n",
    "1. Calculate the probability of a certain score occurring within a given normal distribution and \n",
    "2. Compare two scores that are from different normal distributions.\n",
    "\n",
    "Any normal distribution can be converted to a standard normal distribution and vice versa using this\n",
    "equation:\n",
    "\n",
    "$$\\Large z=\\dfrac{x-\\mu}{\\sigma}$$\n",
    "\n",
    "Here, $x$ is an observation from the original normal distribution, $\\mu$ is the mean and $\\sigma$ is the standard deviation of the original normal distribution. \n",
    "\n",
    "\n",
    "The standard normal distribution is sometimes called the $z$ distribution. A $z$ score always reflects the number of standard deviations above or below the mean. \n",
    "\n",
    "#### Data Standardization\n",
    "\n",
    "Data standardization is common data preprocessing skill, which is used to compare a number of observations belonging to different normal distributions, and having distinct means and standard deviations. \n",
    "\n",
    "Standardization applying a $z$ score calculation on each element of the distribution. The output of this process is a **z-distribution** or a **standard normal distribution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Mass Function\n",
    "\n",
    "A probability mass function (pmf), sometimes also called just a frequency function gives us probabilities for discrete random variables. We already know that discrete random variables from examples like coin flips and dice rolls etc. The **discrete** part in discrete distributions means that there is a known number of possible outcomes. For example, you can only roll a 1,2,3,4,5, or 6 on a die. **Based on our observations** of all the values from 1 to 6 in a number of dice rolls, we can develop a pmf for the dice showing the probability of each possible value occurring. \n",
    "\n",
    "Here is a more formal understanding:\n",
    "\n",
    "> There is a probability that a discrete random variable X takes on a particular value x, so that P(X = x), denoted as f(x). The function f(x) is typically called the probability mass function, or pmf. \n",
    "\n",
    "#### Function for calculating PMF using given collection data\n",
    "\n",
    "    import collections\n",
    "    x = [1,1,1,1,2,2,2,2,3,3,4,5,5]\n",
    "    counter = collections.Counter(x)\n",
    "    print(counter)\n",
    "    print (len(x))\n",
    "    \n",
    "    pmf = []\n",
    "    for key,val in counter.items():\n",
    "        pmf.append(round(val/len(x), 2))\n",
    "\n",
    "    print(counter.keys(), pmf)\n",
    "\n",
    "#### Class Size Paradox\n",
    "\n",
    "The class size paradox describes apparent contradictory findings where a total allocation of resources is fixed. \n",
    "\n",
    "The idea behind this paradox is that there is a difference in how events are actually distributed and how events are perceived to be distributed. These types of divergence can have important consequences for data analysis. \n",
    "\n",
    "During random surveys, for smaller class sizes, the probability of coming across a students is lower than the actual probability. For larger classes, the probability of coming across a student is much higher than actual probability. This explains why the paradox takes place!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Distribution Function\n",
    "\n",
    "The cdf is a function of x just like a pmf where x is any value that can possibly appear in given discrete distribution. To calculate cdf(x) for any value of x, we compute the fraction of values in the distribution less than or equal to x following the percentile intuition.\n",
    "\n",
    "Cdf is a **cumulative** function because it lets you find the probability by adding up the individual probabilities of all the outcomes included. For a die roll, when you want a 2 or less, you have 2 outcomes fulfilling this condition: 1 and 2, each with an individual probability of 1/6. Adding these up as 1/6 + 1/6 equals 2/6 or 1/3, which is the cumulative probability of a 2. \n",
    "\n",
    ">That's what cumulative means - its just adding up probabilities.\n",
    "\n",
    "#### CDF Function\n",
    "\n",
    "    def calculate_cdf(lst, X):\n",
    "    count = 0\n",
    "    for value in lst:\n",
    "        if value <= X:\n",
    "            count += 1\n",
    "\n",
    "    cum_prob = count / len(lst) # normalizing cumulative probabilities (as with pmfs)\n",
    "    return round(cum_prob, 3)\n",
    "\n",
    "    #test data\n",
    "    test_lst = [1,2,3]\n",
    "    test_X = 2\n",
    "\n",
    "    calculate_cdf(test_lst, test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Density Function\n",
    "\n",
    "A Probability Density Function (pdf) helps identify the regions in the distribution where observations are more likely to occur i.e. it is more dense. Remember that while dealing with pmfs, we calculated the mass for each class. For the case of continuous variable, we do not have fixed number of possible outcomes as described above so instead we create a density function.\n",
    "\n",
    "> **pdf is the probability function F(x), such that x falls between two values (a and b), is equals to the integral (area under the curve) from a to b**\n",
    "\n",
    "$$f(x\\ |\\ \\mu,\\ \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}e$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$x$ is the **_point_** we want to calculate the probability for\n",
    "\n",
    "$\\mu$ is the **_mean_** of the sample\n",
    "\n",
    "$\\pi$ is a mathematical constant, the irrational number $3.14159$\n",
    "\n",
    "$\\sigma^2$ is the **_variance_** (since $\\sigma$ is the **_standard deviation_**)\n",
    "\n",
    "$e$ is **_Euler's Constant_**, also known as the **_Base of the Natural Logarithm_**, $2.71828$\n",
    "\n",
    "#### PDF function (provides coordinates for graphing) \n",
    "\n",
    "    def density(x):\n",
    "\n",
    "        n, bins = np.histogram(x, 10, density=1)\n",
    "        # Initialize numpy arrays with zeros to store interpolated values\n",
    "        pdfx = np.zeros(n.size)\n",
    "        pdfy = np.zeros(n.size)\n",
    "\n",
    "        # Interpolate through histogram bins \n",
    "        # identify middle point between two neighbouring bins, in terms of x and y coords\n",
    "        for k in range(n.size):\n",
    "            pdfx[k] = 0.5*(bins[k]+bins[k+1])\n",
    "            pdfy[k] = n[k]\n",
    "\n",
    "        # plot the calculated curve\n",
    "        return pdfx, pdfy\n",
    "\n",
    "\n",
    "    #Generate test data and test the function\n",
    "    np.random.seed(5)\n",
    "    mu, sigma = 0, 0.1 # mean and standard deviation\n",
    "    s = np.random.normal(mu, sigma, 100)\n",
    "    x,y = density(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness and kurtosis\n",
    "\n",
    "### Skewness\n",
    "\n",
    "Skewness is the degree of distortion or deviation from the symmetrical normal distribution. Skewness can be seen as a measure to calculate the lack of symmetry in the data distribution.\n",
    "\n",
    "Skewness helps you identify extreme values in one of the tails. Symmetrical distribution has a skewness of 0. \n",
    "\n",
    "Distributions can be **positively** or **negatively** skewed.\n",
    "\n",
    "##### Positive Skewness\n",
    "\n",
    "A distribution is **positively skewed** when the tail on the right side of the distribution is longer (also often called \"fatter\"). When there is positive skewness, the mean and median are bigger than the mode.\n",
    "\n",
    "##### Negative Skewness\n",
    "\n",
    "Distributions are **negatively skewed** when the tail on the left side of the distribution is longer or fatter than the tail on the right side. The mean and median are smaller than the mode.\n",
    "\n",
    "##### Measuring Skewness\n",
    "\n",
    "For univariate data $Y_1, Y_2, ..., Y_n$ the formula for skewness is:\n",
    "\n",
    "$$\\dfrac{\\dfrac{\\displaystyle\\sum^n_{i=1}(Y_i-Y)^3}{n}}{s^3}$$\n",
    "\n",
    "where $Y$ is the mean, $s$ is the standard deviation, and $n$ is the number of data points. This formula for skewness is referred to as the **Fisher-Pearson coefficient of skewness**. There are also other ways to calculate skewness, yet this one is the one that is used most commonly.\n",
    "\n",
    "##### Using this formula, when is data skewed?\n",
    "\n",
    "The rule of thumb seems to be:\n",
    "\n",
    "* A skewness between -0.5 and 0.5 means that the data are pretty symmetrical\n",
    "* A skewness between -1 and -0.5 (negatively skewed) or between 0.5 and 1 (positively skewed) means that the data are moderately skewed.\n",
    "* A skewness smaller than -1 (negatively skewed) or bigger than 1 (positively skewed) means that the data are highly skewed.\n",
    "\n",
    "### Kurtosis\n",
    "\n",
    "Kurtosis deals with the lengths of tails in the distribution. \n",
    "\n",
    "> **Where skewness talks about extreme values in one tail versus the other, kurtosis aims at identifying extreme values in both tails at the same time!**\n",
    "\n",
    "You can think of Kurtosis as a **measure of outliers** present in the distribution.\n",
    "\n",
    "##### Measuring Kurtosis\n",
    "\n",
    "For univariate data $Y_1, Y_2, \\dots, Y_n$ the formula for kurtosis is:\n",
    "\n",
    "$$\\dfrac{\\dfrac{\\displaystyle\\sum^n_{i=1}(Y_i-Y)^4}{n}}{s^4}$$\n",
    "\n",
    "If there is a high kurtosis, then you may want to investigate why there are so many outliers. \n",
    "Presence of outliers could be indications of errors on the one hand, but they could also be some interesting observations that may need to be explored further. For banking transactions, for example, an outlier may signify a fraudulent activity. How we deal with outliers mainly depends on the domain. \n",
    "\n",
    "Low kurtosis in a data set is an indication that data has light tails or lack of outliers. If we get low kurtosis, then also we need to investigate and trim the dataset of unwanted results.\n",
    "\n",
    "#### How much kurtosis is bad kurtosis?\n",
    "\n",
    "##### Mesokurtic ($\\text{kurtosis}  \\approx 3 $)\n",
    "\n",
    "A mesokurtic distribution has kurtosis statistics that lie close to the ones of a normal distribution. Mesokurtic distributions have a kurtosis of around 3.\n",
    "According to this definition, the standard normal distribution has a kurtosis of three.\n",
    "\n",
    "##### Platykurtic ($\\text{kurtosis} < 3 $):\n",
    "\n",
    "When a distribution is platykurtic, the distribution is shorter and tails are thinner than the normal distribution. The peak is lower and broader than Mesokurtic, which means that the tails are light, and that there are fewer outliers than in a normal distribution. \n",
    "\n",
    "##### Leptokurtic ($\\text{kurtosis}  > 3 $)\n",
    "\n",
    "When you have a leptokurtic distribution, you have a distribution with longer and fatter tails. The peak is higher and sharper than the peak of a normal distribution, which means that data have heavy tails, and that there are more outliers. \n",
    "\n",
    "Outliers stretch your horizontal axis of the distribution, which means that the majority of the data appear in a narrower vertical range. This is why the leptokurtic distribution looks \"skinny\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical testing\n",
    "\n",
    "##### Link to several tests and their descriptions and uses\n",
    "\n",
    "https://stats.idre.ucla.edu/spss/whatstat/what-statistical-analysis-should-i-usestatistical-analyses-using-spss/\n",
    "\n",
    "### Statistical significance\n",
    "Statistical significance is one of those terms that is often used when someone claims that some data collection and analysis proves a point (or hypothesis). The terminology associated with statistical significance is usually not well understood and results are accepted by masses, however, it is a simple idea that can be understood fairly easily. \n",
    "\n",
    "Statistical significance is mainly developed using samples and populations, hypothesis testing, the normal distribution, and p values.\n",
    "\n",
    "### Population vs sample\n",
    "The first step of every statistical analysis you will perform is the population vs sample check or to determine whether the data you are dealing with is a population or a sample.\n",
    "\n",
    "A **population** is the collection of all items of interest to our study and is usually denoted with an uppercase N. The numbers we’ve obtained when using a population are called parameters.\n",
    "\n",
    "A **sample** is a subset of the population and is denoted with a lowercase n, and the numbers we’ve obtained when working with a sample are called statistics.\n",
    "\n",
    "#### One Sample z-Test\n",
    "The one-sample Z test is used when we want to know whether our sample comes from a particular population. Z-scores lets us ask go from \"how far is a value from the mean\" to \"how likely is a value this far from the mean to be from the same group of observations?\". So once again, moving from stats to probability (likelihood is measured as probabilities).\n",
    "\n",
    "A one-sample z-statistic is calculated as:\n",
    "\n",
    "$$ \\large \\text{z-statistic} = \\dfrac{\\bar x - \\mu_0}{{\\sigma}/{\\sqrt{n}}} $$\n",
    "\n",
    "    #calculate z-score\n",
    "    import scipy.stats as stats\n",
    "    from math import sqrt\n",
    "    x_bar =  # sample mean \n",
    "    n =  # sample total\n",
    "    sigma = # sd of population\n",
    "    mu = # Population mean \n",
    "\n",
    "    z = (x_bar - mu)/(sigma/sqrt(n))\n",
    "    z\n",
    "\n",
    "    # Z-table in python \n",
    "\n",
    "    # Probabilities up to z-score\n",
    "    print(stats.norm.cdf(z_score))\n",
    "    # p-value\n",
    "    print (1-stats.norm.cdf(1.5))\n",
    "    \n",
    "\n",
    "\n",
    "#### P-value\n",
    "\n",
    "Statistical summary of the compatibility between the observed data and what we would predict or expect to see if we knew the entire statistical model were correct. **above formula includes solving for p-value**\n",
    "\n",
    "#### Significance Threshold (Alpha)\n",
    "The significance level, also denoted as alpha or α, is the probability of rejecting the null hypothesis when it is true. For example, a significance level of 0.05 indicates a 5% risk of concluding that a difference exists when there is no actual difference.Thresholds have been typically set at the .05 and .01 levels.\n",
    "\n",
    "#### Standard Error\n",
    "\n",
    "The standard error(SE) is very similar to standard deviation. Both are measures of spread. The higher the number, the more spread out your data is. To put it simply, the two terms are essentially equal — but there is one important difference. While the standard error uses statistics (sample data) standard deviations use parameters (population data). We achieve this dividing the standard deviation by the square root of the sample size.\n",
    "\n",
    "$$ \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{s}{\\sqrt{n}}$$\n",
    "\n",
    "$\\sigma$ is the population standard deviation\n",
    "$n$ is the sample size\n",
    "    \n",
    "##### python code for standrad error\n",
    "    # Calculate the standard error by dividing sample means with square root of sample size\n",
    "    err = round(np.std(means)/np.sqrt(n), 2)\n",
    "    \n",
    "#### Confidence Interval\n",
    "\n",
    "A Confidence Interval is a range of values above and below the point estimate that captures the true population parameter at some predetermined confidence level. If we want to have a 95% chance of capturing the true population parameter with a point estimate and a corresponding confidence interval, we would set confidence level to 95%. Higher confidence levels result in a wider confidence intervals.\n",
    "\n",
    "We calculate a confidence interval by taking a point estimate and then adding and subtracting a **margin of error** to create a range. Margin of error is based on your desired confidence level, the spread of the data and the size of your sample. The way you calculate the margin of error depends on whether you know the standard deviation of the population or not.\n",
    "\n",
    "the margin of error for a known population stadard deviation is:\n",
    "\n",
    "**Margin of Error = z ∗ σ / √n**\n",
    "\n",
    "Where σ (sigma) is the population standard deviation, n is sample size, and z is a number known as the z-critical value.\n",
    "\n",
    "The z-critical value is the number of standard deviations you'd have to go from the mean of the normal distribution to capture the proportion of the data associated with the desired confidence level.\n",
    "\n",
    "##### function to calculate z-critical value, confidence interval and margin of error between population and sample \n",
    "\n",
    "    def conf_interval(pop, sample):\n",
    "        '''\n",
    "        Function input: population , sample \n",
    "        Function output: z-critical, Margin of error, Confidence interval\n",
    "        '''\n",
    "        sample_size = 500\n",
    "        n = len(sample)\n",
    "        x_hat = sample.mean()\n",
    "\n",
    "        # Calculate the z-critical value using stats.norm.ppf()\n",
    "        # Note that we use stats.norm.ppf(q = 0.975) to get the desired z-critical value \n",
    "        # instead of q = 0.95 because the distribution has two tails.\n",
    "        z = stats.norm.ppf(q = 0.975)  #  z-critical value for 95% confidence\n",
    "\n",
    "        #Calculate the population std from data\n",
    "        pop_stdev = population_ages.std()\n",
    "\n",
    "        # Calculate the margin of error using formula given above\n",
    "        moe = z * (pop_stdev/math.sqrt(sample_size))\n",
    "\n",
    "        # Calculate the confidence interval by applying margin of error to sample mean \n",
    "        # (mean - margin of error, mean+ margin of error)\n",
    "        conf = (x_hat - moe,x_hat + moe)\n",
    "\n",
    "        return z, moe, conf\n",
    "\n",
    "    #Call above function with sample and population \n",
    "    z_critical, margin_of_error, confidence_interval = conf_interval(population, sample)    \n",
    "\n",
    "    print(\"Z-critical value:\")              \n",
    "    print(z_critical)         \n",
    "    print ('\\nMargin of error')\n",
    "    print(margin_of_error)\n",
    "    print(\"\\nConfidence interval:\")\n",
    "    print(confidence_interval)\n",
    "    \n",
    "#### Confidence Intervals with T-distribution\n",
    "T distributions are similar to the normal distribution in shape, but have heavier tails. T distributions also have a parameter known as degrees of freedom. The higher the degrees of freedom, the closer the distribution resembles that of the normal distribution.\n",
    "\n",
    "##### function to run confidence interval w/o using population mean via t-critical-value\n",
    "\n",
    "    stats.t.interval(alpha = 0.95,              # Confidence level\n",
    "                     df= 24,                    # Degrees of freedom (sample size - 1)\n",
    "                     loc = sample_mean,         # Sample mean\n",
    "                     scale = sigma)             # Standard deviation estimate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
