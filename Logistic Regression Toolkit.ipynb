{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from sklearn.linear_model import LogisticRegression\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "#### Load packages\n",
    "#### Obtain data\n",
    "#### Scrub and Explore (one hot encode categorical, normalize, scale)\n",
    "    # set test,train data\n",
    "    X_train, X_test, y_train, y_test = train_test_split (X,y, random_state = 0)\n",
    "    # instantiate model\n",
    "    logreg = LogisticRegression(C=1000000000000.0, class_weight=None, dual=False,\n",
    "          fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
    "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n",
    "    # fit data\n",
    "    model_log = logreg.fit(X_train, y_train)\n",
    "    model_log\n",
    "###### perform evaluations (ROC/AUC, confusion matrix, classification report, Recursive feature selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression from Scratch\n",
    "\n",
    "    def predict_y(X, w):\n",
    "        return np.dot(X,w)\n",
    "    import numpy as np\n",
    "    # need sigmoid to transform linear to logistic regression\n",
    "    def sigmoid(x):\n",
    "        x = np.array(x)\n",
    "        return 1/(1 + np.e**(-1*x))\n",
    "    # use gradient descent to minimize loss\n",
    "    def grad_desc(X, y, max_iterations, alpha, initial_weights=None):\n",
    "    \"\"\"Be sure to set default behavior for the initial_weights parameter.\"\"\"\n",
    "    if initial_weights == None:\n",
    "        initial_weights = np.ones((X.shape[1],1)).flatten()\n",
    "    weights_col= pd.DataFrame(initial_weights)\n",
    "    weights = initial_weights\n",
    "    #Create a for loop of iterations\n",
    "    for iteration in range(max_iterations):\n",
    "        #Generate predictions using the current feature weights\n",
    "        predictions = sigmoid(np.dot(X,weights))\n",
    "        #Calculate an error vector based on these initial predictions and the correct labels\n",
    "        error_vector = y - predictions\n",
    "        #Calculate the gradient \n",
    "        #As we saw in the previous lab, calculating the gradient is often the most difficult task.\n",
    "        #Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n",
    "        #For more details on the derivation, see the additional resources section below.\n",
    "        gradient = np.dot(X.transpose(),error_vector)\n",
    "        #Update the weight vector take a step of alpha in direction of gradient \n",
    "        weights += alpha * gradient\n",
    "        weights_col = pd.concat([weights_col, pd.DataFrame(weights)], axis=1)\n",
    "    #Return finalized Weights\n",
    "    return weights, weights_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
