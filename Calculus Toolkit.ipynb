{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative Basics\n",
    "The change of rate at an instantaneous point:\n",
    "\n",
    "$$ f'(x) = \\lim_{ h\\to0} \\frac{f(x + h) - f(x)}{h} $$ \n",
    "\n",
    "### Finding Minima/Maxima\n",
    "\n",
    "$f(x) = 2x^2-8x$\n",
    "\n",
    "Then, we know that\n",
    "\n",
    "$f'(x) = 4 x - 8 $\n",
    "\n",
    "We know that $f(x)$ reaches an optimum (in this case, a minimum) for $f'(x) = 0$\n",
    "\n",
    "So, we need to solve for $x$ as follows:\n",
    "\n",
    "$$4x - 8 = 0 $$\n",
    "$$ 4x = 8$$\n",
    "$$ x= 2$$\n",
    "\n",
    "And this is exactly where $f(x)$ reaches the minimum!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative Rules\n",
    "\n",
    "### The power rule\n",
    "\n",
    "The first rule for us to learn is the power rule.  The power rule is expressed as the following.  Given the following:\n",
    "\n",
    "$$f(x) = x^r $$\n",
    "\n",
    "Then, the derivative is: \n",
    "$$ f'(x) = r*x^{r-1} $$\n",
    "\n",
    "This says that if a variable, $x$, is raised to a exponent $r$, then the derivative of that function is the exponent $r$ multiplied by the variable, with the variable raised to the original exponent minus one. \n",
    "\n",
    "### The constant factor rule\n",
    "The constant factor addresses how to take the derivative of a function multiplied by a constant. The rule simply says if a variable is multiplied by a constant (i.e. a number), then to take the derivative of that term, apply our familiar power rule to the variable and multiply the variable by that same constant.\n",
    "\n",
    "So given the function: \n",
    "\n",
    "$$f(x) = 2x^2 $$\n",
    "\n",
    "\n",
    "$$f'(x) = 2*\\frac{\\Delta f}{\\Delta x} x^{2} = 2*2*x^{2-1} = 4x^1 = 4x $$\n",
    "\n",
    "### The addition rule\n",
    "\n",
    "To take a derivative of a function that has multiple terms, simply take the derivative of each of the terms individually.  So for the function above, \n",
    "\n",
    "$$ f(x) = 4x^3 - x^2 + 3x $$\n",
    "\n",
    "$$ f'(x) = 12x^2 - 2x + 3  $$  \n",
    "\n",
    "### The chain rule\n",
    "\n",
    "> ** The chain rule**: In taking the derivative, $\\frac{\\Delta f}{\\Delta x}$ of an outer function, $f(x)$, which depends on an inner function $g(x)$, which depends on $x$, the derivative equals the derivative of the outer function times the derivative of the inner function.\n",
    "\n",
    "### 1. Separate the function into two functions\n",
    "\n",
    "Remember we started with the function $f(x) = (0.5x + 3)^2 $.  Then we used functional composition to split this into two.\n",
    "\n",
    "$$g(x) = (0.5x + 3)$$\n",
    "$$f(x) = (g(x))^2$$\n",
    "\n",
    "### 2. Find the derivatives, $f'(x)$ and $g'(x)$\n",
    "\n",
    "* as we know $g'(x) = 0.5$\n",
    "* and $f'(g(x)) = 2*(g(x))^{1} = 2*g(x)$\n",
    "\n",
    "### 3. Substitute into our chain rule\n",
    "\n",
    "We have: \n",
    "* $ f'(g(x)) = f'g(x)*g'(x) = 2*g(x)*0.5 = 1*g(x)$\n",
    "\n",
    "Then substituting for $g(x)$, which we already defined, we have: \n",
    "\n",
    "$f'(g(x)) = g(x) = (0.5x + 3)$\n",
    "\n",
    "So the derivative of the function $f(x) = (0.5x + 3)^2 $ is $f'(x) = (0.5x + 3) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "The regression line that produces the lowest RSS for a given dataset is called the \"best fit\" line for that dataset. This is the basis for gradient descent, iterating through each regression possibility until the best fit for the next \"step\" is found. Then the process repeats for the next step and so on until the minima has been reached.\n",
    "\n",
    "#### Cost curve and step size\n",
    "\n",
    "The cost function is the function to which you are applying the gradient descent algorithm. The function if viewing from its side, looks like a curve. The goal is to reach the minimum point on the curve. Therefore, steps are taken in order to reach the minimum.\n",
    "* If the slope tilts downwards, then we should walk forward to approach the minimum.  \n",
    "* And if the slope tilts upwards, then we should point walk backwards to approach the minimum.  \n",
    "* The steeper the tilt, the further away we are from our cost curve's minimum, so we should take a larger step.  \n",
    "\n",
    "So by looking to the tilt of a cost curve at a given point, we can discover the direction of our next step and how large of step to take.  The beauty of this, is that as our regression lines become more complicated, we need not plot all of the values of our regression line.  We can see the next variation of the regression line to study simply by looking at the slope of the cost curve.\n",
    "\n",
    "We use the following procedure to find the ideal $m$: \n",
    "1.  Randomly choose a value of $m$, and \n",
    "2.  Update $m$ with the formula $ m = (-.02) * slope_{m = i} + m_i$.\n",
    "\n",
    "The formula above tells us which $m$ value to look at next. We start by choosing a random $m$ value that we can plug into our formula. We take the slope of the curve at that $m$ value and multiply it by $-.02$ then add it to our original $m$ value to produce our next $m$ value.\n",
    "\n",
    "As we can surmise, the larger the slope, the larger the resulting step to the next $m$ value.\n",
    "\n",
    "Here's an example.  We randomly choose a $m$ value of 42.  Then:\n",
    "\n",
    "* $m_{t=0} = 42 $\n",
    "* $m_{t=1} = (-.02) * -239  + 42 = 4.78 + 42 = 46.78 $\n",
    "* $m_{t=2} = (-.02) * -129.4 + 46.8 = 2.588 + 46.8 = 49.388 $\n",
    "\n",
    "* $m_{t=3} = (-.02) * -60.4 + 49.4 = 1.208 + 49.4 = 50.61  $\n",
    "\n",
    "#### Learning Rate\n",
    "The reason we multiply the slope by a fraction like .02 is so that we avoid the risk of overshooting the minimum. This fraction is called the learning rate. Here, the fraction is negative because we always want to move in the opposite direction of the slope. When the slope of the cost curve points downwards, we want to move to a higher  mm  slope for our linear regression problem. Conversely, when we are on the right side of the curve and the slope is rising, we want to move backwards to a lower y-intercept.\n",
    "\n",
    "#### Multivariate and partial derivatives\n",
    "In multiple dimensions, we once again choose an initial regression line, which means that we are choosing a point and begin taking steps towards the minimum.  But of course, we are now able to walk not just forwards and backwards but several directions as well. **Partial Derivatives** measure the slope in each dimension, one after the other.\n",
    "\n",
    "For any multivariable function, the variables that you are **not** taking the derivative with respect to, can just be treated as a constant. For example, with our function of $f(x, y) = y*x^2 $, when taking the partial derivative $\\frac{df}{dy}f(x, y)$, we treat all values of $x$ as a constant.  Let's do it:\n",
    "\n",
    "\n",
    "$$\\frac{df}{dy}f(x,y) =  \\frac{df}{dy}(y) * x^2 = 1*x^2 = x^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
