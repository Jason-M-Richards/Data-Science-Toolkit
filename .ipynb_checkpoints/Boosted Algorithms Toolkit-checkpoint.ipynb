{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "###### Numpy, Pd, Matplotlib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "##### Read data, display head\n",
    "df = pd.read_csv('some_file')\n",
    "\n",
    "df.head()\n",
    "\n",
    "##### Cleaning, Exploration, and Preprocessing\n",
    "###### set target and feature sets\n",
    "target = df.target\n",
    "\n",
    "df.drop('target', axis=1, inplace=True)\n",
    "###### visulaize data\n",
    "target.hist()\n",
    "###### check for nulls\n",
    "df.isna().sum()\n",
    "###### scale data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "scaled_df.head()\n",
    "##### set train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_df, target, test_size=0.25)\n",
    "\n",
    "#### Training\n",
    "###### Create an AdaBoostClassifier\n",
    "adaboost_clf = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
    "          learning_rate=1.0, n_estimators=50, random_state=None)\n",
    "###### fit data\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "###### get prediction values\n",
    "adaboost_train_preds = adaboost_clf.predict(X_train)\n",
    "\n",
    "adaboost_test_preds = adaboost_clf.predict(X_test)\n",
    "###### check confusion matrix and classification report\n",
    "adaboost_confusion_matrix = confusion_matrix(y_test, adaboost_test_preds)\n",
    "\n",
    "adaboost_confusion_matrix\n",
    "\n",
    "adaboost_classification_report = classification_report(y_test, adaboost_test_preds)\n",
    "\n",
    "print(adaboost_classification_report)\n",
    "###### run cross validation score\n",
    "print('Mean Adaboost Cross-Val Score (k=5):')\n",
    "\n",
    "print(cross_val_score(adaboost_clf, scaled_df, target, cv=5).mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost\n",
    "###### Numpy, Pd, Matplotlib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import GradientBoostClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "##### Read data, display head\n",
    "df = pd.read_csv('some_file')\n",
    "\n",
    "df.head()\n",
    "\n",
    "##### Cleaning, Exploration, and Preprocessing\n",
    "###### set target and feature sets\n",
    "target = df.target\n",
    "\n",
    "df.drop('target', axis=1, inplace=True)\n",
    "###### visulaize data\n",
    "target.hist()\n",
    "###### check for nulls\n",
    "df.isna().sum()\n",
    "###### scale data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "scaled_df.head()\n",
    "##### set train, test\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_df, target, test_size=0.25)\n",
    "\n",
    "###### create GBclassifier with parameters\n",
    "gbt_clf = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=1, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "              n_iter_no_change=None, presort='auto', random_state=None,\n",
    "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
    "              verbose=0, warm_start=False)\n",
    "###### fit data\n",
    "gbt_clf.fit(X_train, y_train)\n",
    "###### create predictions\n",
    "gbt_clf_train_preds = gbt_clf.predict(X_train)\n",
    "\n",
    "gbt_clf_test_preds = gbt_clf.predict(X_test)\n",
    "###### create confusion matrix and classification report\n",
    "gbt_confusion_matrix = confusion_matrix(y_test, gbt_clf_test_preds)\n",
    "\n",
    "gbt_confusion_matrix\n",
    "\n",
    "gbt_classification_report = classification_report(y_test, gbt_clf_test_preds)\n",
    "\n",
    "print(gbt_classification_report)\n",
    "###### cross validation score\n",
    "print('Mean GBT Cross-Val Score (k=5):')\n",
    "\n",
    "print(cross_val_score(gbt_clf, scaled_df, target, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "- originally written in C++\n",
    "- algorithm is parallelizable for multicore machines\n",
    "- consistently outperforms single-algorithm methods\n",
    "- is an ensemble algorithm\n",
    "\n",
    "#### When to Use\n",
    "- large number of training samples\n",
    "    - >1000 samples < 100 features\n",
    "- mixture of categorical and numeric or just numeric features\n",
    "\n",
    "#### When Not To Use\n",
    "- image recognition\n",
    "- computer vision\n",
    "- NLP\n",
    "- small training sets <100\n",
    "\n",
    "#### Classification\n",
    "- reg:logistic - want a decision\n",
    "- binary:logistic - want a probability\n",
    "##### Decision Tree\n",
    "- uses a Classification and Regression Tree (CART)\n",
    "##### Boosting\n",
    "- meta algorithm\n",
    "- converts weak learners into strong learners:\n",
    "    - iteratively learns a set of weak learners on subsets of data\n",
    "    - weighs each weak prediction\n",
    "    - combines weighted predictions\n",
    "    \n",
    "#### Regression\n",
    "- using a loss function\n",
    "    - reg:squarederror - use for regression problems\n",
    "    - data is split with train_test_split\n",
    "- using base learners\n",
    "    - need to convert train and test sets into their own variables and use the parameters dictionary\n",
    "    \n",
    "            # Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "            DM_train = xgb.DMatrix(X_train, y_train)\n",
    "            DM_test =  xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "            # Create the parameter dictionary: params\n",
    "            params = {\"objective\":\"reg:linear\", \"booster\":\"gblinear\"}\n",
    "\n",
    "            # Train the model: xg_reg\n",
    "            xg_reg = xgb.train(dtrain=DM_train, params=params, num_boost_round=5)\n",
    "\n",
    "            # Predict the labels of the test set: preds\n",
    "            preds = xg_reg.predict(DM_test)\n",
    "            \n",
    "#### Regularization methods\n",
    "- gamma - minimum loss reduction allowed for a split to occur\n",
    "- alpha (L1) - on leaf weight\n",
    "- lambda (L2) - on leaf weights\n",
    "\n",
    "#### DMatrix\n",
    "- optimized data framework for use with the algorithm\n",
    "\n",
    "        xgb.DMatrix(data=data, label=target)\n",
    "        \n",
    "#### xgb.cv\n",
    "- runs cross validation check on model\n",
    "- needs dictionary parameter inputs to specify model details\n",
    "\n",
    "        # Create the parameter dictionary: params\n",
    "        params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "        # Perform cross-validation: cv_results\n",
    "        cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                          nfold=3, num_boost_round=5, \n",
    "                          metrics=\"error\", as_pandas=True, seed=123)\n",
    "                          \n",
    "#### Visulalizing in XGBoost\n",
    "##### xgb.plot_trees(booster_name, num_trees=, rankdir=' ')\n",
    "- will plot the tree decisions\n",
    "##### xgb.plot_importance()\n",
    "- will plot feature importance in ranked order\n",
    "\n",
    "#### Tunable Parameters\n",
    "- learning rate\n",
    "- regularizers\n",
    "- max_depth\n",
    "- subsample - % samples for trees\n",
    "- colsample_bytree - % features per tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Sample XGBoost Workflow\n",
    " \n",
    "      import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    import xgboost as xgb\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "#### Read data, display head\n",
    "    df = pd.read_csv('some_file')\n",
    "\n",
    "    df.head()\n",
    "\n",
    "#### Cleaning, Exploration, and Preprocessing\n",
    "    # set target and feature sets\n",
    "    target = df.target\n",
    "\n",
    "    df.drop('target', axis=1, inplace=True)\n",
    "    # visulaize data\n",
    "    target.hist()\n",
    "    # check for nulls\n",
    "    df.isna().sum()\n",
    "    # scale data\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    scaled_df.head()\n",
    "    # set train, test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_df, target, test_size=0.25)\n",
    "    # create xgb instance with parameters\n",
    "    clf = xgb.XGBClassifier()\n",
    "    # fit data\n",
    "    clf.fit(X_train, y_train)\n",
    "    # make predictions\n",
    "    training_preds = clf.predict(X_train)\n",
    "\n",
    "    val_preds = clf.predict(X_test)\n",
    "    # run classification report and confusion matrix\n",
    "    gbt_confusion_matrix = confusion_matrix(y_test, val_preds)\n",
    "\n",
    "    gbt_confusion_matrix\n",
    "\n",
    "    gbt_classification_report = classification_report(y_test, val_preds)\n",
    "\n",
    "    print(gbt_classification_report)\n",
    "    # cross validation score\n",
    "    print('Mean XGB Cross-Val Score (k=5):')\n",
    "\n",
    "    print(cross_val_score(clf, scaled_df, target, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
