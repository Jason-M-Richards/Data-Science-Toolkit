{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "It is pretty straightforward that, to evaluate our model, you'll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train test split, you'll compare your residuals for both test set and training set:\n",
    "\n",
    "$r_{i,train} = y_{i,train} - \\hat y_{i,train}$ \n",
    "\n",
    "$r_{i,test} = y_{i,test} - \\hat y_{i,test}$ \n",
    "\n",
    "To get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\n",
    "\n",
    "RMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\n",
    "\n",
    "MSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\n",
    "\n",
    "Again, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting.\n",
    "\n",
    "#### R squared\n",
    "\n",
    "    r2 = model.score(X,y)\n",
    "    print(r2)\n",
    "    \n",
    "\n",
    "\n",
    "#### Mean Squared Error for test and train \n",
    "\n",
    "    train_residuals = y_hat_train - y_train\n",
    "    test_residuals = y_hat_test - y_test\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    train_mse = mean_squared_error(y_train, y_hat_train)\n",
    "    test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "    print('Train Mean Squared Error:', train_mse)\n",
    "    print('Test Mean Squared Error:', test_mse)\n",
    "    \n",
    "#### Mean Absolute Error\n",
    "    \n",
    "    train_residuals = y_hat_train - y_train\n",
    "    test_residuals = y_hat_test - y_test\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    train_mse = mean_squared_error(y_train, y_hat_train)\n",
    "    test_mse = mean_absolute_error(y_test, y_hat_test)\n",
    "    print('Train Mean Absolute Error:', train_mse)\n",
    "    print('Test Mean Absolute Error:', test_mse)\n",
    "\n",
    "    \n",
    "#### K-Fold Cross ValidationÂ¶\n",
    "K-Fold Cross Validation expands on the idea of training and testing splits by splitting the entire dataset into {K} equal sections of data. We'll then iteratively train {K} linear regression models on the data, with each linear model using a different section of data as the testing set, and all other sections combined as the training set.\n",
    "\n",
    "We can then average the individual results frome each of these linear models to get a Cross-Validation MSE. This will be closer to the model's actual MSE, since \"noisy\" results that are higher than average will cancel out the \"noisy\" results that are lower than average.\n",
    "\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    cv_5_results = cross_val_score(linreg, X, y, cv=5, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias variance trade-off (underfitting and overfitting)\n",
    "\n",
    "Another perspective on this problem of overfitting versus underfitting is the bias variance tradeoff. The idea is that We can decompose the Mean Squared Error as the sum of \n",
    "- *bias*\n",
    "- *variance*, and\n",
    "- *irreducible error*:\n",
    "\n",
    "Formally, this is written as: \n",
    "$ MSE = Bias(\\hat{f}(x))^2 + Var(\\hat{f}(x)) + \\sigma^2$. The derivation of this result can be found on the wikipedia page of the bias-variance trade-off, [here](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation).\n",
    "\n",
    "<img src=\"./images/bias_variance.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIC and BIC\n",
    "\n",
    "#### AIC\n",
    "\n",
    "The AIC is generally used to compare each candidate model. The nice thing about the AIC is that for every model that uses **Maximum Likelihood Estimation**, the log-likelihood is automatically computed, and as a consequence the AIC is very easy to calculate.\n",
    "\n",
    "The AIC acts as a penalised log-likelihood criterion, giving a balance between a good fit\n",
    "(high value of log-likelihood) and complexity (complex models are penalized more than fairly simple ones). The AIC is unbounded so can take any type of value, but the bottom line is that when comparing models, the model with the lowest AIC should be selected.\n",
    "\n",
    "Note that directly comparing the values of log-likelihood maxima for different models (without including the penalty) is not good enough for model comparison, because including more parameters in a model will *always* give rise to an increased value of the maximum likelihood. Because of that reason, searching for the model with maximal log-likelihood\n",
    "would always lead to the model with the most parameters. The AIC balances this by penalizing for number of parameters, hence searching for models with few parameters but fitting the data well.\n",
    "data well.\n",
    "\n",
    "#### BIC\n",
    "\n",
    "The BIC (Bayesian Information Criterion) is very similar to the AIC and emerged as a Bayesian response to the AIC, but can be used for the exact same purposes. The idea is to select the candidate model with the highest probability\n",
    "given the data. \n",
    "This idea can be formalised inside a Bayesian framework, involving prior probabilities on candidate models along with prior densities on all parameters in the models. The penalty is slightly changed and depends on the number of rows to the data set:\n",
    "\n",
    "**BIC(model) = -2 \\* log-likelihood(model) + log(number of observations) \\* (length of the parameter space)**\n",
    "\n",
    "#### LassoLarsIC to visualize AIC/BIC \n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\n",
    "    model_bic = LassoLarsIC(criterion='bic')\n",
    "    model_bic.fit(df_inter, y)\n",
    "    alpha_bic_ = model_bic.alpha_\n",
    "\n",
    "    model_aic = LassoLarsIC(criterion='aic')\n",
    "    model_aic.fit(df_inter, y)\n",
    "    alpha_aic_ = model_aic.alpha_\n",
    "\n",
    "\n",
    "    def plot_ic_criterion(model, name, color):\n",
    "        alpha_ = model.alpha_\n",
    "        alphas_ = model.alphas_\n",
    "        criterion_ = model.criterion_\n",
    "        plt.plot(-np.log10(alphas_), criterion_, '--', color=color, linewidth=2, label= name)\n",
    "        plt.axvline(-np.log10(alpha_), color=color, linewidth=2,\n",
    "                    label='alpha for %s ' % name)\n",
    "        plt.xlabel('-log(alpha)')\n",
    "        plt.ylabel('criterion')\n",
    "\n",
    "    plt.figure()\n",
    "    plot_ic_criterion(model_aic, 'AIC', 'green')\n",
    "    plot_ic_criterion(model_bic, 'BIC', 'blue')\n",
    "    plt.legend()\n",
    "    plt.title('Information-criterion for model selection');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "#### Provides accuracy, recall and f1 score\n",
    "    from sklearn import classification_report\n",
    "    print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Visualization\n",
    "\n",
    "### Best for multi-class classification evaluation\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    # compute initial confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_hat_test, y_test)\n",
    "    # visualization packages\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "\n",
    "    def plot_confusion_matrix(cm, classes,\n",
    "                              normalize=False,\n",
    "                              title='Confusion matrix',\n",
    "                              cmap=plt.cm.Blues):\n",
    "     \"\"\" function to plot a confusion matrix, with color and normalization options. If normalize = True, will show a % rather than a raw number\"\"\"\n",
    "        #Add Normalization Option\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            print(\"Normalized confusion matrix\")\n",
    "        else:\n",
    "            print('Confusion matrix, without normalization')\n",
    "\n",
    "        print(cm)\n",
    "\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        tick_marks = np.arange(len(classes))\n",
    "        plt.xticks(tick_marks, classes, rotation=45)\n",
    "        plt.yticks(tick_marks, classes)\n",
    "    \n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance \n",
    "\n",
    "    \n",
    "    def plot_feature_importances(model):\n",
    "        n_features = X_train.shape[1]\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "        plt.yticks(np.arange(n_features), X_train.columns.values) \n",
    "        plt.xlabel(\"Feature importance\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "\n",
    "    plot_feature_importances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC, AUC Visualization (can only use with regression classification)\n",
    "\n",
    "### Best evaluation for binary classification problems\n",
    "\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    #for various decision boundaries given the case member probabilites\n",
    "\n",
    "    #First calculate the probability scores of each of the datapoints:\n",
    "    y_score = model_log.decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "    y_train_score = model_log.decision_function(X_train)\n",
    "    train_fpr, train_tpr, thresholds = roc_curve(y_train, y_train_score)\n",
    "    \n",
    "    \n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=5, label='ROC curve')\n",
    "    plt.plot([0,1], [0,1], color = 'blue', linestyle = '--')\n",
    "    plt.xlim([-0.1,1.0])\n",
    "    plt.ylim([0.0,1.1])\n",
    "    plt.xticks([i/20.0 for i in range(21)])\n",
    "    plt.yticks([i/20.0 for i in range(21)])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor Evaluations\n",
    "#### MAE, MSE, RMSE, R2\n",
    "\n",
    "    from sklearn import metrics\n",
    "\n",
    "    print('R Squared Value:', metrics.r2_score(y_test, y_pred))\n",
    "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "139.489px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
