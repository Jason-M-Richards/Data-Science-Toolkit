{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition¶\n",
    "A decision tree is a DAG (directed acyclic graph) type of classifier where each branch node represents a choice between a number of alternatives and each leaf node represents a classification. An unknown (or test) instance is routed down the tree according to the values of the attributes in the successive nodes. When the instance reaches a leaf, it is classified according to the label assigned to the corresponded leaf. The idea of feature importance is of high importance as selecting the correct feature to make a split that define complexity and effectiveness of the classification process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy and Information Gain raw code\n",
    "\n",
    "    from math import log\n",
    "#### Entropy\n",
    "    def entropy(pi):\n",
    "        '''\n",
    "        return the Entropy of a probability distribution:\n",
    "        entropy(p) = - SUM (Pi * log(Pi) )\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        for p in pi:\n",
    "            p = p / sum(pi)\n",
    "            if p != 0:\n",
    "                total +=  p * log(p, 2)\n",
    "            else:\n",
    "                total += 0\n",
    "        total *= -1\n",
    "        return total\n",
    "\n",
    "    print(entropy([1,1])) # Maximum Entropy e.g. a coin toss\n",
    "    print (entropy([0,6])) # No entropy, ignore the -ve with zero , its there due to log function\n",
    "    print (entropy([2,10])) # A random mix of classes\n",
    "\n",
    "    # Information Gain\n",
    "    def IG(D, a):\n",
    "        '''\n",
    "        return the information gain:\n",
    "        gain(D, A) = entropy(D)− SUM( |Di| / |D| * entropy(Di) )\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        for Di in a:\n",
    "            total += abs(sum(Di) / sum(D)) * entropy(Di)\n",
    "\n",
    "        gain = entropy(D) - total\n",
    "        return gain\n",
    "\n",
    "    test_dist = [6, 6] # Yes, No\n",
    "    test_attr = [ [4,0], [2,4], [0,2] ] # class1, class2, class3 of attr1 according to YES/NO classes in \n",
    "    test_dist\n",
    "    print(IG(test_dist, test_attr))\n",
    "    # the process of entropy is repeated until no more splits can be made, which is called the 'pure' split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "#### Import libraries\n",
    "#### Obtain Data\n",
    "#### Scrub Data\n",
    "#### Explore Data (normalize and scale)\n",
    "#### Create target and features\n",
    "    features = dataset.drop('target', axis=1)  \n",
    "    target = dataset['target']  \n",
    "    # create train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.30, random_state= 0) \n",
    "    # instantiate classifier\n",
    "    classifier = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
    "            splitter='best')  \n",
    "    # fit classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # predict data\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    # run classification report, confusion matrix and visualize auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "#### Import libraries\n",
    "#### Obtain Data\n",
    "#### Scrub Data\n",
    "#### Explore Data (normalize and scale)\n",
    "#### Create target and features\n",
    "    features = dataset.drop('target', axis=1)  \n",
    "\n",
    "    target = dataset['target']  \n",
    "    # create train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.30, random_state= 0)\n",
    "    # instantiate regressor\n",
    "    from sklearn.tree import DecisionTreeRegressor  \n",
    "    regressor = DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
    "               max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "               min_impurity_split=None, min_samples_leaf=1,\n",
    "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "               presort=False, random_state=None, splitter='best')  \n",
    "    # fit regressor\n",
    "    regressor.fit(X_train, y_train)  \n",
    "    # predict data\n",
    "    y_train_pred = regressor.predict(X_train)\n",
    "    # run regressor evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged Tree\n",
    "#### Bootstrap resampling and aggregation. Bootstrapping refers to the subsets of your dataset by sampling with replacement. Aggregation refers to the practice of combining all the different estimates to arrive at a single estimate. Used on Classifiers that have no bootstrap option.\n",
    "    bagged_tree =  BaggingClassifier(Classifier(), bootstrap=True, bootstrap_features=False, max_features=1.0, max_samples=1.0, n_estimators=20, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
    "    # fit data\n",
    "    bagged_tree.fit(data_train, target_train)\n",
    "    # run evaluation metrics (accuracy, f1, recall, confusion matrix, feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "    forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "    # fit data\n",
    "    forest.fit(data_train, target_train)\n",
    "    # run evaluation metrics (accuracy, f1, recall, confusion matrix, feature importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
