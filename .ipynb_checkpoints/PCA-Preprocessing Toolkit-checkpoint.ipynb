{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Types of missing data:\n",
    "- MCAR - Missing Completely at Random (not many missing values)\n",
    "- MAR - Missing at Random (some missing values)\n",
    "- MNAR - Missing Not at Random (many missing values)\n",
    "\n",
    "##### Visualizing Correlation of Missing Data\n",
    "import missingno as msno\n",
    "msno.heatmap(df) #visualizes correlation\n",
    "msno.dendrogram(df) #shows tree diagram of correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compares missingness between missing and non-missing data\n",
    "def fill_dummy_values(df, scaling_factor=1):\n",
    "    df_dummy = df.copy(deep=True)\n",
    "    for col in df_dummy:\n",
    "        col=df_dummy[col]\n",
    "        col_null=col.isnull()\n",
    "        num_nulls=col_null.sum()\n",
    "        col_range=col.max()-col.min()\n",
    "        dummy_values=(rand(num_nulls)-2)*scaling_factor*col_range+col.min()\n",
    "        col[col_null]=dummy_values\n",
    "return df_dummy\n",
    "#can visualize results with a scatterplot\n",
    "# Fill dummy values in diabetes_dummy\n",
    "df_dummy = fill_dummy_values()\n",
    "\n",
    "# Sum the nullity of one column and another column\n",
    "nullity = df[col_name].isna() + diabetes[col_name].isna()\n",
    "\n",
    "# Create a scatter plot of Skin Fold and BMI \n",
    "diabetes_dummy.plot(x='col_name', y='col_name', kind='scatter', alpha=0.5, c= nullity, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing missing data\n",
    "import missingno as msno\n",
    "msno.bar(df) #visualizes missing data as a bar chart (remember to plt.show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing missing data\n",
    "import missingno as msno\n",
    "msno.matrix(df) #shows missing data and can parse through date frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Missing Values\n",
    "\n",
    "1. pairwise - skips missing value (automatically happens in pandas)\n",
    "2. listwise - using df.dropna() to remove data by row or column\n",
    "    **only use when missing data is MCAR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Values\n",
    "replacing missing values with another value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fillna()\n",
    "\n",
    "##### Types:\n",
    "- ffill - forward fill - replace NaN with last observed value\n",
    "- bfill - backfill - replace NaN with next observed value\n",
    "example:\n",
    "\\\n",
    "df.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interpolate()\n",
    "**preferred method for time-series data**\n",
    "##### Types:\n",
    "- linear - extrapolates straight line between last and next observations and imputes equidistantly\n",
    "- quadratic - takes parabolic trajectory in negative direction and shoots back positive value\n",
    "- nearest - combination of ffill and bfill\n",
    "example:\n",
    "\\\n",
    "df.interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "df_copy = df.copy(deep=True) #makes copy for comparison to original\n",
    "si = SimpleImputer(strategy='', fill_value=#constant) #mean, median, constant, most-frequent(mode)\n",
    "df_copy.iloc[:,:] = si.fit_transform(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FancyImpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fancyimpute import KNN, IterateImputer\n",
    "#KNN uses K nearest neighbor to replace values\n",
    "#IterateImputer uses multiple regressions to replace values (most robust)\n",
    "example:\n",
    "\\\n",
    "ki = KNN()\n",
    "df_copy = df.copy(deep=True) #make copy for comparison to original\n",
    "df_copy.iloc[:,:] = ki.fit_transform(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Imputation\n",
    "convert, then impute if data are strings then fill Nan using most frequent value (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that will loop through each column and encode strings to integers, then impute missing\n",
    "#values with KNN and return those columns back to the original dataframe\n",
    "# Create an empty dictionary ordinal_enc_dict\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_enc_dict = {}\n",
    "def cat_data_imputer(df):\n",
    "    for col_name in df:\n",
    "    # Create Ordinal encoder for col\n",
    "        ordinal_enc_dict[col_name] = OrdinalEncoder()\n",
    "        col = df[col_name]\n",
    "    \n",
    "    # Select non-null values of col in users\n",
    "        col_not_null = col[col.notnull()]\n",
    "        reshaped_vals = col_not_null.values.reshape(-1, 1)\n",
    "        encoded_vals = ordinal_enc_dict[col_name].fit_transform(reshaped_vals)\n",
    "    \n",
    "    # Store the values to column in users\n",
    "        df.loc[col.notnull(), col_name] = np.squeeze(encoded_vals)\n",
    "    \n",
    "    # Create KNN imputer\n",
    "    KNN_imputer = KNN()\n",
    "\n",
    "# Impute and round the users DataFrame\n",
    "    df.iloc[:, :] = np.round(KNN_imputer.fit_transform(df))\n",
    "\n",
    "# Loop over the column names in users\n",
    "    for col_name in df:\n",
    "    \n",
    "    # Reshape the data\n",
    "        reshaped = df[col_name].values.reshape(-1, 1)\n",
    "    \n",
    "    # Perform inverse transform of the ordinally encoded columns\n",
    "        df[col_name] = ordinal_enc_dict[col_name].inverse_transform(reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Imputations\n",
    "1. Use linear regression for each imputed datset and compare results with original dataset\n",
    "2. Use KDE plots for each imputed dataset and compare shape with original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Categorical Variables\n",
    "\n",
    "#### Identifying categorical variables\n",
    "As categorical variables need to be treated in a particular manner, as you'll see later on, you need to make sure to identify which variables are categorical. In some cases, identifying will be easy (e.g. if they are stored as strings), in other cases they are numeric and the fact that they are categorical is not always immediately apparent.  Note that this may not be trivial. A first thing you can do is use the `.describe()` function and `.info()`-function and get a better sense. `.describe()` will give you info on the data types (like strings, integers, etc), but even then continuous variables might have been imported as strings, so it's very important to really have a look at your data.\n",
    "\n",
    "#### Transforming categorical variables\n",
    "When you want to use categorical variables in regression models, they need to be transformed. There are two approaches to this:\n",
    "- 1) Perform label encoding\n",
    "- 2) Create dummy variables / one-hot-encoding\n",
    "\n",
    "##### Label Encoding\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    lb_make = LabelEncoder()\n",
    "\n",
    "    origin_encoded = lb_make.fit_transform(cat_origin)\n",
    "    \n",
    "##### One hot encoding\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
    "\n",
    "    #pandas\n",
    "    pd.get_dummies(cat_origin)\n",
    "    \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n",
    "   \n",
    "    #sklearn\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    lb = LabelBinarizer()\n",
    "    origin_dummies = lb.fit_transform(cat_origin)\n",
    "    # you need to convert this back to a dataframe\n",
    "    origin_dum_df = pd.DataFrame(origin_dummies,columns=lb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity\n",
    "\n",
    "Because the idea behind regression is that you can change one variable and keep the others constant, correlation is a problem, because it indicates that changes in one predictor are associated with changes in another one as well. Because of this, the estimates of the coefficients can have big fluctuations as a result of small changes in the model. As a result, you may not be able to trust the p-values associated with correlated predictors.\n",
    "\n",
    "#### Checking for multicollinearity\n",
    "\n",
    "##### scatter matrix\n",
    "    pd.plotting.scatter_matrix(data,figsize  = [11, 11]);\n",
    "    \n",
    "##### correlation matrix\n",
    "    data.corr()\n",
    "    \n",
    "##### heatmap\n",
    "    import seaborn as sns\n",
    "    sns.heatmap(data_pred.corr(), center=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling and Normalization\n",
    "\n",
    "The idea behind this is that, around every point of the regression line, we would assume the data is spread around the eventual regression line in a \"homogenous\" way, with more points closer to the regression line and less points further away.\n",
    "\n",
    "Often, your dataset will contain features that largely vary in magnitudes. If we leave these magnitudes unchanged, coefficient sizes will largely fluctuate in magnitude as well. This can give the false impression that some variables are less important than others.\n",
    "\n",
    "Even though this is not always a formal issue when estimating linear regression models, this can be an issue in more advanced machine learning models. This is because most machine learning algorithms use Euclidean distance between two data points in their computations. Because of that, making sure that features have similar scales is formally required there. Some algorithms even require features to be zero centric.\n",
    "\n",
    "A good rule of thumb is, however, to check your features for normality, and while you're at it, scale your features so they have similar magnitudes, even for a \"simple\" model like linear regression.\n",
    "\n",
    "#### Popular transformations\n",
    "\n",
    "##### Log transformation\n",
    "\n",
    "Log transformation is a very useful tool when you have data that clearly does not follow a normal distribution. log transformation can help reducing skewness when you have skewed data, and can help reducing variability of data. \n",
    "\n",
    "    import numpy as np\n",
    "    data_log= pd.DataFrame([])\n",
    "    data_log[\"column\"] = np.log(data[\"column\"])\n",
    "\n",
    "##### Min-max scaling\n",
    "\n",
    "When performing min-max scaling, you can transform x to get the transformed $x'$ by using the formula:\n",
    "$$x' = \\dfrac{x - \\min(x)}{\\max(x)-\\min(x)}$$\n",
    "This way of scaling brings values between 0 and 1\n",
    "\n",
    "    features_final[\"CRIM\"] = (logcrim-min(logcrim))/(max(logcrim)-min(logcrim))\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data['column'])\n",
    "    \n",
    "\n",
    "##### Standardization\n",
    "\n",
    "When \n",
    "$$x' = \\dfrac{x - \\bar x}{\\sigma}$$\n",
    "x' will have mean $\\mu = 0$ and $\\sigma = 1$\n",
    "Note that standardization does not make data $more$ normal, it will just change the mean and the standard error!\n",
    "\n",
    "    features_final[\"DIS\"]   = (logdis-np.mean(logdis))/np.sqrt(np.var(logdis))\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data['column'])\n",
    "\n",
    "##### Mean normalization\n",
    "When performing mean normalization, you use the following formula:\n",
    "$$x' = \\dfrac{x - \\text{mean}(x)}{\\max(x)-\\min(x)}$$\n",
    "The distribution will have values between -1 and 1, and a mean of 0.\n",
    "\n",
    "    features_final[\"LSTAT\"] = (loglstat-np.mean(loglstat))/(max(loglstat)-min(loglstat))\n",
    "\n",
    "##### Unit vector transformation\n",
    " When performing unit vector transformations, you can create a new variable x' with a range [0,1]:\n",
    "$$x'= \\dfrac{x}{{||x||}}$$\n",
    "Recall that the norm of x $||x||= \\sqrt{(x_1^2+x_2^2+...+x_n^2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "#### Stepwise Selection\n",
    "In stepwise selection, you start with and empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\n",
    "\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    def stepwise_selection(X, y, \n",
    "                           initial_list=[], \n",
    "                           threshold_in=0.01, \n",
    "                           threshold_out = 0.05, \n",
    "                           verbose=True):\n",
    "        \"\"\" Perform a forward-backward feature selection \n",
    "        based on p-value from statsmodels.api.OLS\n",
    "        Arguments:\n",
    "            X - pandas.DataFrame with candidate features\n",
    "            y - list-like with the target\n",
    "            initial_list - list of features to start with (column names of X)\n",
    "            threshold_in - include a feature if its p-value < threshold_in\n",
    "            threshold_out - exclude a feature if its p-value > threshold_out\n",
    "            verbose - whether to print the sequence of inclusions and exclusions\n",
    "        Returns: list of selected features \n",
    "        Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "        See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "        \"\"\"\n",
    "        included = list(initial_list)\n",
    "        while True:\n",
    "            changed=False\n",
    "            # forward step\n",
    "            excluded = list(set(X.columns)-set(included))\n",
    "            new_pval = pd.Series(index=excluded)\n",
    "            for new_column in excluded:\n",
    "                model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "                new_pval[new_column] = model.pvalues[new_column]\n",
    "            best_pval = new_pval.min()\n",
    "            if best_pval < threshold_in:\n",
    "                best_feature = new_pval.idxmin()\n",
    "                included.append(best_feature)\n",
    "                changed=True\n",
    "                if verbose:\n",
    "                    print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "            # backward step\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "            # use all coefs except intercept\n",
    "            pvalues = model.pvalues.iloc[1:]\n",
    "            worst_pval = pvalues.max() # null if pvalues is empty\n",
    "            if worst_pval > threshold_out:\n",
    "                changed=True\n",
    "                worst_feature = pvalues.argmax()\n",
    "                included.remove(worst_feature)\n",
    "                if verbose:\n",
    "                    print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "            if not changed:\n",
    "                break\n",
    "        return included\n",
    "        \n",
    "#### Recursive Feature Elimination\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE\n",
    "\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    linreg = LinearRegression()\n",
    "    selector = RFE(linreg, n_features_to_select = 2)\n",
    "    selector = selector.fit(predictors, data_fin[\"mpg\"])\n",
    "    \n",
    "    \n",
    "#### Forward Selection using Adjusted R-squared    \n",
    "    \n",
    "    import statsmodels.formula.api as smf\n",
    "\n",
    "    def forward_selected(data, response):\n",
    "        \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "        response: string, name of response column in data\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        model: an \"optimal\" fitted statsmodels linear model\n",
    "               with an intercept\n",
    "               selected by forward selection\n",
    "               evaluated by adjusted R-squared\n",
    "        \"\"\"\n",
    "        remaining = set(data.columns)\n",
    "        remaining.remove(response)\n",
    "        selected = []\n",
    "        current_score, best_new_score = 0.0, 0.0\n",
    "        while remaining and current_score == best_new_score:\n",
    "            scores_with_candidates = []\n",
    "            for candidate in remaining:\n",
    "                formula = \"{} ~ {} + 1\".format(response,\n",
    "                                               ' + '.join(selected + [candidate]))\n",
    "                score = smf.ols(formula, data).fit().rsquared_adj\n",
    "                scores_with_candidates.append((score, candidate))\n",
    "            scores_with_candidates.sort()\n",
    "            best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "            if current_score < best_new_score:\n",
    "                remaining.remove(best_candidate)\n",
    "                selected.append(best_candidate)\n",
    "                current_score = best_new_score\n",
    "        formula = \"{} ~ {} + 1\".format(response,\n",
    "                                       ' + '.join(selected))\n",
    "        model = smf.ols(formula, data).fit()\n",
    "        return model\n",
    "        \n",
    "#### Permutation Importance for Classification\n",
    "\n",
    "    #oob classifier accuracy for classification scoring\n",
    "    def oob_classifier_accuracy(rf, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Compute out-of-bag (OOB) accuracy for a scikit-learn random forest\n",
    "        classifier. We learned the guts of scikit's RF from the BSD licensed\n",
    "        code:\n",
    "        https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/forest.py#L425\n",
    "        \"\"\"\n",
    "        X = X_train\n",
    "        y = y_train\n",
    "\n",
    "        n_samples = len(X)\n",
    "        n_classes = len(np.unique(y))\n",
    "        predictions = np.zeros((n_samples, n_classes))\n",
    "        for tree in rf.estimators_:\n",
    "            unsampled_indices = _generate_unsampled_indices(tree.random_state, n_samples)\n",
    "            tree_preds = tree.predict_proba(X[unsampled_indices, :])\n",
    "            predictions[unsampled_indices] += tree_preds\n",
    "\n",
    "        predicted_class_indexes = np.argmax(predictions, axis=1)\n",
    "        predicted_classes = [rf.classes_[i] for i in predicted_class_indexes]\n",
    "\n",
    "        oob_score = np.mean(y == predicted_classes)\n",
    "        return oob_score\n",
    "\n",
    "    #package for PI\n",
    "    import eli5\n",
    "    from eli5.sklearn import PermutationImportance\n",
    "    from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "    X_train, X_test, y_train, y_test = train_test_split(f_scale,target_resample, random_state=0)\n",
    "    perm = PermutationImportance(rf, cv=5, scoring = oob_classifier_accuracy) #can change scoring for other forms of models\n",
    "    perm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions in Regression Models\n",
    "\n",
    "In statistics, an interaction is a particular property of three or more variables, where two or more variables interact in a non-additive manner when affecting a third variable. In other words, the two variables interact to have an effect that is more (or less) than the sum of their parts. Not accounting for them might lead to results that are wrong. You'll also notice that including them when they're needed will increase your  R2R2  value!\n",
    "\n",
    "#### Iterate through combinations of features to get top three interactions\n",
    "\n",
    "    from itertools import combinations\n",
    "    combinations = list(combinations(data.feature_names, 2))\n",
    "    interactions = []\n",
    "    data = df.copy()\n",
    "    for comb in combinations:\n",
    "        data[\"interaction\"] = data[comb[0]] * data[comb[1]]\n",
    "        score = np.mean(cross_val_score(regression, data, y, scoring=\"r2\", cv=crossvalidation))\n",
    "        if score > baseline: interactions.append((comb[0], comb[1], round(score,3)))\n",
    "\n",
    "    print(\"Top 3 interactions: %s\" %sorted(interactions, key=lambda inter: inter[2], reverse=True)[:5])\n",
    "    \n",
    "#### Feature engineer interactions into dataframe\n",
    "\n",
    "    df_inter = df.copy()#make a copy of dataframe so original is not affected\n",
    "    df_inter[\"RM_LSTAT\"] = df[\"RM\"] * df[\"LSTAT\"] #combines the two features\n",
    "    df_inter[\"RM_TAX\"] = df[\"RM\"] * df[\"TAX\"]\n",
    "    df_inter[\"RM_RAD\"] = df[\"RM\"] * df[\"RAD\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomials in Regression (curved relationship)\n",
    "\n",
    "When relationships between predictors and outcome are not linear and show some sort of a curvature, polynomials can be used to generate better approximations. The idea is that you can transform your input variable by e.g, squaring it.\n",
    "\n",
    "$\\hat y = \\hat \\beta_0 + \\hat \\beta_1x + \\hat \\beta_2 x^2$ \n",
    "\n",
    "The use of polynomials is not restricted to quadratic relationships, you can explore cubic relationships,... as well! Imagine you want to go until the power of 10, it would be quite annoying to transform your variable 9 times. Of course, Scikit-Learn has a built-in Polynomial option in the preprocessing library!\n",
    "\n",
    "#### sci-kit learn polynomial selection with visual feedback and MSE scores\n",
    "\n",
    "    for index, degree in enumerate([2,3,4]):\n",
    "        poly = PolynomialFeatures(degree)\n",
    "        X = poly.fit_transform(X)\n",
    "        X_plot = poly.fit_transform(X_plot)\n",
    "        reg_poly = LinearRegression().fit(X, y)\n",
    "        y_plot = reg_poly.predict(X_plot)\n",
    "        plt.plot(x_plot, y_plot, color=colors[index], linewidth = 2 ,\n",
    "                 label=\"degree %d\" % degree)\n",
    "        print(\"degree %d\" % degree, r2_score(y, reg_poly.predict(X)))\n",
    "\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparseness in n-Dimensional Space\n",
    "\n",
    "Points in n-dimensional space become increasingly sparse as the number of dimensions increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence Time\n",
    "Another issue with increasing feature space is the training time required to fit a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA example on Iris DatasetÂ¶\n",
    "\n",
    "#### loading dataset into Pandas DataFrame\n",
    "#### before PCA is performed, ensure that dataset is explored and standardized.\n",
    "#### Initialize an instance of PCA from scikit-learn with n components\n",
    "\n",
    "    pca=PCA(n_components=n)\n",
    "    transformed = pca.fit_transform(X)\n",
    "\n",
    "##### To visualize the components, it will be useful to also look at the target associated with the particular observation. As such, append the target (flower name) to the principal components in a pandas dataframe.\n",
    "\n",
    "    # Create a new dataset from principal components \n",
    "\n",
    "    df = pd.DataFrame(data = transformed, columns = ['PC1', 'PC2'])\n",
    "    result_df = pd.concat([df, iris[['target']]], axis = 1)\n",
    "    result_df.head()\n",
    "\n",
    "#### Visualize Principal Components Using the target data\n",
    "\n",
    "    # PCA scatter plot\n",
    "\n",
    "    plt.style.use('seaborn-dark')\n",
    "    fig = plt.figure(figsize = (10,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('First Principal Component ', fontsize = 15)\n",
    "    ax.set_ylabel('Second Principal Component ', fontsize = 15)\n",
    "    ax.set_title('Principal Component Analysis (2PCs) for Iris Dataset', fontsize = 20)\n",
    "\n",
    "    targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "    colors = ['r', 'g', 'b']\n",
    "    for target, color in zip(targets,colors):\n",
    "        indicesToKeep = iris['target'] == target\n",
    "        ax.scatter(result_df.loc[indicesToKeep, 'PC1']\n",
    "                   , result_df.loc[indicesToKeep, 'PC2']\n",
    "                   , c = color\n",
    "                   , s = 50)\n",
    "    ax.legend(targets)\n",
    "    ax.grid()\n",
    "\n",
    "#### Calculate the variance explained by priciple components\n",
    "\n",
    "    print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "    print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "#### Run a KNeighborsClassifier to classify the dataset after PCA\n",
    "\n",
    "    X = result_df[['PC1', 'PC2']]\n",
    "    y = iris.target\n",
    "    y = preprocessing.LabelEncoder().fit_transform(y)\n",
    "    start = timeit.timeit()\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=9)\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Yhat = model.predict(X_test)\n",
    "    acc = metrics.accuracy_score(Yhat, Y_test)\n",
    "    end = timeit.timeit()\n",
    "    print(\"Accuracy:\",acc)\n",
    "    print (\"Time Taken:\", end - start)\n",
    "\n",
    "##### some accuracy is lost after performing PCA, but computing time is reduced and accuracy can be improved in some complex cases\n",
    "\n",
    "#### Plot decision boundary using principal components \n",
    "\n",
    "    def decision_boundary(pred_func):\n",
    "    \n",
    "###### Set the boundary\n",
    "    \n",
    "    x_min, x_max = X.iloc[:, 0].min() - 0.5, X.iloc[:, 0].max() + 0.5\n",
    "    y_min, y_max = X.iloc[:, 1].min() - 0.5, X.iloc[:, 1].max() + 0.5\n",
    "    h = 0.01\n",
    "    \n",
    "###### build meshgrid\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "###### plot the contour\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.afmhot)\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.Spectral, marker='x')\n",
    "\n",
    "    decision_boundary(lambda x: model.predict(x))\n",
    "\n",
    "    plt.title(\"decision boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Recognition with PCA\n",
    "\n",
    "#### Obtain Data\n",
    "#### Scrub and Explore\n",
    "#### Baseline Model w/ SVC\n",
    "    from sklearn import svm\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=22)\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "#### Compressing with PCA\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    import seaborn as sns\n",
    "    sns.set_style('darkgrid')\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "#### Plot the Explained Variance versus Number of Features\n",
    "\n",
    "    plt.plot(range(1,65), \n",
    "    pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "#### Determine the Number of Features to Capture 95% of the Datasets Variance\n",
    "\n",
    "    total_explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "    n_over_95 = len(total_explained_variance[total_explained_variance >= .95])\n",
    "    n_to_reach_95 = X.shape[1] - n_over_95 + 1\n",
    "    print(\"Number features: {}\\tTotal Variance Explained: {}\".format(n_to_reach_95, total_explained_variance[n_to_reach_95-1]))\n",
    "\n",
    "\n",
    "#### Subset the Dataset\n",
    "\n",
    "    pca = PCA(n_components=n_to_reach_95)\n",
    "    X_pca_train = pca.fit_transform(X_train)\n",
    "    pca.explained_variance_ratio_.cumsum()[-1]\n",
    "\n",
    "#### Refit a Model on the Compressed Dataset\n",
    "\n",
    "    X_pca_test = pca.transform(X_test)\n",
    "    clf = svm.SVC()\n",
    "    %timeit clf.fit(X_pca_train, y_train)\n",
    "    train_pca_acc = clf.score(X_pca_train, y_train)\n",
    "    test_pca_acc = clf.score(X_pca_test, y_test)\n",
    "    print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_pca_acc, test_pca_acc))\n",
    "\n",
    "#### Evaluate model and optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw PCA using Numpy\n",
    "\n",
    "    # Normalize the Data\n",
    "    data = data - data.mean()\n",
    "    data.head()\n",
    "    # Calculate the Covariance Matrix\n",
    "    cov_mat = data.cov()\n",
    "    cov_mat\n",
    "    # Calculate the Eigenvectors\n",
    "    import numpy as np\n",
    "    eig_values, eig_vectors = np.linalg.eig(cov_mat)\n",
    "    # Sorting the Eigenvectors to Determine Primary Components\n",
    "    e_indices = np.argsort(eig_values)[::-1] \n",
    "    # Get the index values of the sorted eigenvalues\n",
    "    eigenvectors_sorted = eig_vectors[:,e_indices]\n",
    "    eigenvectors_sorted\n",
    "    # Reprojecting the Data to n dimensions\n",
    "    eigenvectors_sorted[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
