{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.cluster import KMeans\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "- Select k initial seeds\n",
    "- Assign each observation to the clusted to which it is \"closest\"\n",
    "- Recompute the cluster centroids\n",
    "- Reassign the observations to one of the clusters according to some rule\n",
    "- Stop if there is no reallocation.\n",
    "\n",
    "k_means = KMeans(n_clusters=3) ###### Must set number of clusters at initialization time!\n",
    "\n",
    "k_means.fit(some_df) ###### Run the clustering algorithm\n",
    "\n",
    "cluster_assignments = k_means.predict(some_df) ###### Generate cluster index values for each row in df\n",
    "\n",
    "##### cluster predictions for each point are also stored in k_means.labels_ attribute\n",
    "\n",
    "#### Computing Variance Ratios\n",
    " from sklearn.metrics import calinski_harabaz_score\n",
    "\n",
    "print(calinski_harabaz_score(some_df, cluster_assignments))\n",
    " \n",
    "###### The higher the score, the better the fit.\n",
    "###### Note: could also pass in k_means.labels_ instead of cluster_assignments, as they are the same thing\n",
    "#### Finding the Optimal Value of K by visualizing the scores using an Elbow Plot:\n",
    "###### Calinski Harabaz Score\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "\n",
    "CH_score = []\n",
    "\n",
    "for k in k_list:\n",
    "    labels=k.labels_\n",
    "    chs=calinski_harabaz_score(X_2, labels)\n",
    "    CH_score.append(chs)\n",
    "\n",
    "###### plotting CH Score\n",
    "plt.plot([3, 4, 5, 6, 7], CH_score)\n",
    "\n",
    "plt.xticks([3,4,5,6,7])\n",
    "\n",
    "plt.title(\"Calinski Harabaz Scores for Different Values of K\")\n",
    "\n",
    "plt.ylabel(\"Variance Ratio\")\n",
    "\n",
    "plt.xlabel(\"K=\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heirarchal Agglomerative Clustering - HAC\n",
    "agg_clust = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "agg_clust\n",
    "\n",
    "assigned_clust = agg_clust.fit_predict(X)\n",
    "\n",
    "### Dendrogram Plot\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "\n",
    "linkage_array = ward(X)\n",
    "\n",
    "###### Now we plot the dendrogram for the linkage_array containing the distances between clusters\n",
    "dendrogram(linkage_array)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "bounds = ax.get_xbound()\n",
    "\n",
    "plt.xlabel(\"Sample index\")\n",
    "\n",
    "plt.ylabel(\"Cluster distance\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Evaluations\n",
    "from sklearn import metrics\n",
    "#### Adjusted Rand Index\n",
    "metrics.adjusted_rand_score(labels_kmeans, y)\n",
    "###### generates number between -1 and 1. Better score as it gets closer to 1\n",
    "#### Fowlkes-Mallows Score\n",
    "metrics.fowlkes_mallows_score(labels_kmeans, y)\n",
    "###### generates number between 0 and 1. Better score as it gets closer to 1\n",
    "#### Calinksi-Harabaz Index\n",
    "metrics.calinski_harabaz_score(X, labels_kmeans)\n",
    "###### higher the score, the better\n",
    "#### Silhouette Coefficient\n",
    "metrics.silhouette_score(X, labels_kmeans)\n",
    "###### generates number between -1 and 1. Closer to -1 means incorrect clustering, closer to 1 means each cluster is dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Segmentation with Clustering\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "###### load data and explore\n",
    "raw_df = pd.read_csv('wholesale_customers_data.csv')\n",
    "\n",
    "raw_df.head()\n",
    "\n",
    "Now, let's go ahead and store the 'Channel' column in a separate variable, and then drop both the 'Channel' and 'Region' columnns. Then, display the head of the new DataFrame to ensure everything worked correctly.\n",
    "\n",
    "###### set target and features\n",
    "channels = raw_df['Channel']\n",
    "\n",
    "df=raw_df.drop(['Channel', 'Region'], axis = 1, inplace=False) \n",
    "\n",
    "df.head()\n",
    "\n",
    "#### Scale the Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scale=StandardScaler()\n",
    "\n",
    "df_scale=scale.fit_transform(df)\n",
    "\n",
    "#### Run K-Means with 2 clusters\n",
    "k_means2=KMeans(n_clusters=2)\n",
    "\n",
    "k_means2.fit(df_scale)\n",
    "\n",
    "scaled_preds=k_means2.predict(df_scale)\n",
    "\n",
    "#### get adjusted rand index\n",
    "ajs2=adjusted_rand_score(channels, scaled_preds)\n",
    "\n",
    "print(ajs2)\n",
    "\n",
    "#### Incorporating PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "pca_df = pca.fit_transform(df_scale)\n",
    "\n",
    "np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "###### rerun KMeans model and adjust PCA n_components until highest evaluation score achieved.\n",
    "\n",
    "#### Hierarchical Agglomerative Clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "hac = AgglomerativeClustering(n_clusters=2)\n",
    "###### fitted to PCA data\n",
    "hac.fit(pca_df)\n",
    "\n",
    "hac_pca_preds = hac.labels_\n",
    "\n",
    "adjusted_rand_score(channels, hac_pca_preds)\n",
    "\n",
    "hac2 = AgglomerativeClustering(n_clusters=2)\n",
    "###### fitted to scaled only data\n",
    "hac2.fit(df_scale)\n",
    "\n",
    "hac_scaled_preds = hac2.labels_\n",
    "\n",
    "adjusted_rand_score(channels, hac_scaled_preds)\n",
    "\n",
    "hac3 = AgglomerativeClustering(n_clusters=2)\n",
    "###### fitted to original data\n",
    "hac3.fit(df)\n",
    "\n",
    "hac__preds = hac3.labels_\n",
    "\n",
    "adjusted_rand_score(channels, hac__preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Function for Manhattan, Euclidean and Minkowski\n",
    "def distance(a, b, c=2, verbose=True):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Both vectors must be of equal length!\")\n",
    "    \n",
    "    root = 1 / c\n",
    "    running_total = 0\n",
    "    \n",
    "    if verbose:\n",
    "        if c == 1:\n",
    "            print(\"Calculating Manhattan Distance:\")\n",
    "        elif c == 2:\n",
    "            print('Calculating Euclidean Distance:')\n",
    "        else:\n",
    "            print(\"Calcuating Minkowski Distance (c={}):\".format(c))\n",
    "    \n",
    "    for ind, val_a in enumerate(a):\n",
    "        val_b = b[ind]\n",
    "        running_total += np.power(np.abs(val_a - val_b), c)\n",
    "    \n",
    "    return np.power(running_total, root)\n",
    "\n",
    "test_point_1 = ()\n",
    "\n",
    "test_point_2 = ()\n",
    "\n",
    "print(distance(test_point_1, test_point_2))\n",
    "\n",
    "print(distance(test_point_1, test_point_2, c=1))\n",
    "\n",
    "print(distance(test_point_1, test_point_2, c=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN raw class creation \n",
    "Getting StartedÂ¶\n",
    "We'll begin this lab by creating our classifier. To keep things simple, we'll be using a helper function from the scipy library to calcluate euclidean distance for us--specifically, the euclidean() function from the scipy.spatial.distance module. Import this function in the cell below.\n",
    "\n",
    "#### Create Helper Function\n",
    "from scipy.spatial.distance import euclidean as euc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "#### Create an class called KNN.\n",
    "class KNN(object):\n",
    "    def fit():\n",
    "        pass\n",
    "    def predict():\n",
    "        pass\n",
    "#### Completing the fit Method\n",
    "def fit(self, X_train, y_train):\n",
    "    self.X_train = X_train\n",
    "    self.y_train = y_train\n",
    "    \n",
    "###### This line updates the knn.fit method to point to the function we've just written\n",
    "KNN.fit = fit\n",
    "\n",
    "#### Helper Functions\n",
    "def _get_distances(self, x):\n",
    "    distances = []\n",
    "    for ind, val in enumerate(self.X_train):\n",
    "        dist_to_i = euc(x, val)\n",
    "        distances.append((ind, dist_to_i))\n",
    "    return distances\n",
    "###### This line attaches the function we just created as a method to our KNN class.\n",
    "KNN._get_distances = _get_distances\n",
    "\n",
    "def _get_k_nearest(self, dists, k):\n",
    "    sorted_dists = sorted(dists, key=lambda x: x[1])\n",
    "    return sorted_dists[:k]\n",
    "###### This line attaches the function we just created as a method to our KNN class.\n",
    "KNN._get_k_nearest = _get_k_nearest\n",
    "\n",
    "def _get_label_prediction(self, k_nearest):\n",
    "    labels = [self.y_train[i] for i, _ in k_nearest]\n",
    "    counts = np.bincount(labels)\n",
    "    return np.argmax(counts)\n",
    "###### This line attaches the function we just created as a method to our KNN class.\n",
    "KNN._get_label_prediction = _get_label_prediction\n",
    "Great! Now, we need to complete the predict method. This will be much simpler, now that we have some\n",
    "\n",
    "#### Completing the predict Method\n",
    "def predict(self, X_test, k=3):\n",
    "    preds = []\n",
    "    # Iterate through each item in X_test\n",
    "    for i in X_test:\n",
    "        # Get distances between i and each item in X_train\n",
    "        dists = self._get_distances(i)\n",
    "        k_nearest = self._get_k_nearest(dists, k)\n",
    "        predicted_label = self._get_label_prediction(k_nearest)\n",
    "        preds.append(predicted_label)\n",
    "    return preds\n",
    "        \n",
    "KNN.predict = predict\n",
    "\n",
    "#### Testing Our KNN Classifier with Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "data = iris.data\n",
    "\n",
    "target = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25)\n",
    "\n",
    "knn = KNN()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "preds = knn.predict(X_test)\n",
    "\n",
    "print(\"Testing Accuracy: {}\".format(accuracy_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Titanic Survivors with KNN\n",
    "import pandas as pd\n",
    "###### get data\n",
    "raw_df = pd.read_csv('titanic.csv')\n",
    "\n",
    "raw_df.head()\n",
    "#### Preprocessing Our Data\n",
    "###### drop unnecessary columns\n",
    "df = raw_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=False)\n",
    "###### change binary selection for male/female to integer\n",
    "df.Sex = df.Sex.map({'female': 0, 'male': 1})\n",
    "###### check for nulls\n",
    "df.isna().sum()\n",
    "###### fill null age with median\n",
    "df.Age = df.Age.fillna(df.Age.median())\n",
    "###### drop rest of nulls in dataset\n",
    "df = df.dropna()\n",
    "##### one hot encode categorical data to convert to numerical\n",
    "one_hot_df = pd.get_dummies(df)\n",
    "\n",
    "one_hot_df.head()\n",
    "###### set target and features \n",
    "labels = one_hot_df.Survived\n",
    "\n",
    "one_hot_df.drop('Survived', axis=1, inplace=True)\n",
    "#### Normalizing Our Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_data = scaler.fit_transform(one_hot_df)\n",
    "###### recover column names and save as dataframe\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=one_hot_df.columns)\n",
    "\n",
    "scaled_df.head()\n",
    "\n",
    "#### Creating Training and Testing Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_df, labels, test_size=0.25)\n",
    "\n",
    "#### Creating and Fitting our KNN Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf1 = KNeighborsClassifier()\n",
    "\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "test_preds = clf1.predict(X_test)\n",
    "\n",
    "#### Evaluate initial model\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "def print_metrics(labels, preds):\n",
    "    print(\"Precision Score: {}\".format(precision_score(labels, preds)))\n",
    "    print(\"Recall Score: {}\".format(recall_score(labels, preds)))\n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(labels, preds)))\n",
    "    print(\"F1 Score: {}\".format(f1_score(labels, preds)))\n",
    "    \n",
    "###### Note: Overall, f1-score is the most informative about the performance of the model, followed by accuracy. For multicategorical models, accuracy is best.\n",
    "\n",
    "#### Improving Model Performance (k hyperparameter only raw code)\n",
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_k = 0\n",
    "    best_score = 0.0\n",
    "    for k in range(min_k, max_k+1, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        if f1 > best_score:\n",
    "            best_k = k\n",
    "            best_score = f1\n",
    "    \n",
    "    print(\"Best Value for k: {}\".format(best_k))\n",
    "    print(\"F1-Score: {}\".format(best_score))\n",
    "find_best_k(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
