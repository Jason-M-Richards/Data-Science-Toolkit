{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Basics\n",
    "\n",
    "Networks come in all shapes and sizes.\n",
    "\n",
    "- We can add more features (nodes) in the input layer.\n",
    "- We can add more nodes in the hidden layer. Also, we can simply add more hidden layers. This is what turns a neural network in a \"deep\" neural network (hence, deep learning)\n",
    "- We can have several nodes in the output layer.\n",
    "\n",
    "And there is one more thing that makes deep learning extremely powerful: unlike many other statistical and machine learning techniques, deep learning can deal extremely well with **unstructured data**.\n",
    "\n",
    "Types or Neural networks:\n",
    "- Standard neural networks\n",
    "- Convolutional neural networks (input = images, video)\n",
    "- Recurrent neural networks (input = audio files, text, time series data)\n",
    "- Generative adversarial networks\n",
    "\n",
    "#### Logistic regression as a neural network (no hidden layers)\n",
    "We'll need some expression here in order to make a prediction.\n",
    "The parameters here are $w \\in  \\mathbb{R}^n$ and $b \\in \\mathbb{R}$. Some expression to get to $\\hat y$ could be $\\hat y = w^T x + b$. The problem here is, however, that this type of expression does not ensure that the eventual outcome $ \\hat y$ will be between zero and one, and it could be much bigger than one or even negative!\n",
    "\n",
    "#### activation functions\n",
    "**sigmoid function**\n",
    "Recall that the mathematical expression of the sigmoid is $ a=\\dfrac{1}{1+ \\exp(-z)}$, and this outputs activation values somewhere between 0 and 1.\n",
    "    \n",
    "        def sigmoid(x, derivative=False):\n",
    "        f = 1 / (1 + np.exp(-x))\n",
    "        if (derivative == True):\n",
    "            return f * (1 - f)\n",
    "        return f\n",
    "\n",
    "**tanh**\n",
    "The hyperbolic tangent (or tanh) function goes between -1 and +1, and is in fact a shifted version of the sigmoid function, with formula $ a=\\dfrac{\\exp(z)- \\exp(-z)}{\\exp(z)+ \\exp(-z)}$. For intermediate layers, the tanh function generally performs pretty well because, with values between -1 and +1, the means of the activations coming out are closer to zero! \n",
    "\n",
    "    def tanh(x, derivative=False):\n",
    "        f = np.tanh(x)\n",
    "        if (derivative == True):\n",
    "            return (1 - (f ** 2))\n",
    "        return np.tanh(x)\n",
    "        \n",
    "**ReLU** (Rectified Linear Unit function)\n",
    "This is probably the most popular activation function, along with the tanh! The fact that the activation is exactly 0 when $z <0$  is slightly cumbersome when taking derivatives though. $a=\\max(0,z)$\n",
    "\n",
    "    def relu(x, derivative=False):\n",
    "        f = np.zeros(len(x))\n",
    "        if (derivative == True):\n",
    "            for i in range(0, len(x)):\n",
    "                if x[i] > 0:\n",
    "                    f[i] = 1  \n",
    "                else:\n",
    "                    f[i] = 0\n",
    "            return f\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = x[i]  \n",
    "            else:\n",
    "                f[i] = 0\n",
    "        return f\n",
    "\n",
    "**leaky ReLU**\n",
    "\n",
    "The leaky ReLU solves the derivative issue by allowing for the activation to be slightly negative when $z <0$ ! $a=\\max(0.001*z,z)$\n",
    "\n",
    "    def leaky_relu(x, leakage = 0.05, derivative=False):\n",
    "        f = np.zeros(len(x))\n",
    "        if (derivative == True):\n",
    "            for i in range(0, len(x)):\n",
    "                if x[i] > 0:\n",
    "                    f[i] = 1  \n",
    "                else:\n",
    "                    f[i] = leakage\n",
    "            return f\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = x[i]  \n",
    "            else:\n",
    "                f[i] = x[i]* leakage\n",
    "        return f\n",
    "        \n",
    "**arctan**\n",
    "The inverse tangent (arctan) function has a lot of the same qualities that tanh has, but the range roughly goes from -1.6 to 1.6, and  the slope is more gentle than the one we saw using the tanh function.\n",
    "\n",
    "    def arctan(x, derivative=False):\n",
    "        if (derivative == True):\n",
    "            return 1/(1+np.square(x))\n",
    "        return np.arctan(x)\n",
    "\n",
    "    z = np.arange(-10,10,0.2)\n",
    "    \n",
    "#### Loss and Cost Function\n",
    "The **loss function** is used to measure the inconsistency between the predicted value $(\\hat y)$ and the actual label $y$. In logistic regression the loss function is defined as\n",
    "$\\mathcal{L}(\\hat y, y) = - ( y \\log (\\hat y) + (1-y) \\log(1-\\hat y))$. The advantage of this loss function expression is that the optimization space here is convex, which makes optimizing using gradient descent easier. The loss function is defined over 1 particular training sample. \n",
    "\n",
    "The **cost function** takes the average loss over all the samples: $J(w,b) = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})$\n",
    "When you train your logistic regression model, the purpose is to find parameters $w$ and $b$ such that your cost function is minimized!\n",
    "\n",
    "#### Forward propagation\n",
    "Initialize $J= 0$, $dw_1= 0$, $dw_2= 0$, $db= 0$. \n",
    "\n",
    "For each training sample $1,...,l$ you'll need to compute:\n",
    "\n",
    "$ z^{(i)} = w^T x^ {(i)} +b $\n",
    "\n",
    "$\\hat y^{(i)} = \\sigma (z^{(i)})$\n",
    "\n",
    "$dz^{(i)} = \\hat y^{(i)}- y^{(i)}$\n",
    "\n",
    "#### Backward propagation (after updating values with forward propagation)\n",
    "$J_{+1} = - [y^{(i)} \\log (\\hat y^{(i)}) + (1-y^{(i)}) \\log(1-\\hat y^{(i)})$\n",
    "\n",
    "$dw_{1, +1}^{(i)} = x_1^{(i)} * dz^{(i)}$\n",
    "\n",
    "$dw_{2, +1}^{(i)} = x_2^{(i)} * dz^{(i)}$\n",
    "\n",
    "$db_{+1}^{(i)} =  dz^{(i)}$\n",
    "\n",
    "$\\dfrac{J}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{db}{m}$\n",
    "\n",
    "#### Update weights\n",
    "$w_1 := w_1 - \\alpha dw_1$\n",
    "\n",
    "$w_2 := w_2 - \\alpha dw_2$\n",
    "\n",
    "$b := b - \\alpha db$\n",
    "\n",
    "repeat until convergence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks Process Overview\n",
    "\n",
    "To summarize the process once more, we begin by defining a model architecture which includes the number of hidden layers, activation functions (sigmoid or relu) and the number of units in each of these.   \n",
    "\n",
    "We then initialize parameters for each of these layers (typically randomly). After the initial parameters are set, forward propagation evaluates the model giving a prediction, which is then used to evaluate a cost function. Forward propogation involves evaluating each layer and then piping this output into the next layer. \n",
    "\n",
    "Each layer consists of a linear transformation and an activation function.  The parameters for the linear transformation in **each** layer include $W^l$ and $b^l$. The output of this linear transformation is represented by $Z^l$. This is then fed through the activation function (again, for each layer) giving us an output $A^l$ which is the input for the next layer of the model.  \n",
    "\n",
    "After forward propogation is completed and the cost function is evaluated, backpropogation is used to calculate gradients of the initial parameters with respect to this cost function. Finally, these gradients are then used in an optimization algorithm, such as gradient descent, to make small adjustments to the parameters and the entire process of forward propogation, back propogation and parameter adjustments is repeated until the modeller is satisfied with the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Deep Neural Network from Scratch\n",
    "   \n",
    "    import numpy as np\n",
    "    import h5py\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    %matplotlib inline\n",
    "    plt.rcParams['figure.figsize'] = (5.0, 5.0) \n",
    "    plt.rcParams['image.interpolation'] = 'nearest'\n",
    "    plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    np.random.seed(123)\n",
    "    \n",
    "#### Initialization in an L-layer Neural Network\n",
    "    def initialize_parameters(n_0, n_1, n_2):\n",
    "    np.random.seed(123) \n",
    "    W1 = np.random.randn(n_1, n_0) * 0.05 \n",
    "    b1 = np.zeros((n_1, 1))\n",
    "    W2 =  np.random.randn(n_2, n_1) * 0.05 \n",
    "    b2 = np.zeros((n_2, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "    \n",
    "#### create a dictionary of parameters for W and b given a list of layer dimensions.\n",
    "    #Simply randomly initialize values in accordance to the shape each parameter should have.\n",
    "    #Use random seed 123 (as provided)\n",
    "    def initialize_parameters_deep(list_layer_dimensions):\n",
    "\n",
    "        np.random.seed(123)\n",
    "        parameters = {}\n",
    "        L = len(list_layer_dimensions)           \n",
    "\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.randn(list_layer_dimensions[l], list_layer_dimensions[l-1])*0.05\n",
    "            parameters['b' + str(l)] = np.zeros((list_layer_dimensions[l], 1))\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "#### forward propagation through linear activation\n",
    "    def linear_activation_forward(A_prev, W, b, activation):\n",
    "        Z = np.dot(W, A_prev) + b #Your code here; see the linear transformation above for how to compute Z\n",
    "        linear_cache = (A_prev, W, b)\n",
    "        activation_cache = Z\n",
    "\n",
    "        #Here we define two possible activation functions\n",
    "        if activation == \"sigmoid\":\n",
    "            A = 1/(1+np.exp(-Z))\n",
    "        elif activation == \"relu\":\n",
    "            A = np.maximum(0,Z)\n",
    "        assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "        cache = (linear_cache, activation_cache)\n",
    "\n",
    "        return A, cache\n",
    "        \n",
    "#### continue forward propagation through L layer\n",
    "    def L_model_forward(X, parameters):\n",
    "        #Initialize a cache list to keep track of the caches\n",
    "        caches = [] #Your code here\n",
    "        A = X\n",
    "        L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "        # Implement the RELU activation L-1 times. Add \"cache\" to the \"caches\" list.\n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            A, cache = linear_activation_forward(A_prev, parameters['W'+ str(l)], parameters['b' + str(l)], activation = \"relu\")        \n",
    "            caches.append(cache)\n",
    "        #Implement the sigmoid function for the last layer. Add \"cache\" to the \"caches\" list.\n",
    "        AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "\n",
    "        assert(AL.shape == (1,X.shape[1]))\n",
    "\n",
    "        return AL, caches\n",
    "        \n",
    "#### create cost function\n",
    "    def compute_cost(AL, Y):\n",
    "\n",
    "        m = Y.shape[1]\n",
    "\n",
    "        cost = -(1/m)* np.sum((Y*np.log(AL))+ (1-Y)*np.log(1-AL))\n",
    "        cost = np.squeeze(cost)      #turn [[17]] into 17\n",
    "\n",
    "        return cost\n",
    "    \n",
    "#### start backward propagation with a linear backward function  \n",
    "    def linear_backward(dZ, cache):\n",
    "        A_prev, W, b = cache #Unpacking our complex object\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        dW = (1/m) * np.dot(dZ,A_prev.T)\n",
    "        db = (1/m) * np.sum(dZ, axis =1, keepdims = True)\n",
    "        dA_prev = np.dot(W.T , dZ) #Your code here; see the formulas above\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "        \n",
    "#### combine activation to linear backward function\n",
    "    def linear_activation_backward(dA, cache, activation):\n",
    "        linear_cache, activation_cache = cache\n",
    "        Z= activation_cache\n",
    "\n",
    "        if activation == \"sigmoid\": \n",
    "            s = 1/(1+np.exp(-Z))\n",
    "            dZ = dA * s * (1-s)\n",
    "            dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "        elif activation == \"relu\":\n",
    "            dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "            dZ[Z <= 0] = 0\n",
    "            dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "        \n",
    "#### commence backword propagation\n",
    "    def L_model_backward(AL, Y, caches):\n",
    "        grads = {}\n",
    "        L = len(caches) # the number of layers\n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "        # Initializing the backpropagation\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "        # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "        current_cache = caches[L-1]\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "\n",
    "        # Loop from l=L-2 to l=0\n",
    "        for l in reversed(range(L-1)):\n",
    "            # (RELU -> LINEAR) gradients\n",
    "            # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"relu\") #Your code here; use the helper function defined above\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "            grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "        return grads \n",
    "    \n",
    "#### update parameters with updated weights\n",
    "    def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "        L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "\n",
    "        for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Recognition using Deep Learning from Scratch\n",
    "    \n",
    "#### import images\n",
    "    import matplotlib.image as mpimg\n",
    "    filename = 'data/validation/santa/00000448.jpg'\n",
    "    img=mpimg.imread(filename)\n",
    "    plt.imshow(img)\n",
    "    print(img.shape)\n",
    "    plt.show()\n",
    "    \n",
    "#### keras preprocessing through image downgrade\n",
    "    import time\n",
    "    import matplotlib.pyplot as plt\n",
    "    import scipy\n",
    "    from PIL import Image\n",
    "    from scipy import ndimage\n",
    "    from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "    %matplotlib inline\n",
    "    plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "    plt.rcParams['image.interpolation'] = 'nearest'\n",
    "    plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "    np.random.seed(1)\n",
    "    \n",
    "#### set directory path\n",
    "    train_data_dir = 'data/train'\n",
    "    test_data_dir = 'data/validation'\n",
    "\n",
    "#### get all the data in the directory data/validation (132 images), and reshape them\n",
    "    test_generator = ImageDataGenerator().flow_from_directory(\n",
    "            test_data_dir, \n",
    "            target_size=(64, 64), batch_size=132) \n",
    "\n",
    "#### get all the data in the directory data/train (790 images), and reshape them\n",
    "    train_generator = ImageDataGenerator().flow_from_directory(\n",
    "            train_data_dir, \n",
    "            target_size=(64, 64), batch_size=790)\n",
    "\n",
    "#### create the data sets\n",
    "    train_images, train_labels = next(train_generator)\n",
    "    test_images, test_labels = next(test_generator)\n",
    "    \n",
    "#### Explore your dataset again\n",
    "    m_train = train_images.shape[0]\n",
    "    num_px = train_images.shape[1]\n",
    "    m_test = test_images.shape[0]\n",
    "\n",
    "    print (\"Number of training examples: \" + str(m_train))\n",
    "    print (\"Number of testing examples: \" + str(m_test))\n",
    "    print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "    print (\"train_images shape: \" + str(train_images.shape))\n",
    "    print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "    print (\"test_images_orig shape: \" + str(test_images.shape))\n",
    "    print (\"test_labels shape: \" + str(test_labels.shape))\n",
    "    \n",
    "#### Reshape the training and test examples \n",
    "    train_img = train_images.reshape(train_images.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "    test_img = test_images.reshape(test_images.shape[0], -1).T\n",
    "\n",
    "    # Standardize data to have feature values between 0 and 1.\n",
    "    train_x = train_img/255.\n",
    "    test_x = test_img/255.\n",
    "\n",
    "    print (\"train_img's shape: \" + str(train_img.shape))\n",
    "    print (\"test_img's shape: \" + str(test_img.shape))\n",
    "\n",
    "#### Reshape the labels\n",
    "    train_labels_final = train_labels.T[[1]]\n",
    "    test_labels_final = test_labels.T[[1]]\n",
    "\n",
    "    print (\"train_labels_final's shape: \" + str(train_labels_final.shape))\n",
    "    print (\"test_labels_final's shape: \" + str(test_labels_final.shape))\n",
    "    \n",
    "#### putting the functions together\n",
    "    def L_layer_model(X, Y, layers_dims, learning_rate = 0.005, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "        np.random.seed(1)\n",
    "        costs = []                         \n",
    "\n",
    "        # Parameters initialization. (≈ 1 line of code)\n",
    "        parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "        # Loop (gradient descent)\n",
    "        for i in range(0, num_iterations):\n",
    "\n",
    "            # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "            AL, caches = L_model_forward(X, parameters) #Your code here; use the previous helper functions\n",
    "        \n",
    "            # Compute cost.\n",
    "            cost = compute_cost(AL, Y) #Your code here; use the previous helper functions\n",
    "\n",
    "            # Backward propagation.\n",
    "            grads = L_model_backward(AL, Y, caches) #Your code here; use the previous helper functions\n",
    "\n",
    "            # Update parameters.\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)  #Your code here; use the previous helper functions\n",
    "\n",
    "            # Print the cost every 100 training example\n",
    "            if print_cost and i % 100 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            if print_cost and i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        return parameters\n",
    "        \n",
    "#### run function    \n",
    "    parameters = L_layer_model(train_img, train_labels_final, layers_dims, num_iterations = 1000, print_cost = True)\n",
    "    \n",
    "    #make predictions\n",
    "    def predict(X, parameters, y=None):\n",
    "\n",
    "        m = X.shape[1]\n",
    "        n = len(parameters) // 2\n",
    "\n",
    "        # Forward propagation\n",
    "        probs, caches = L_model_forward(X, parameters)\n",
    "\n",
    "        # convert probs to 0/1 predictions\n",
    "        for i in range(0, probs.shape[1]):\n",
    "            if probs[0,i] > 0.50:\n",
    "                probs[0,i] = 1\n",
    "            else:\n",
    "                probs[0,i] = 0\n",
    "\n",
    "        #print (\"predictions: \" + str(probs)); print (\"true labels: \" + str(y))\n",
    "        if type(y) != type(None):\n",
    "            print(\"Accuracy: \"  + str(np.sum((probs == y)/m)))\n",
    "\n",
    "        return probs\n",
    "        \n",
    "#### print misslabeled images\n",
    "    def print_mislabeled_images(classes, X, y, p):\n",
    "        a = p + y\n",
    "        mislabeled_indices = np.asarray(np.where(a == 1))\n",
    "        plt.rcParams['figure.figsize'] = (90.0, 90.0) # set default size of plots\n",
    "        num_images = len(mislabeled_indices[0])\n",
    "        for i in range(num_images):\n",
    "            index = mislabeled_indices[1][i]\n",
    "\n",
    "            plt.subplot(2, num_images, i + 1)\n",
    "            plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
    "            plt.axis('off')\n",
    "            \n",
    "    print_mislabeled_images(list(train_generator.class_indices), test_img, test_labels_final, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
