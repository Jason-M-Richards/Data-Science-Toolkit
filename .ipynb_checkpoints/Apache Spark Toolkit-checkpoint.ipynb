{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheat Sheet (pyspark)\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "\n",
    "#### The three Vs\n",
    "\n",
    "##### VOLUME¶\n",
    "Volume refers to the amount of data generated through websites, portals and online applications in a data-driven business. Especially for online retailers, volume encompasses the available data that are out there and need to be assessed for relevance.\n",
    "\n",
    "##### VELOCITY¶\n",
    "Velocity refers to the speed with which data is generated, and as internet speeds have increased and the number of users has increased, the velocity has also increased substantially.\n",
    "\n",
    "##### VARIETY¶\n",
    "Variety in Big Data refers to all the structured and unstructured data that has the possibility of getting generated either by humans or by machines. Structured data is whatever data you could store in a spreadsheet. It can easily be cataloged and summary statistics can be calculated for it. Unstructured data are raw things like texts, tweets, pictures, videos, emails, voice mails, hand-written text, ECG reading, and audio recordings. Humans can only make sense of data that is structured, and it is usually up to data scientists to create some organization and structure to unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel and Distributed Computing with Map-Reduce\n",
    "\n",
    "MapReduce is a programming paradigm that enables the ability to scale across hundreds or thousands of servers for big data analytics. The underlying concept can be somewhat difficult to grasp, because this paradigm differs from the traditional programming practices. This lesson aims to present a simple yet intuitive account of MapReduce that we shall put into practice in upcoming labs. \n",
    "\n",
    "*In a nutshell, the term \"MapReduce\" refers to two distinct tasks. The first is the __Map__ job, which takes one set of data and transforms it into another set of data, where individual elements are broken down into tuples __(key/value pairs)__, while the __Reduce__ job takes the output from a map as input and combines those data tuples into a smaller set of tuples.*\n",
    "\n",
    "#### Distributed Processing Systems\n",
    ">A distributed processing system is a group of computers in a network working in tandem to accomplish a task\n",
    "\n",
    "#### Parallel Processing\n",
    "With parallel computing:\n",
    "\n",
    "* a larger problem is broken up into smaller pieces\n",
    "* every part of the problem follows a series of instructions\n",
    "* each one of the instructions is executed simultaneously on different processors\n",
    "* all of the answers are collected from the small problems and combined into one final answer\n",
    "\n",
    "#### MapReduce process\n",
    "\n",
    "##### 1. MAP Task ((Splitting & Mapping)\n",
    "The dataset that needs processing must first be transformed into <key:value> pairs and split into fragments, which are then assigned to map tasks. Each computing cluster is assigned a number of map tasks, which are subsequently distributed among its nodes. In this example, let's assume that we are using 5 nodes (a server with 5 different worker.\n",
    "\n",
    "First, split the data from one file or files into however many nodes are being used.\n",
    "\n",
    "We will then use the map function to create key value pairs represented by:   \n",
    "*{animal}* , *{# of animals per zoo}* \n",
    "\n",
    "After processing of the original key:value pairs, some __intermediate__ key:value pairs are generated. The intermediate key:value pairs are __sorted by their key values__ to create a new list of key:value pairs.\n",
    "\n",
    "##### 2. Shuffling\n",
    "This list from the map task is divided into a new set of fragments that sorts and shuffles the mapped objects into an order or grouping that will make it easier to reduce them. __The number these new fragments, will be the same as the number of the reduce tasks__. \n",
    "\n",
    "##### 3. REDUCE Task (Reducing)\n",
    "Now, every properly shuffled segment will have a reduce task applied to it. After the task is completed, the final output is written onto a file system. The underlying file system is usually HDFS (Hadoop Distributed File System). \n",
    "\n",
    "It's important to note that MapReduce will generally only be powerful when dealing with large amounts of data. When using on a small dataset, it will be faster to perform operations not in the MapReduce framework.\n",
    "\n",
    "There are two groups of entities in this process to ensuring that the map reduce task gets done properly:\n",
    "\n",
    "__Job Tracker__: a \"master\" node that informs the other nodes which map and reduce jobs to complete\n",
    "\n",
    "__Task Tracker__: the \"worker\" nodes that complete the map and reduce operations\n",
    "\n",
    "There are different names for these components depending on the technology used, but there will always be a master node that informs worker nodes what tasks to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Context\n",
    "\n",
    "#### Create a local spark context with pyspark\n",
    "    import pyspark\n",
    "    sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "#### Display the type of the Spark Context\n",
    "    type(sc)\n",
    "\n",
    "#### Use Python's dir(obj) to get a list of all attributes of SparkContext\n",
    "    dir(sc)\n",
    "\n",
    "#### Use Python's help ( help(object) ) function to get information on attributes and methods for sc object. \n",
    "    help(sc)\n",
    "\n",
    "#### Check the number of cores being used\n",
    "    print (\"Default number of cores being used:\", sc.defaultParallelism) \n",
    "\n",
    "#### Check for the current version of Spark\n",
    "    print (\"Current version of Spark:\", sc.version)\n",
    "    \n",
    "#### Check the name of application currently running in spark environment\n",
    "    sc.appName\n",
    "    \n",
    "#### Access complete configuration settings (including all defaults) for the current spark context \n",
    "    sc._conf.getAll()\n",
    "    \n",
    "#### Shut down SparkContext\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "Resilient Distributed Datasets (RDD) are fundamental data structures of Spark. An RDD is, essentially, the Spark representation of a set of data, spread across multiple machines, with APIs to let you act on it. An RDD could come from any datasource, e.g. text files, a database, a JSON file etc.\n",
    "\n",
    "#### create RDD\n",
    "    rdd = sc.parallelize(data,numSlices=10) #creates 10 partitions\n",
    "    print(type(rdd))\n",
    "    \n",
    "#### Get # of partitions\n",
    "    rdd.getNumPartitions()\n",
    "    \n",
    "#### Basic actions\n",
    "    rdd.count() #returns the total count of items in the RDD\n",
    "    rdd.first() #returns the first item in the RDD\n",
    "    rdd.take() #returns the first n items in the RDD\n",
    "    rdd.top() #returns the top n items\n",
    "    rdd.collect() #returns everything from your RDD\n",
    "    \n",
    "#### Mapping function to data (creates tuples of paired data)\n",
    "    def sales_tax(num):\n",
    "        return num * 0.92\n",
    "\n",
    "    revenue_minus_tax = price_items.map(sales_tax)\n",
    "    \n",
    "#### Applying lambda function to data\n",
    "    discounted = revenue_minus_tax.map(lambda x : x*0.9)\n",
    "    \n",
    "#### chain methods in spark\n",
    "    price_items.map(sales_tax).map(lambda x : x*0.9).top(15)\n",
    "    \n",
    "#### See the full lineage of all the operations that have been performed on an RDD\n",
    "    discounted.toDebugString()\n",
    "    \n",
    "#### Flatmap (creates a list of data - all same level)\n",
    "    flat_mapped = price_items.flatMap(lambda x : (x, x*0.92*0.9 ))\n",
    "\n",
    "#### A filter method is a specialized form of a map function that only returns the items that match a certain criteria\n",
    "    selected_items = discounted.filter(lambda x: x>300)\n",
    "    \n",
    "####  Use a reduce method with a lambda function to to add up all of the values in the RDD\n",
    "    selected_items.reduce(lambda x,y :x + y)\n",
    "    \n",
    "#### reduceByKey to perform reducing operations while grouping by keys.\n",
    "    total_spent = sales_data.reduceByKey(lambda x,y :x + y)\n",
    "    \n",
    "####  sortBy method on the RDD to rank the users from highest spending to least spending.\n",
    "    total_spent.sortBy(lambda x: x[1],ascending = False).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning in Spark\n",
    "\n",
    "### A Tale of Two Libraries\n",
    "\n",
    "If you look at the pyspark documentation, you'll notice that there are two different libraries for machine learning [mllib](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html) and [ml](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html). These libraries are extremely similar to one another, the only difference being that the mllib library is built upon the RDDs you just practiced using; whereas, the ml library is built on higher level Spark DataFrames (SQL), which has methods and attributes very similar to pandas. It's important to note that these libraries are much younger than pandas and many of the kinks are still being worked out. \n",
    "\n",
    "#### import packages (SparkSession is dataframe instationation)\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.master(\"local\").appName(\"machine learning\").getOrCreate() #opens ml library\n",
    "    spark_df = spark.read.csv('./forestfires.csv',header='true',inferSchema='true') #import data\n",
    "    #observing the datatype of df\n",
    "    type(spark_df)\n",
    "    \n",
    "#### spark methods similar to pandas\n",
    "    spark_df.head() #returns n rows of data as column=data pair \n",
    "    spark_df.columns #returns column names\n",
    "    spark_df[['month','day','rain']].head() #returns first data in selected columns\n",
    "\n",
    "#### Aggregate data for display\n",
    "    spark_df_months = spark_df.groupBy('month').agg({'area':'mean'})\n",
    "    spark_df_months.collect() #must use collect in spark\n",
    "    \n",
    "### ML\n",
    "\n",
    "Pyspark openly admits that they used sklearn as an inspiration for their implementation of a machine learning library. As a result, many of the methods and functionalities look similar, but there are some crucial distinctions. There are four main concepts found within the ML library:\n",
    "\n",
    "`Transformer`: An algorithm that transforms one pyspark DataFrame into another DataFrame. \n",
    "\n",
    "`Estimator`: An algorithm that can be fit onto a pyspark DataFrame that can then be used as a Transformer. \n",
    "\n",
    "`Pipeline`: A pipeline very similar to an sklearn pipeline that chains together different actions.\n",
    "\n",
    "The reasoning behind this separation of the fitting and transforming step is because sklearn is lazily evaluated, so the 'fitting' of a model does not actually take place until the Transformation action is called.\n",
    "\n",
    "#### Creating an ML pipeline object\n",
    "\n",
    "    from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "    #set pipeline parameters\n",
    "    string_indexer = StringIndexer(inputCol='month',outputCol='month_num',handleInvalid='keep')\n",
    "    one_hot_encoder = OneHotEncoderEstimator(inputCols=['month_num'],outputCols=['month_vec'])\n",
    "    vector_assember = VectorAssembler(inputCols=features,outputCol='features')\n",
    "    random_forest = RandomForestRegressor(featuresCol='features',labelCol='area')\n",
    "    stages =  [string_indexer, one_hot_encoder, vector_assember,random_forest]\n",
    "\n",
    "    pipeline = Pipeline(stages=stages) #instantiate pipeline\n",
    "\n",
    "    params = ParamGridBuilder()\\\n",
    "    .addGrid(random_forest.maxDepth, [5,10,15])\\\n",
    "    .addGrid(random_forest.numTrees, [20,50,100])\\\n",
    "    .build() #performs gridsearch on set parameters\n",
    "\n",
    "    reg_evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='area',metricName = 'mae') #evaluates model\n",
    "\n",
    "    cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params,evaluator=reg_evaluator)\n",
    "\n",
    "    cross_validated_model = cv.fit(spark_df) #fits model\n",
    "\n",
    "    cross_validated_model.avgMetrics #returns best metrics based on metricName\n",
    "\n",
    "    #shows selected predictions\n",
    "    predictions = cross_validated_model.transform(spark_df)\n",
    "    predictions.select('prediction','area').show(300)\n",
    "\n",
    "    cross_validated_model.bestModel.stages #checking best model by stage\n",
    "\n",
    "    optimal_rf_model = cross_validated_model.bestModel.stages[3] #looking at stage 3 of process\n",
    "    optimal_rf_model.fe\n",
    "\n",
    "    optimal_rf_model.featureImportances #checking feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Spark Word Count Function\n",
    "\n",
    "    stopWordList = ['', 'the','a','in','of','on','at','for','by','i','you','me'] \n",
    "    def wordCount(filename, stopWordlist):\n",
    "        output = sc.textFile(filename)\n",
    "        words1 = lines.flatMap(lambda x: x.split(' '))\n",
    "        words2 = words1.map(lambda x: (x.lower(), 1))\n",
    "        wordCount = words2.reduceByKey(lambda x,y: x+y)\n",
    "        freqWords = wordCount.filter(lambda x:  x[1] >= 5 )\n",
    "        stopWords = freqWords.filter(lambda x:  x[0] in stopWordList) \n",
    "        output = stopWords.collect()\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
