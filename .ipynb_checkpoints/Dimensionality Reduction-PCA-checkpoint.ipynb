{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality\n",
    "- ensure data is tidy\n",
    "    - all observations have values\n",
    "- high dimensionality\n",
    "    - many columns\n",
    "    - reduction reduces the complexity\n",
    "- require less memory\n",
    "- lower chance of overfitting\n",
    "- remove features that have little/no variance\n",
    "    - use variancethreshold\n",
    "- remove features with many missing values\n",
    "    - find with df.isna().sum/len(df)\n",
    "- remove features with multicollinearity\n",
    "    - find with correlation matrix and sns heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE visualization\n",
    "- t-distributed stochastic neighbor embedding\n",
    "- shows how dimensions relate to each other through colored clusters\n",
    "    - closer the clusters, the more closely related\n",
    "    \n",
    "            # Non-numerical columns in the dataset\n",
    "            non_numeric = ['Branch', 'Gender', 'Component']\n",
    "\n",
    "            # Drop the non-numerical columns from df\n",
    "            df_numeric = df.drop(non_numeric, axis=1)\n",
    "\n",
    "            # Create a t-SNE model with learning rate 50\n",
    "            m = TSNE(learning_rate=50)\n",
    "\n",
    "            # Fit and transform the t-SNE model on the numeric dataset\n",
    "            tsne_features = m.fit_transform(df_numeric)\n",
    "            print(tsne_features.shape)\n",
    "            # Color the points by Gender\n",
    "            sns.scatterplot(x=\"x\", y=\"y\", hue='Gender', data=df)\n",
    "\n",
    "            # Show the plot\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VarianceThreshold\n",
    "- you set the variance threshold\n",
    "- provides a boolean list (mask) of features that exceed that threshold\n",
    "- may need to normalize data if scales and variance levels differ per features\n",
    "\n",
    "        # create boxplot to look at variation\n",
    "        head_df.boxplot()\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        # Normalize the data\n",
    "        normalized_df = head_df / head_df.mean()\n",
    "\n",
    "        normalized_df.boxplot()\n",
    "        plt.show()\n",
    "        \n",
    "        # Normalize the data\n",
    "        normalized_df = head_df / head_df.mean()\n",
    "\n",
    "        # Print the variances of the normalized data\n",
    "        print(normalized_df.var())\n",
    "        \n",
    "        from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "        # Create a VarianceThreshold feature selector\n",
    "        sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "        # Fit the selector to normalized head_df\n",
    "        sel.fit(head_df / head_df.mean())\n",
    "\n",
    "        # Create a boolean mask\n",
    "        mask = sel.get_support()\n",
    "\n",
    "        # Apply the mask to create a reduced dataframe\n",
    "        reduced_df = head_df.loc[:, mask]\n",
    "\n",
    "        print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "#### Stepwise Selection\n",
    "In stepwise selection, you start with and empty model (which only includes the intercept), and each time, the variable that has an associated parameter estimate with the lowest p-value is added to the model (forward step). After adding each new variable in the model, the algorithm will look at the p-values of all the other parameter estimates which were added to the model previously, and remove them if the p-value exceeds a certain value (backward step). The algorithm stops when no variables can be added or removed given the threshold values.\n",
    "\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    def stepwise_selection(X, y, \n",
    "                           initial_list=[], \n",
    "                           threshold_in=0.01, \n",
    "                           threshold_out = 0.05, \n",
    "                           verbose=True):\n",
    "        \"\"\" Perform a forward-backward feature selection \n",
    "        based on p-value from statsmodels.api.OLS\n",
    "        Arguments:\n",
    "            X - pandas.DataFrame with candidate features\n",
    "            y - list-like with the target\n",
    "            initial_list - list of features to start with (column names of X)\n",
    "            threshold_in - include a feature if its p-value < threshold_in\n",
    "            threshold_out - exclude a feature if its p-value > threshold_out\n",
    "            verbose - whether to print the sequence of inclusions and exclusions\n",
    "        Returns: list of selected features \n",
    "        Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "        See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "        \"\"\"\n",
    "        included = list(initial_list)\n",
    "        while True:\n",
    "            changed=False\n",
    "            # forward step\n",
    "            excluded = list(set(X.columns)-set(included))\n",
    "            new_pval = pd.Series(index=excluded)\n",
    "            for new_column in excluded:\n",
    "                model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "                new_pval[new_column] = model.pvalues[new_column]\n",
    "            best_pval = new_pval.min()\n",
    "            if best_pval < threshold_in:\n",
    "                best_feature = new_pval.idxmin()\n",
    "                included.append(best_feature)\n",
    "                changed=True\n",
    "                if verbose:\n",
    "                    print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "            # backward step\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "            # use all coefs except intercept\n",
    "            pvalues = model.pvalues.iloc[1:]\n",
    "            worst_pval = pvalues.max() # null if pvalues is empty\n",
    "            if worst_pval > threshold_out:\n",
    "                changed=True\n",
    "                worst_feature = pvalues.argmax()\n",
    "                included.remove(worst_feature)\n",
    "                if verbose:\n",
    "                    print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "            if not changed:\n",
    "                break\n",
    "        return included\n",
    "        \n",
    "#### Recursive Feature Elimination\n",
    "- can use several estimator functions such as LinReg, LogReg, RandomForest....\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE\n",
    "\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    #creating an RFE using Linear Regression\n",
    "    linreg = LinearRegression()\n",
    "    selector = RFE(linreg, n_features_to_select = 2)\n",
    "    selector = selector.fit(predictors, data_fin[\"mpg\"])\n",
    "    # Print the features and their ranking (high = dropped early on)\n",
    "    print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "    # Print the features that are not eliminated\n",
    "    print(X.columns[rfe.support_])\n",
    "    \n",
    "    \n",
    "#### Forward Selection using Adjusted R-squared    \n",
    "    \n",
    "    import statsmodels.formula.api as smf\n",
    "\n",
    "    def forward_selected(data, response):\n",
    "        \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "        response: string, name of response column in data\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        model: an \"optimal\" fitted statsmodels linear model\n",
    "               with an intercept\n",
    "               selected by forward selection\n",
    "               evaluated by adjusted R-squared\n",
    "        \"\"\"\n",
    "        remaining = set(data.columns)\n",
    "        remaining.remove(response)\n",
    "        selected = []\n",
    "        current_score, best_new_score = 0.0, 0.0\n",
    "        while remaining and current_score == best_new_score:\n",
    "            scores_with_candidates = []\n",
    "            for candidate in remaining:\n",
    "                formula = \"{} ~ {} + 1\".format(response,\n",
    "                                               ' + '.join(selected + [candidate]))\n",
    "                score = smf.ols(formula, data).fit().rsquared_adj\n",
    "                scores_with_candidates.append((score, candidate))\n",
    "            scores_with_candidates.sort()\n",
    "            best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "            if current_score < best_new_score:\n",
    "                remaining.remove(best_candidate)\n",
    "                selected.append(best_candidate)\n",
    "                current_score = best_new_score\n",
    "        formula = \"{} ~ {} + 1\".format(response,\n",
    "                                       ' + '.join(selected))\n",
    "        model = smf.ols(formula, data).fit()\n",
    "        return model\n",
    "        \n",
    "#### Permutation Importance for Classification\n",
    "\n",
    "    #oob classifier accuracy for classification scoring\n",
    "    def oob_classifier_accuracy(rf, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Compute out-of-bag (OOB) accuracy for a scikit-learn random forest\n",
    "        classifier. We learned the guts of scikit's RF from the BSD licensed\n",
    "        code:\n",
    "        https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/forest.py#L425\n",
    "        \"\"\"\n",
    "        X = X_train\n",
    "        y = y_train\n",
    "\n",
    "        n_samples = len(X)\n",
    "        n_classes = len(np.unique(y))\n",
    "        predictions = np.zeros((n_samples, n_classes))\n",
    "        for tree in rf.estimators_:\n",
    "            unsampled_indices = _generate_unsampled_indices(tree.random_state, n_samples)\n",
    "            tree_preds = tree.predict_proba(X[unsampled_indices, :])\n",
    "            predictions[unsampled_indices] += tree_preds\n",
    "\n",
    "        predicted_class_indexes = np.argmax(predictions, axis=1)\n",
    "        predicted_classes = [rf.classes_[i] for i in predicted_class_indexes]\n",
    "\n",
    "        oob_score = np.mean(y == predicted_classes)\n",
    "        return oob_score\n",
    "\n",
    "    #package for PI\n",
    "    import eli5\n",
    "    from eli5.sklearn import PermutationImportance\n",
    "    from sklearn.ensemble.forest import _generate_unsampled_indices\n",
    "    X_train, X_test, y_train, y_test = train_test_split(f_scale,target_resample, random_state=0)\n",
    "    perm = PermutationImportance(rf, cv=5, scoring = oob_classifier_accuracy) #can change scoring for other forms of models\n",
    "    perm.fit(X_train, y_train)\n",
    "    \n",
    "#### Lasso Regressor\n",
    "\n",
    "    # Create the Lasso model\n",
    "    la = Lasso()\n",
    "\n",
    "    # Fit it to the standardized training data\n",
    "    la.fit(X_train_std, y_train)\n",
    "    \n",
    "    # Transform the test set with the pre-fitted scaler\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "\n",
    "    # Calculate the coefficient of determination (R squared) on X_test_std\n",
    "    r_squared = la.score(X_test_std, y_test)\n",
    "    print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "\n",
    "    # Create a list that has True values when coefficients equal 0\n",
    "    zero_coef = la.coef_ == 0\n",
    "\n",
    "    # Calculate how many features have a zero coefficient\n",
    "    n_ignored = sum(zero_coef)\n",
    "    print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))\n",
    "    \n",
    "    # Find the highest alpha value with R-squared above 98%\n",
    "    la = Lasso(alpha = 0.01, random_state=0)\n",
    "\n",
    "    # Fits the model and calculates performance stats\n",
    "    la.fit(X_train_std, y_train)\n",
    "    r_squared = la.score(X_test_std, y_test)\n",
    "    n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "#### LassoCV\n",
    "\n",
    "- finds the optimal alpha value\n",
    "\n",
    "        from sklearn.linear_model import LassoCV\n",
    "        lcv = LassoCV()\n",
    "        lcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Feature Selectors\n",
    "\n",
    "    from sklearn.linear_model import LassoCV\n",
    "\n",
    "    # Create and fit the LassoCV model on the training set\n",
    "    lcv = LassoCV()\n",
    "    lcv.fit(X_train, y_train)\n",
    "    print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "    # Calculate R squared on the test set\n",
    "    r_squared = lcv.score(X_test, y_test)\n",
    "    print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
    "\n",
    "    # Create a mask for coefficients not equal to zero\n",
    "    lcv_mask = lcv.coef_ != 0\n",
    "    print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))\n",
    "\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "    # Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "    rfe_gb = RFE(estimator=GradientBoostingRegressor(), \n",
    "                 n_features_to_select=10, step=3, verbose=1)\n",
    "    rfe_gb.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate the R squared on the test set\n",
    "    r_squared = rfe_gb.score(X_test, y_test)\n",
    "    print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "    \n",
    "    # Assign the support array to gb_mask\n",
    "    gb_mask = rfe_gb.support_\n",
    "    \n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    # Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "    rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
    "                 n_features_to_select=10, step=3, verbose=1)\n",
    "    rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate the R squared on the test set\n",
    "    r_squared = rfe_rf.score(X_test, y_test)\n",
    "    print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "    # Assign the support array to gb_mask\n",
    "    rf_mask = rfe_rf.support_\n",
    "    \n",
    "    # Sum the votes of the three models\n",
    "    votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "    print(votes)\n",
    "    \n",
    "    # Create a mask for features selected by all 3 models\n",
    "    meta_mask = votes >= 3\n",
    "    print(meta_mask)\n",
    "    \n",
    "    # Apply the dimensionality reduction on X\n",
    "    X_reduced = X.loc[:,meta_mask]\n",
    "    print(X_reduced.columns)\n",
    "    \n",
    "    # Plug the reduced dataset into a linear regression pipeline\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "    lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "    r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "    print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "- creates new features from combinations of original features\n",
    "- can lose some information\n",
    "- examples\n",
    "    - creating a mean value from several similar values (height, weight)\n",
    "    - finding price from revenue/quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Component Analysis (PCA)\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\n",
    "\n",
    "- values have to be scaled before applying PCA\n",
    "\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "        # Create the scaler and standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        ansur_std = scaler.fit_transform(ansur_df)\n",
    "        \n",
    "        # Create the PCA instance and fit and transform the data with pca\n",
    "        pca = PCA()\n",
    "        pc = pca.fit_transform(ansur_std)\n",
    "\n",
    "        # This changes the numpy array output back to a dataframe\n",
    "        pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
    "        \n",
    "        # Inspect the explained variance ratio per component\n",
    "        print(pca.explained_variance_ratio_)\n",
    "        \n",
    "        # Print the cumulative sum of the explained variance ratio\n",
    "        print(pca.explained_variance_ratio_.cumsum())\n",
    "        \n",
    "#### PC selection\n",
    "\n",
    "- set threshold in n_components (0.0-1.0) (explains that much variance in data)\n",
    "\n",
    "#### pca.inverse_transform\n",
    "\n",
    "- reverses the fitted and transfomed data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Example: Iris Data\n",
    "\n",
    "- before PCA is performed, ensure that dataset is explored and standardized.\n",
    "\n",
    "        # Initialize an instance of PCA from scikit-learn with n components\n",
    "        pca=PCA(n_components=n)\n",
    "        transformed = pca.fit_transform(X)\n",
    "\n",
    "##### To visualize the components, it will be useful to also look at the target associated with the particular observation. As such, append the target (flower name) to the principal components in a pandas dataframe.\n",
    "\n",
    "    # Create a new dataset from principal components \n",
    "\n",
    "    df = pd.DataFrame(data = transformed, columns = ['PC1', 'PC2'])\n",
    "    result_df = pd.concat([df, iris[['target']]], axis = 1)\n",
    "    result_df.head()\n",
    "\n",
    "#### Visualize Principal Components Using the target data\n",
    "\n",
    "    # PCA scatter plot\n",
    "\n",
    "    plt.style.use('seaborn-dark')\n",
    "    fig = plt.figure(figsize = (10,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('First Principal Component ', fontsize = 15)\n",
    "    ax.set_ylabel('Second Principal Component ', fontsize = 15)\n",
    "    ax.set_title('Principal Component Analysis (2PCs) for Iris Dataset', fontsize = 20)\n",
    "\n",
    "    targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "    colors = ['r', 'g', 'b']\n",
    "    for target, color in zip(targets,colors):\n",
    "        indicesToKeep = iris['target'] == target\n",
    "        ax.scatter(result_df.loc[indicesToKeep, 'PC1']\n",
    "                   , result_df.loc[indicesToKeep, 'PC2']\n",
    "                   , c = color\n",
    "                   , s = 50)\n",
    "    ax.legend(targets)\n",
    "    ax.grid()\n",
    "\n",
    "#### Calculate the variance explained by priciple components\n",
    "\n",
    "    print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "    print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "#### Run a KNeighborsClassifier to classify the dataset after PCA\n",
    "\n",
    "    X = result_df[['PC1', 'PC2']]\n",
    "    y = iris.target\n",
    "    y = preprocessing.LabelEncoder().fit_transform(y)\n",
    "    start = timeit.timeit()\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=9)\n",
    "    model = KNeighborsClassifier()\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    Yhat = model.predict(X_test)\n",
    "    acc = metrics.accuracy_score(Yhat, Y_test)\n",
    "    end = timeit.timeit()\n",
    "    print(\"Accuracy:\",acc)\n",
    "    print (\"Time Taken:\", end - start)\n",
    "\n",
    "##### some accuracy is lost after performing PCA, but computing time is reduced and accuracy can be improved in some complex cases\n",
    "\n",
    "#### Plot decision boundary using principal components \n",
    "\n",
    "    def decision_boundary(pred_func):\n",
    "    \n",
    "###### Set the boundary\n",
    "    \n",
    "    x_min, x_max = X.iloc[:, 0].min() - 0.5, X.iloc[:, 0].max() + 0.5\n",
    "    y_min, y_max = X.iloc[:, 1].min() - 0.5, X.iloc[:, 1].max() + 0.5\n",
    "    h = 0.01\n",
    "    \n",
    "###### build meshgrid\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "###### plot the contour\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.afmhot)\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.Spectral, marker='x')\n",
    "\n",
    "    decision_boundary(lambda x: model.predict(x))\n",
    "\n",
    "    plt.title(\"decision boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Example: Image Recognition\n",
    "\n",
    "#### Obtain Data\n",
    "#### Scrub and Explore\n",
    "#### Baseline Model w/ SVC\n",
    "    from sklearn import svm\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=22)\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "#### Compressing with PCA\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    import seaborn as sns\n",
    "    sns.set_style('darkgrid')\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_train)\n",
    "\n",
    "#### Plot the Explained Variance versus Number of Features\n",
    "\n",
    "    plt.plot(range(1,65), \n",
    "    pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "#### Determine the Number of Features to Capture 95% of the Datasets Variance\n",
    "\n",
    "    total_explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "    n_over_95 = len(total_explained_variance[total_explained_variance >= .95])\n",
    "    n_to_reach_95 = X.shape[1] - n_over_95 + 1\n",
    "    print(\"Number features: {}\\tTotal Variance Explained: {}\".format(n_to_reach_95, total_explained_variance[n_to_reach_95-1]))\n",
    "\n",
    "\n",
    "#### Subset the Dataset\n",
    "\n",
    "    pca = PCA(n_components=n_to_reach_95)\n",
    "    X_pca_train = pca.fit_transform(X_train)\n",
    "    pca.explained_variance_ratio_.cumsum()[-1]\n",
    "\n",
    "#### Refit a Model on the Compressed Dataset\n",
    "\n",
    "    X_pca_test = pca.transform(X_test)\n",
    "    clf = svm.SVC()\n",
    "    %timeit clf.fit(X_pca_train, y_train)\n",
    "    train_pca_acc = clf.score(X_pca_train, y_train)\n",
    "    test_pca_acc = clf.score(X_pca_test, y_test)\n",
    "    print('Training Accuracy: {}\\tTesting Accuracy: {}'.format(train_pca_acc, test_pca_acc))\n",
    "\n",
    "#### Evaluate model and optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Example: Manual Numpy Code\n",
    "\n",
    "\n",
    "    # Normalize the Data\n",
    "    data = data - data.mean()\n",
    "    data.head()\n",
    "    # Calculate the Covariance Matrix\n",
    "    cov_mat = data.cov()\n",
    "    cov_mat\n",
    "    # Calculate the Eigenvectors\n",
    "    import numpy as np\n",
    "    eig_values, eig_vectors = np.linalg.eig(cov_mat)\n",
    "    # Sorting the Eigenvectors to Determine Primary Components\n",
    "    e_indices = np.argsort(eig_values)[::-1] \n",
    "    # Get the index values of the sorted eigenvalues\n",
    "    eigenvectors_sorted = eig_vectors[:,e_indices]\n",
    "    eigenvectors_sorted\n",
    "    # Reprojecting the Data to n dimensions\n",
    "    eigenvectors_sorted[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
