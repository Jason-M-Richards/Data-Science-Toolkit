{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "- deep learning framework\n",
    "- enables fast experimentation\n",
    "- runs on top of other frameworks\n",
    "- fast industry ready models\n",
    "- less code\n",
    "- build any architecture\n",
    "- deploys in multiple platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN with Keras\n",
    "\n",
    "### Non-Sequential Model\n",
    "\n",
    "        from keras.layers import Input, Dense\n",
    "\n",
    "        # Input layer\n",
    "        input_tensor = Input(shape=(1,))\n",
    "\n",
    "        # connect dense layer to input_tensor layer\n",
    "        output_layer = Dense(1)(input_tensor)\n",
    "        \n",
    "        # Build the model\n",
    "        from keras.models import Model\n",
    "        model = Model(input_tensor, output_tensor)\n",
    "        \n",
    "        # Summarize the model\n",
    "        model.summary()\n",
    "\n",
    "### Sequential model\n",
    "\n",
    "    # Import the Sequential model and Dense layer\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "\n",
    "    # Create a Sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add an input layer and a hidden layer with 10 neurons\n",
    "    model.add(Dense(10, input_shape=(2,), activation=\"relu\"))\n",
    "\n",
    "    # Add a 1-neuron output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Summarise your model\n",
    "    model.summary()\n",
    "    \n",
    "### compile model\n",
    "\n",
    "    # Compile your model\n",
    "    model.compile(optimizer = 'adam', loss = 'mse')\n",
    "\n",
    "    print(\"Training started..., this can take a while:\")\n",
    "\n",
    "### train model\n",
    "\n",
    "    # Fit your model on your data for 30 epochs\n",
    "    model.fit(time_steps,y_positions, epochs = 30)\n",
    "\n",
    "### predict new data\n",
    "\n",
    "    # Predict the eighty minute orbit\n",
    "    eighty_min_orbit = model.predict(np.arange(-40, 41))\n",
    "\n",
    "### evaluate results\n",
    "\n",
    "    # Evaluate your model \n",
    "    print(\"Final lost value:\",model.evaluate(time_steps, y_positions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize and Visualize Keras Model Flow\n",
    "\n",
    "\n",
    "        # Import the plotting function\n",
    "        from keras.utils import plot_model\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # Summarize the model\n",
    "        model.summary()\n",
    "\n",
    "        # Plot the model\n",
    "        plot_model(model, to_file='model.png')\n",
    "\n",
    "        # Display the image\n",
    "        data = plt.imread('model.png')\n",
    "        plt.imshow(data)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification with Keras\n",
    "\n",
    "### explore dataset\n",
    "\n",
    "    # Import seaborn\n",
    "    import seaborn as sns\n",
    "\n",
    "    # Use pairplot and set the hue to be our class\n",
    "    sns.pairplot(banknotes, hue='class') \n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Describe the data\n",
    "    print('Dataset stats: \\n', banknotes.describe())\n",
    "\n",
    "    # Count the number of observations of each class\n",
    "    print('Observations per class: \\n', banknotes['class'].value_counts())\n",
    "    \n",
    "### build model\n",
    "\n",
    "    # Import the sequential model and dense layer\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "\n",
    "    # Create a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add a dense layer - use sigmoid activation function as last activation function for binary classification \n",
    "    model.add(Dense(1, input_shape=(4,), activation='sigmoid'))\n",
    "\n",
    "### Compile model\n",
    "  \n",
    "      #use binary crossentropy loss function for binary classification model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    # Display a summary of your model\n",
    "    model.summary()\n",
    "    \n",
    "### train model\n",
    "\n",
    "    # Train your model for 20 epochs\n",
    "    model.fit(X_train, y_train, epochs=20)\n",
    "    \n",
    "### evaluate model\n",
    "\n",
    "    # Evaluate your model accuracy on the test set\n",
    "    accuracy = model.evaluate(X_test, y_test)[1]\n",
    "\n",
    "    # Print accuracy\n",
    "    print('Accuracy:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification with Keras\n",
    "\n",
    "    # Instantiate a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add 3 dense layers of 128, 64 and 32 neurons each\n",
    "    model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "\n",
    "    # Add a dense layer with as many neurons as competitors\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "### compile model\n",
    "\n",
    "    # Compile your model using categorical_crossentropy loss (for multi class classification)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "                  \n",
    "### transforming label strings to numeric data using pandas\n",
    "\n",
    "    # Transform into a categorical variable\n",
    "    darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "    # Assign a number to each category (label encoding)\n",
    "    darts.competitor = darts.competitor.cat.codes \n",
    "\n",
    "    # Print the label encoded competitors\n",
    "    print('Label encoded competitors: \\n',darts.competitor.head())\n",
    "    \n",
    "    # Import to_categorical from keras utils module\n",
    "    from keras.utils import to_categorical\n",
    "\n",
    "    # Use to_categorical on your labels\n",
    "    coordinates = darts.drop(['competitor'], axis=1)\n",
    "    competitors = to_categorical(darts.competitor)\n",
    "\n",
    "    # Now print the to_categorical() result\n",
    "    print('One-hot encoded competitors: \\n',competitors)\n",
    "    \n",
    "### train and evaluate model\n",
    "\n",
    "    # Train your model on the training data for 200 epochs\n",
    "    model.fit(coord_train,competitors_train,epochs=200)\n",
    "\n",
    "    # Evaluate your model accuracy on the test data\n",
    "    accuracy = model.evaluate(coord_test, competitors_test)[1]\n",
    "\n",
    "    # Print accuracy\n",
    "    print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label Classification with Keras\n",
    "\n",
    "- single input can be assigned to more than one class\n",
    "\n",
    "        # Instantiate a Sequential model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Add a hidden layer of 64 neurons and a 20 neuron's input\n",
    "        model.add(Dense(64, input_shape=(20,), activation='relu'))\n",
    "\n",
    "        # Add an output layer of 3 neurons with sigmoid activation for multi label classification\n",
    "        model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "        # Compile your model with adam and binary crossentropy loss for multi label classification\n",
    "        model.compile(optimizer='adam',\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "        # Train for 100 epochs using a validation split of 0.2\n",
    "        model.fit(sensors_train, parcels_train, epochs = 100, validation_split = 0.2)\n",
    "\n",
    "        # Predict on sensors_test and round up the predictions\n",
    "        preds = model.predict(sensors_test)\n",
    "        preds_rounded = np.round(preds)\n",
    "\n",
    "        # Print rounded preds\n",
    "        print('Rounded Predictions: \\n', preds_rounded)\n",
    "\n",
    "        # Evaluate your model's accuracy on the test data\n",
    "        accuracy = model.evaluate(sensors_test, parcels_test)\n",
    "\n",
    "        # Print accuracy\n",
    "        print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Callbacks\n",
    "\n",
    "- function that is executed after the training has finished\n",
    "    - history\n",
    "    - early stopping\n",
    "    - model checkpoint\n",
    "    \n",
    "### using history to plot loss and accuracy differences between train and test sets\n",
    "\n",
    "    # Train your model and save it's history\n",
    "    history = model.fit(X_train, y_train, epochs = 50,\n",
    "                   validation_data=(X_test, y_test))\n",
    "\n",
    "    # Plot train vs test loss during training\n",
    "    plot_loss(history.history['loss'], history.history['val_loss'])\n",
    "\n",
    "    # Plot train vs test accuracy during training\n",
    "    plot_accuracy(history.history['acc'], history.history['val_acc'])\n",
    "    \n",
    "### using EarlyStopping to maximize test accuracy and save best model using ModelCheckpoint\n",
    "\n",
    "    # Import the EarlyStopping and ModelCheckpoint callbacks\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "    # Early stop on validation accuracy\n",
    "    monitor_val_acc = EarlyStopping(monitor = 'val_acc', patience=3)\n",
    "\n",
    "    # Save the best model as best_banknote_model.hdf5\n",
    "    modelCheckpoint = ModelCheckpoint('best_banknote_model.hdf5', save_best_only = True)\n",
    "\n",
    "    # Fit your model for a stupid amount of epochs\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs = 10000000,\n",
    "                        callbacks = [monitor_val_acc, modelCheckpoint],\n",
    "                        validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Model Performance\n",
    "\n",
    "### Learning Curves\n",
    "\n",
    "- visuals that show differences in accuracy and loss from training to test data\n",
    "\n",
    "            # Train your model and save it's history\n",
    "            history = model.fit(X_train, y_train, epochs = 50,\n",
    "                           validation_data=(X_test, y_test))\n",
    "\n",
    "            # Plot train vs test loss during training\n",
    "            plot_loss(history.history['loss'], history.history['val_loss'])\n",
    "\n",
    "            # Plot train vs test accuracy during training\n",
    "            plot_accuracy(history.history['acc'], history.history['val_acc'])\n",
    "          \n",
    "### activation functions\n",
    "\n",
    "- sigmoid\n",
    "- tanh\n",
    "- ReLu\n",
    "- Leaky ReLu\n",
    "- no magic formula for which to use\n",
    "- ReLu makes a good first choice\n",
    "- sigmoids not recommended for deep networks\n",
    "- experiment!\n",
    "\n",
    "### batch size and normalization\n",
    "\n",
    "#### mini batches\n",
    "- great for large datasets that require lots of RAM\n",
    "- performed during .fit as a batch_size= argument\n",
    "\n",
    "#### batch normalization\n",
    "- improves gradient flow\n",
    "- allows higher learning rates\n",
    "- reduces dependence on weight initialization\n",
    "- acts as unintended form of regularization\n",
    "- limits internal covariate shift\n",
    "- applied as a layer in between layers\n",
    "\n",
    "        # Import batch normalization from keras layers\n",
    "        from keras.layers import BatchNormalization\n",
    "\n",
    "        # Build your deep network\n",
    "        batchnorm_model = Sequential()\n",
    "        batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "        batchnorm_model.add(BatchNormalization())\n",
    "        batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "        batchnorm_model.add(BatchNormalization())\n",
    "        batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "        batchnorm_model.add(BatchNormalization())\n",
    "        batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "\n",
    "        # Compile your model with sgd\n",
    "        batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "### Hyperparameter Tuning\n",
    "- random search > grid search\n",
    "- don't use many epochs\n",
    "- use smaller sample of large datasets\n",
    "- play with batch sizes, activations, optimizers and learning rates\n",
    "\n",
    "#### create model\n",
    "\n",
    "    # Creates a model given an activation and learning rate\n",
    "    def create_model(learning_rate=0.01, activation='relu'):\n",
    "\n",
    "        # Create an Adam optimizer with the given learning rate\n",
    "        opt = Adam(lr=learning_rate)\n",
    "\n",
    "        # Create your binary classification model  \n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_shape=(30,), activation=activation))\n",
    "        model.add(Dense(256, activation=activation))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compile your model with your optimizer, loss, and metrics\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "        \n",
    "#### use sklearn to perform randomized search\n",
    "\n",
    "    # Import KerasClassifier from keras wrappers\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "    # Create a KerasClassifier\n",
    "    model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "    # Define the parameters to try out\n",
    "    params = {'activation': ['relu', 'tanh'], 'batch_size': [32, 128, 256], \n",
    "              'epochs': [50, 100, 200], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "\n",
    "    # Create a randomize search cv object passing in the parameters to try\n",
    "    random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
    "\n",
    "    # Running random_search.fit(X,y) would start the search,but it takes too long! \n",
    "    show_results()\n",
    "    \n",
    "#### run best parameters on test data and evaluate\n",
    "\n",
    "    # Create a KerasClassifier\n",
    "    model = KerasClassifier(build_fn = create_model, epochs = 50, \n",
    "                 batch_size = 128, verbose = 0)\n",
    "\n",
    "    # Calculate the accuracy score for each fold\n",
    "    kfolds = cross_val_score(model, X, y, cv = 3)\n",
    "\n",
    "    # Print the mean accuracy\n",
    "    print('The mean accuracy was:', kfolds.mean())\n",
    "\n",
    "    # Print the accuracy standard deviation\n",
    "    print('With a standard deviation of:', kfolds.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Model Architectures\n",
    "\n",
    "### accessing layers, inputs, ouputs and weights\n",
    "\n",
    "    first_layer = model.layers[0]\n",
    "    print(first_layer.input)\n",
    "    print(first_layer.output)\n",
    "    print(first_layer.weights)\n",
    "    \n",
    "### tracking layer inputs to outputs\n",
    "\n",
    "    # Import keras backend\n",
    "    import keras.backend as K\n",
    "\n",
    "    # Input tensor from the 1st layer of the model\n",
    "    inp = model.layers[0].input\n",
    "\n",
    "    # Output tensor from the 1st layer of the model\n",
    "    out = model.layers[0].output\n",
    "\n",
    "    # Define a function from inputs to outputs\n",
    "    inp_to_out = K.function([inp],[out])\n",
    "\n",
    "    # Print the results of passing X_test through the 1st layer\n",
    "    print(inp_to_out([X_test]))\n",
    "\n",
    "### autoencoders\n",
    "\n",
    "- dimensionality reduction\n",
    "- de-noising data\n",
    "- anomoly detection\n",
    "\n",
    "#### build a model\n",
    "\n",
    "    # Start with a sequential model\n",
    "    autoencoder = Sequential()\n",
    "\n",
    "    # Add a dense layer with the original image as input\n",
    "    autoencoder.add(Dense(32, input_shape=(784, ), activation=\"relu\"))\n",
    "\n",
    "    # Add an output layer with as many nodes as the image\n",
    "    autoencoder.add(Dense(784, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile your model\n",
    "    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "    # Take a look at your model structure\n",
    "    autoencoder.summary()\n",
    "    \n",
    "#### build encoder\n",
    "\n",
    "    # Build your encoder\n",
    "    encoder = Sequential()\n",
    "    encoder.add(autoencoder.layers[0])\n",
    "\n",
    "    # Encode the images and show the encodings\n",
    "    preds = encoder.predict(X_test_noise)\n",
    "    show_encodings(preds)\n",
    "    \n",
    "### Categorical Embeddings\n",
    "\n",
    "- inputs are integers, outputs are floats\n",
    "- increases dimensionality, flatten back output layer to 2D array\n",
    "- useful with high cardinality categorical data, images and text\n",
    "\n",
    "        # Imports\n",
    "        from keras.layers import Input, Embedding, Flatten\n",
    "        from keras.models import Model\n",
    "\n",
    "        # Create an input layer for the team ID\n",
    "        teamid_in = Input(shape=(1,))\n",
    "\n",
    "        # Lookup the input in the team strength embedding layer\n",
    "        strength_lookup = team_lookup(teamid_in)\n",
    "\n",
    "        # Flatten the output\n",
    "        strength_lookup_flat = Flatten()(strength_lookup)\n",
    "\n",
    "        # Combine the operations into a single, re-usable model\n",
    "        team_strength_model = Model(teamid_in, strength_lookup_flat, name='Team-Strength-Model')\n",
    "        \n",
    "### Multiple Inputs\n",
    "\n",
    "#### Shared Layers\n",
    "\n",
    "- require functional API\n",
    "- flexible\n",
    "- two input layers -> shared layer -> two output layers\n",
    "\n",
    "        # Load the input layer from keras.layers\n",
    "        from keras.layers import Input\n",
    "\n",
    "        # Input layer for team 1\n",
    "        team_in_1 = Input((1,), name='Team-1-In')\n",
    "\n",
    "        # Separate input layer for team 2\n",
    "        team_in_2 = Input((1,),name='Team-2-In')\n",
    "        \n",
    "        # Lookup team 1 in the team strength model\n",
    "        team_1_strength = team_strength_model(team_in_1)\n",
    "\n",
    "        # Lookup team 2 in the team strength model\n",
    "        team_2_strength = team_strength_model(team_in_2)\n",
    "        \n",
    " #### Merge Layers\n",
    " \n",
    " - add, subtract, multiply, concatenate layers\n",
    " - two input layers -> one merged layer -> one output layer\n",
    " \n",
    "         # Import the Subtract layer from keras\n",
    "        from keras.layers import Subtract\n",
    "\n",
    "        # Create a subtract layer using the inputs from the previous exercise\n",
    "        score_diff = Subtract()([team_1_strength, team_2_strength])\n",
    "        \n",
    "        # Create the model\n",
    "        model = Model([team_in_1, team_in_2], score_diff)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer = 'adam', loss= 'mean_absolute_error')\n",
    "     \n",
    "#### Fit two inputs\n",
    "     \n",
    "        # Get the team_1 column from the regular season data\n",
    "        input_1 = games_season['team_1']\n",
    "\n",
    "        # Get the team_2 column from the regular season data\n",
    "        input_2 = games_season['team_2']\n",
    "\n",
    "        # Fit the model to input 1 and 2 using a list, using score diff as a target\n",
    "        model.fit([input_1, input_2],\n",
    "                  games_season['score_diff'],\n",
    "                  epochs=1,\n",
    "                  batch_size=2048,\n",
    "                  validation_split=0.1,\n",
    "                  verbose=True)\n",
    "                  \n",
    "          # Evaluate the model using these inputs\n",
    "        print(model.evaluate([input_1, input_2], games_season['score_diff'], verbose=False))\n",
    "        \n",
    "#### Three or More Layers\n",
    "\n",
    "- can use combinations of shared and merged layers\n",
    "\n",
    "        # Create an Input for each team\n",
    "        team_in_1 = Input(shape=(1,), name='Team-1-In')\n",
    "        team_in_2 = Input(shape=(1,), name='Team-2-In')\n",
    "\n",
    "        # Create an input for home vs away\n",
    "        home_in = Input(shape=(1,), name='Home-In')\n",
    "\n",
    "        # Lookup the team inputs in the team strength model\n",
    "        team_1_strength = team_strength_model(team_in_1)\n",
    "        team_2_strength = team_strength_model(team_in_2)\n",
    "\n",
    "        # Combine the team strengths with the home input using a Concatenate layer, then add a Dense layer\n",
    "        out = Concatenate()([team_1_strength, team_2_strength, home_in])\n",
    "        out = Dense(1)(out)\n",
    "        \n",
    "        # Import the model class\n",
    "        from keras.models import Model\n",
    "\n",
    "        # Make a Model\n",
    "        model = Model([team_in_1, team_in_2, home_in], out)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "        \n",
    "        # Fit the model to the games_season dataset\n",
    "        model.fit([games_season['team_1'], games_season['team_2'], games_season['home']],\n",
    "                  games_season['score_diff'],\n",
    "                  epochs=1,\n",
    "                  verbose=True,\n",
    "                  validation_split=0.1,\n",
    "                  batch_size=2048)\n",
    "\n",
    "        # Evaluate the model on the games_tourney dataset\n",
    "        print(model.evaluate([games_tourney['team_1'], games_tourney['team_2'], games_tourney['home']],\n",
    "                  games_tourney['score_diff'], verbose=False))\n",
    "                  \n",
    "### Stacking Models\n",
    "\n",
    "- using predictions from models as inputs for new model\n",
    "\n",
    "        # Predict one model on three inputs\n",
    "        games_tourney['pred'] = model.predict([games_tourney['team_1'], games_tourney['team_2'], games_tourney['home']])\n",
    "        \n",
    "        # Create an input layer with 3 columns\n",
    "        input_tensor = Input((3,))\n",
    "\n",
    "        # Pass it to a Dense layer with 1 unit\n",
    "        output_tensor = Dense(1)(input_tensor)\n",
    "\n",
    "        # Create a model\n",
    "        model = Model(input_tensor, output_tensor)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "        \n",
    "        # Fit the model using predicted input\n",
    "        model.fit(games_tourney_train[['home', 'seed_diff', 'pred']],\n",
    "                  games_tourney_train['score_diff'],\n",
    "                  epochs=1,\n",
    "                  verbose=True)\n",
    "                  \n",
    "        # Evaluate the model on the games_tourney_test dataset\n",
    "        print(model.evaluate(games_tourney_test[['home', 'seed_diff', 'prediction']],\n",
    "                   games_tourney_test['score_diff'], verbose=False))\n",
    "                   \n",
    "### Two Output Models\n",
    "\n",
    "- predicting multiple tragets from a model\n",
    "\n",
    "        # Define the input with a shape of 2\n",
    "        input_tensor = Input((2,))\n",
    "\n",
    "        # Define the multiple outputs\n",
    "        output_tensor = Dense(2,)(input_tensor)\n",
    "\n",
    "        # Create a model\n",
    "        model = Model(input_tensor, output_tensor)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam', loss= 'mean_absolute_error')\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(games_tourney_train[['seed_diff', 'pred']],\n",
    "                  games_tourney_train[['score_1', 'score_2']],\n",
    "                  verbose=True,\n",
    "                  epochs=100,\n",
    "                  batch_size=16384)\n",
    "                  \n",
    "        # Print the model's weights\n",
    "        print(model.get_weights())\n",
    "\n",
    "        # Print the column means of the training data\n",
    "                print(games_tourney_train.mean())\n",
    "\n",
    "                # Evaluate the model on the tournament test data\n",
    "        print(model.evaluate(games_tourney_test[['seed_diff', 'pred']], games_tourney_test[['score_1', 'score_2']], verbose=False))\n",
    "        \n",
    "### Performing Regression and Classification in One Model\n",
    "\n",
    "- need to specify different losses for regression and classification\n",
    "- need to input 2 target classes\n",
    "\n",
    "        # Create an input layer with 2 columns\n",
    "        input_tensor = Input((2,))\n",
    "\n",
    "        # Create the first output (regression)\n",
    "        output_tensor_1 = Dense(1, activation='linear', use_bias=False)(input_tensor)\n",
    "\n",
    "        # Create the second output (use the first output as input here) (classification)\n",
    "        output_tensor_2 = Dense(1, activation='sigmoid', use_bias=False)(output_tensor_1)\n",
    "\n",
    "        # Create a model with 2 outputs\n",
    "        model = Model(input_tensor, [output_tensor_1, output_tensor_2])\n",
    "        \n",
    "        # Import the Adam optimizer\n",
    "        from keras.optimizers import Adam\n",
    "\n",
    "        # Compile the model with 2 losses and the Adam optimzer with a higher learning rate\n",
    "        model.compile(loss=['mean_absolute_error', 'binary_crossentropy'], optimizer=Adam(lr=0.01))\n",
    "\n",
    "        # Fit the model to the tournament training data, with 2 inputs and 2 outputs\n",
    "        model.fit(games_tourney_train[['seed_diff', 'pred']],\n",
    "                  [games_tourney_train[['score_diff']], games_tourney_train[['won']]],\n",
    "                  epochs=10,\n",
    "                  verbose=True,\n",
    "                  batch_size=16384)\n",
    "                  \n",
    "        # Print the model weights\n",
    "        print(model.get_weights())\n",
    "\n",
    "        # Print the training data means\n",
    "        print(games_tourney_train.mean())\n",
    "        \n",
    "        # Import the sigmoid function from scipy\n",
    "        from scipy.special import expit as sigmoid\n",
    "\n",
    "        # Weight from the model\n",
    "        weight = 0.14\n",
    "\n",
    "        # Print the approximate win probability predicted close game\n",
    "        print(sigmoid(1 * weight))\n",
    "\n",
    "        # Print the approximate win probability predicted blowout game\n",
    "        print(sigmoid(10 * weight))\n",
    "        \n",
    "        # Evaluate the model on new data\n",
    "        print(model.evaluate(games_tourney_test[['seed_diff', 'pred']],\n",
    "                       [games_tourney_test[['score_diff']], games_tourney_test[['won']]], verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with Keras\n",
    "\n",
    "### standard Conv2D model\n",
    "\n",
    "    # Import the Conv2D and Flatten layers and instantiate model\n",
    "    from keras.layers import Conv2D,Flatten\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add a convolutional layer of 32 filters of size 3x3\n",
    "    model.add(Conv2D(32, input_shape=(28, 28, 1), kernel_size=3, activation='relu'))\n",
    "\n",
    "    # Add a convolutional layer of 16 filters of size 3x3\n",
    "    model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
    "\n",
    "    # Flatten the previous layer output\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add as many outputs as classes with softmax activation\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Obtain a reference to the outputs of the first layer\n",
    "    layer_output = model.layers[0].output\n",
    "\n",
    "    # Build a model using the model's input and the first layer output\n",
    "    first_layer_model = Model(inputs = model.input, outputs = layer_output)\n",
    "\n",
    "    # Use this model to predict on X_test\n",
    "    activations = first_layer_model.predict(X_test)\n",
    "\n",
    "    # Plot the activations of first digit of X_test for the 15th filter\n",
    "    axs[0].matshow(activations[0,:,:,14], cmap = 'viridis')\n",
    "\n",
    "    # Do the same but for the 17th filter now\n",
    "    axs[1].matshow(activations[0,:,:,16], cmap = 'viridis')\n",
    "    plt.show()\n",
    "\n",
    "### using ResNet trained model to classify images\n",
    "\n",
    "    # Import image and preprocess_input\n",
    "    from keras.preprocessing import image\n",
    "    from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "    # Load the image with the right target size for your model\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "    # Turn it into an array\n",
    "    img_array = image.img_to_array(img)\n",
    "\n",
    "    # Expand the dimensions of the image\n",
    "    img_expanded = np.expand_dims(img_array, axis = 0)\n",
    "\n",
    "    # Pre-process the img in the same way original images were\n",
    "    img_ready = preprocess_input(img_expanded)\n",
    "    \n",
    "    # Instantiate a ResNet50 model with imagenet weights\n",
    "    model = ResNet50(weights='imagenet')\n",
    "\n",
    "    # Predict with ResNet50 on your already processed img\n",
    "    preds = model.predict(img_ready)\n",
    "\n",
    "    # Decode predictions\n",
    "    print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "    \n",
    "### padding\n",
    "- adding an additional layer of 0 value pixels\n",
    "- assists in aligning input with output\n",
    "- added as an input argument\n",
    "\n",
    "        #making output size same as input\n",
    "        padding = 'same'\n",
    "\n",
    "### striding\n",
    "- kernel will jump the stated number\n",
    "- allows smaller output than input\n",
    "- added as an input argument\n",
    "        \n",
    "        #adding strides\n",
    "        strides = 2\n",
    "        \n",
    "### dilated\n",
    "- skipping kernels between \n",
    "- good for values varying in scale\n",
    "\n",
    "        #adding dilation\n",
    "        dilation_rate = 2\n",
    "\n",
    "### formula for anticipated output\n",
    "\n",
    "O = ((I - K +2P)/S + 1\n",
    "\n",
    "- O = output size\n",
    "- I = input size (pixels)\n",
    "- K = size of kernel (pixels)\n",
    "- P = size of padding\n",
    "- S = strides\n",
    "\n",
    "### maxpool2D\n",
    "- reduces the # of parameters by pooling pixels together to equate to one value\n",
    "- maxpool2D layer added after each conv2D layer\n",
    "\n",
    "### storing and loading saved weights\n",
    "- use callback method to save best weight\n",
    "\n",
    "        #from keras.callbacks import ModelCheckpoint\n",
    "        # This checkpoint object will store the model parameters\n",
    "        # in the file \"weights.hdf5\"\n",
    "        checkpoint = ModelCheckpoint('weights.hdf5'\n",
    "        , monitor=\n",
    "        'val_loss'\n",
    "        ,\n",
    "        save_best_only=True)\n",
    "        # Store in a list to be used during training\n",
    "        callbacks_list = [checkpoint]\n",
    "        # Fit the model on a training set, using the checkpoint as a\n",
    "        #callback\n",
    "        model.fit(train_data, train_labels, validation_split=0.2,\n",
    "        epochs=3, callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights('weights.hdf5')\n",
    "        model.predict_classes(test_data)\n",
    "        \n",
    "### dropout\n",
    "- assists in regularization\n",
    "- selects a subset of units\n",
    "- ignore it in the first pass and in the back-propagation error\n",
    "- added after a layer where we want units ignored\n",
    "\n",
    "### batch normalization\n",
    "- rescales the output to normalize\n",
    "- added after a layer that should be normalized\n",
    "\n",
    "### there is disharmony between dropout and batch normalization\n",
    "- avoid using together\n",
    "\n",
    "### Accessing parts of the model for visual interpretation\n",
    "\n",
    "    # Load the weights into the model\n",
    "    model.load_weights('weights.hdf5')\n",
    "\n",
    "    # Get the first convolutional layer from the model\n",
    "    c1 = model.layers[0]\n",
    "\n",
    "    # Get the weights of the first convolutional layer\n",
    "    weights1 = c1.get_weights()\n",
    "\n",
    "    # Pull out the first channel of the first kernel in the first layer\n",
    "    kernel = weights1[0][...,0, 0]\n",
    "    print(kernel)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Convolve with the fourth image in test_data\n",
    "    out = convolution(test_data[3, :, :, 0], kernel)\n",
    "\n",
    "    # Visualize the result\n",
    "    plt.imshow(out)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Keras\n",
    "\n",
    "- RNN network (text)\n",
    "- Long / Short Term Memory\n",
    "- when to use\n",
    "    - image captions\n",
    "    - speech to text\n",
    "    - translation\n",
    "    - document summaries\n",
    "    - text generation\n",
    "    - musical composition\n",
    " \n",
    " ### prep data\n",
    " \n",
    "        #Split text into an array of words \n",
    "        words = text.split()\n",
    "\n",
    "        #Make lines of 4 words each, moving one word at a time\n",
    "        lines = []\n",
    "        for i in range(4, len(words)):\n",
    "          lines.append(' '.join(words[i-4:i]))\n",
    "\n",
    "        #Instantiate a Tokenizer, then fit it on the lines\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(lines)\n",
    "\n",
    "        #Turn lines into a sequence of numbers\n",
    "        sequences = tokenizer.texts_to_sequences(lines)\n",
    "        print(\"Lines: \\n {} \\n Sequences: \\n {}\".format(lines[:5],sequences[:5]))\n",
    "\n",
    "### build LSTM model\n",
    "\n",
    "            #Import the Embedding, LSTM and Dense layer\n",
    "        from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "            #Add an Embedding layer with the right parameters\n",
    "        model.add(Embedding(input_dim=vocab_size, output_dim=8, input_length=3))\n",
    "\n",
    "            #Add a 32 unit LSTM layer\n",
    "        model.add(LSTM(32))\n",
    "\n",
    "            #Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dense(vocab_size, activation='softmax'))\n",
    "        model.summary()\n",
    "    \n",
    "### decode predictions\n",
    "\n",
    "            def predict_text(test_text):\n",
    "              if len(test_text.split())!=3:\n",
    "                print('Text input should be 3 words!')\n",
    "                return False\n",
    "\n",
    "              # Turn the test_text into a sequence of numbers\n",
    "              test_seq = tokenizer.texts_to_sequences([test_text])\n",
    "              test_seq = np.array(test_seq)\n",
    "\n",
    "              # Get the model's next word prediction by passing in test_seq\n",
    "              pred = model.predict(test_seq).argmax(axis = 1)[0]\n",
    "\n",
    "              # Return the word associated to the predicted index\n",
    "              return tokenizer.index_word[pred]\n",
    "\n",
    "            # use the predict function to return the next word\n",
    "            predict_text('enter_text_here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Loss vs. Validation Loss with Matplotlib\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Train a model and store the training object\n",
    "    training = model.fit(train_data, train_labels, validation_split=0.2, epochs=3, batch_size=10)\n",
    "    \n",
    "    # Extract the history from the training object\n",
    "    history = training.history\n",
    "\n",
    "    # Plot the training loss \n",
    "    plt.plot(history['loss'])\n",
    "    # Plot the validation loss\n",
    "    plt.plot(history['val_loss'])\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
