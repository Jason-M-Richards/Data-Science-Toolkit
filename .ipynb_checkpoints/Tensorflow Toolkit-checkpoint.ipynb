{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow \n",
    "\n",
    "from tensorflow import tf\n",
    "\n",
    "- open source library for graph-based numerical computation\n",
    "- developed by Google\n",
    "- low and high level APIs\n",
    "\n",
    "### Tensor\n",
    "- generalization of vectors and matrices\n",
    "\n",
    "### Constant\n",
    "- simplest category of a tensor\n",
    "- not trainable\n",
    "- can have any dimension\n",
    "\n",
    "        # Import constant from TensorFlow\n",
    "        from tensorflow import constant\n",
    "\n",
    "        # Convert the credit_numpy array into a tensorflow constant\n",
    "        credit_constant = constant(credit_numpy)\n",
    "\n",
    "        # Print constant datatype\n",
    "        print('The datatype is:', credit_constant.dtype)\n",
    "\n",
    "        # Print constant shape\n",
    "        print('The shape is:', credit_constant.shape)\n",
    "\n",
    "### Variable\n",
    "\n",
    "    from tensorflow import Variable\n",
    "    \n",
    "    # Define the 1-dimensional variable A1\n",
    "    A1 = Variable([1, 2, 3, 4])\n",
    "\n",
    "    # Print the variable A1\n",
    "    print(A1)\n",
    "\n",
    "    # Convert A1 to a numpy array and assign it to B1\n",
    "    B1 = A1.numpy()\n",
    "\n",
    "    # Print B1\n",
    "    print(B1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations\n",
    "\n",
    "- graph based operations\n",
    "    - tensors = edges\n",
    "    - operations = nodes\n",
    "\n",
    "### Addition\n",
    "\n",
    "- tensor addition\n",
    "    - the add() operation performs element-wise addition w/ 2 tensors\n",
    "    - tensors to be added must be the same shape\n",
    "    - can also use the + symbol\n",
    "\n",
    "### Multiplication\n",
    "\n",
    "- tensors must have the same shape\n",
    "\n",
    "#### multiply() for tensors\n",
    "    \n",
    "        # Define tensors A1 and A23 as constants\n",
    "    A1 = constant([1, 2, 3, 4])\n",
    "    A23 = constant([[1, 2, 3], [1, 6, 4]])\n",
    "\n",
    "    # Define B1 and B23 to have the correct shape\n",
    "    B1 = ones_like(A1)\n",
    "    B23 = ones_like(A23)\n",
    "\n",
    "    # Perform element-wise multiplication\n",
    "    C1 = multiply(B1,A1)\n",
    "    C23 = multiply(B23,A23)\n",
    "\n",
    "    # Print the tensors C1 and C23\n",
    "    print('C1: {}'.format(C1.numpy()))\n",
    "    print('C23: {}'.format(C23.numpy()))\n",
    "    \n",
    "#### matmul() for matrix multiplication\n",
    "\n",
    "    # Define features, params, and bill as constants\n",
    "    features = constant([[2, 24], [2, 26], [2, 57], [1, 37]])\n",
    "    params = constant([[1000], [150]])\n",
    "    bill = constant([[3913], [2682], [8617], [64400]])\n",
    "\n",
    "    # Compute billpred using features and params\n",
    "    billpred = matmul(features, params)\n",
    "\n",
    "    # Compute and print the error\n",
    "    error = bill - billpred\n",
    "    print(error.numpy())\n",
    "\n",
    "    \n",
    "### Summation over Tensor\n",
    "\n",
    "- sums over all the dimensions to reduce tensor size\n",
    "\n",
    "#### reduce_sum()\n",
    "\n",
    "    reduce_sum(data, resulting_dim).numpy()\n",
    "    \n",
    "#### gradient()\n",
    "- computes the slope of a function at a point\n",
    "- assists in finding optimums\n",
    "    - minimum - lowest value of a  loss function (change in gradient > 0)\n",
    "    - maximum - highest value of objective function (change in gradient < 0)\n",
    "    - optimum - point where gradient = 0\n",
    "   \n",
    "            def compute_gradient(x0):\n",
    "            # Define x as a variable with an initial value of x0\n",
    "            x = Variable(x0)\n",
    "            with GradientTape() as tape:\n",
    "                tape.watch(x)\n",
    "                # Define y using the multiply operation\n",
    "                y = multiply(x,x)\n",
    "            # Return the gradient of y with respect to x\n",
    "            return tape.gradient(y, x).numpy()\n",
    "\n",
    "            # Compute and print gradients at x = -1, 1, and 0\n",
    "            print(compute_gradient(-1.0))\n",
    "            print(compute_gradient(1.0))\n",
    "            print(compute_gradient(0.0))\n",
    "\n",
    "#### reshape\n",
    "- reshapes a tensor\n",
    "- good for reshaping images\n",
    "\n",
    "        # Reshape the grayscale image tensor into a vector\n",
    "        gray_vector = reshape(gray_tensor, (784, 1))\n",
    "\n",
    "        # Reshape the color image tensor into a vector\n",
    "        color_vector = reshape(color_tensor, (2352, 1))\n",
    "\n",
    "#### random()\n",
    "- populates tensor w/ entries drawn from a prob. distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n",
    "### Input Data\n",
    "\n",
    "- data can be imported using tensorflow for pipeline management\n",
    "- simpler option is to import with pandas and convert to numpy array\n",
    "\n",
    "#### tf.cast()\n",
    "- sets datatypes as a tensor\n",
    "\n",
    "        # Define waterfront as a Boolean using cast\n",
    "        waterfront = tf.cast(housing['waterfront'], tf.bool)\n",
    "        \n",
    "### Loss Functions\n",
    "\n",
    "- used to train models\n",
    "- measure of model fit\n",
    "- goal is to minimize loss\n",
    "- MSE, MAE and Huber loss functions all available through tf.keras.losses.####()\n",
    "\n",
    "        # Import the keras module from tensorflow\n",
    "        from tensorflow import keras\n",
    "\n",
    "        # Compute the mean squared error (mse)\n",
    "        loss = keras.losses.mse(price, predictions)\n",
    "        \n",
    "        # Compute the mean absolute error (mae)\n",
    "        loss = keras.losses.mse(price, predictions)\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "#### univariate\n",
    "\n",
    "    # Define a linear regression model\n",
    "    def linear_regression(intercept, slope, features = size_log):\n",
    "        return intercept + (slope*features)\n",
    "\n",
    "    # Set loss_function() to take the variables as arguments\n",
    "    def loss_function(intercept, slope, features = size_log, targets = price_log):\n",
    "        # Set the predicted values\n",
    "        predictions = linear_regression(intercept, slope, features)\n",
    "\n",
    "        # Return the mean squared error loss\n",
    "        return keras.losses.mse(targets, predictions)\n",
    "\n",
    "    # Compute the loss for different slope and intercept values\n",
    "    print(loss_function(0.1, 0.1).numpy())\n",
    "    print(loss_function(0.1, 0.5).numpy())\n",
    "    \n",
    "    # Initialize an adam optimizer\n",
    "    opt = keras.optimizers.Adam(0.5)\n",
    "\n",
    "    for j in range(100):\n",
    "        # Apply minimize, pass the loss function, and supply the variables\n",
    "        opt.minimize(lambda: loss_function(intercept,slope), var_list=[intercept, slope])\n",
    "\n",
    "        # Print every 10th value of the loss\n",
    "        if j % 10 == 0:\n",
    "            print(loss_function(intercept, slope).numpy())\n",
    "\n",
    "    # Plot data and regression line\n",
    "    plot_results(intercept, slope)\n",
    "    \n",
    "#### multivariate\n",
    "\n",
    "    # Define the linear regression model\n",
    "    def linear_regression(params, feature1 = size_log, feature2 = bedrooms):\n",
    "        return params[0] + feature1*params[1] + feature2*params[2]\n",
    "\n",
    "    # Define the loss function\n",
    "    def loss_function(params, targets = price_log, feature1 = size_log, feature2 = bedrooms):\n",
    "        # Set the predicted values\n",
    "        predictions = linear_regression(params, feature1, feature2)\n",
    "\n",
    "        # Use the mean absolute error loss\n",
    "        return keras.losses.mae(targets, predictions)\n",
    "\n",
    "    # Define the optimize operation\n",
    "    opt = keras.optimizers.Adam()\n",
    "\n",
    "    # Perform minimization and print trainable variables\n",
    "    for j in range(10):\n",
    "        opt.minimize(lambda: loss_function(params), var_list=[params])\n",
    "        print_results(params)\n",
    "        \n",
    "### Batch Training\n",
    "- divides large datasets into batches (epochs)\n",
    "- performed in pandas using chunksize argument and implementing to linear regression model using a loop\n",
    "\n",
    "        #example already has model setup\n",
    "        # Load data in batches\n",
    "        for batch in pd.read_csv('kc_house_data.csv', chunksize=100):\n",
    "            size_batch = np.array(batch['sqft_lot'], np.float32)\n",
    "\n",
    "            # Extract the price values for the current batch\n",
    "            price_batch = np.array(batch['price'], np.float32)\n",
    "\n",
    "            # Complete the loss, fill in the variable list, and minimize\n",
    "            opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
    "\n",
    "        # Print trained parameters\n",
    "        print(intercept.numpy(), slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks (TF)\n",
    "\n",
    "### Dense Layers\n",
    "\n",
    "- applies weights to all nodes from the previous layer\n",
    "\n",
    "#### low level approach\n",
    "\n",
    "    # Initialize bias1\n",
    "    bias1 = Variable(1.0)\n",
    "\n",
    "    # Initialize weights1 as 3x2 variable of ones\n",
    "    weights1 = Variable(ones((3, 2)))\n",
    "\n",
    "    # Perform matrix multiplication of borrower_features and weights1\n",
    "    product1 = matmul(borrower_features, weights1)\n",
    "\n",
    "    # Apply sigmoid activation function to product1 + bias1\n",
    "    dense1 = keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "    # Print shape of dense1\n",
    "    print(\"\\n dense1's output shape: {}\".format(dense1.shape))\n",
    "\n",
    "    # Initialize bias2 and weights2\n",
    "    bias2 = Variable(1.0)\n",
    "    weights2 = Variable(ones((2, 1)))\n",
    "\n",
    "    # Perform matrix multiplication of dense1 and weights2\n",
    "    product2 = matmul(dense1, weights2)\n",
    "\n",
    "    # Apply activation to product2 + bias2 and print the prediction\n",
    "    prediction = keras.activations.sigmoid(product2 + bias2)\n",
    "    print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
    "    print('\\n actual: 1')\n",
    "    \n",
    "### Activation Function\n",
    "\n",
    "- nonlinear operation\n",
    "- sigmoid\n",
    "    - binary classification\n",
    "- relu\n",
    "    - hidden layers\n",
    "- softmax\n",
    "    - multiclass classification (>2 classes)\n",
    "    \n",
    "            # Construct input layer from borrower features\n",
    "            inputs = constant(borrower_features, float32)\n",
    "\n",
    "            # Define first dense layer\n",
    "            dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "            # Define second dense layer\n",
    "            dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
    "\n",
    "            # Define output layer\n",
    "            outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "            # Print first five predictions\n",
    "            print(outputs.numpy()[:5])\n",
    "            \n",
    "### Optimizers\n",
    "\n",
    "- stochastic gradient descent (SGD)\n",
    "    - learning rate\n",
    "    \n",
    "            # Initialize x_1 and x_2\n",
    "            x_1 = Variable(6.0,float32)\n",
    "            x_2 = Variable(0.3,float32)\n",
    "\n",
    "            # Define the optimization operation\n",
    "            opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "            for j in range(100):\n",
    "                # Perform minimization using the loss function and x_1\n",
    "                opt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "                # Perform minimization using the loss function and x_2\n",
    "                opt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "            # Print x_1 and x_2 as numpy arrays\n",
    "            print(x_1.numpy(), x_2.numpy())\n",
    "            \n",
    "- root mean squared (RMS)\n",
    "    - learning rate, momentum, decay\n",
    "    \n",
    "            # Initialize x_1 and x_2\n",
    "            x_1 = Variable(0.05,float32)\n",
    "            x_2 = Variable(0.05,float32)\n",
    "\n",
    "            # Define the optimization operation for opt_1 and opt_2\n",
    "            opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "            opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
    "\n",
    "            for j in range(100):\n",
    "                opt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "                # Define the minimization operation for opt_2\n",
    "                opt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "            # Print x_1 and x_2 as numpy arrays\n",
    "            print(x_1.numpy(), x_2.numpy())\n",
    "            \n",
    "- adaptive movement (Adam)\n",
    "    - learning rate, beta1\n",
    "    \n",
    "### Training Networks\n",
    "\n",
    "- use dropout to prevent overfitting\n",
    "- use random draws for functions without values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks (Keras)\n",
    "\n",
    "#### building models\n",
    "\n",
    "- sequential API Sequential()\n",
    "    - input layer\n",
    "    - hidden layers\n",
    "    - output layers\n",
    "    \n",
    "            # Define a Keras sequential model\n",
    "            model = keras.Sequential()\n",
    "\n",
    "            # Define the first dense layer\n",
    "            model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "            # Define the second dense layer\n",
    "            model.add(keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "            # Define the output layer\n",
    "            model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "            \n",
    "            # Compile the model\n",
    "            model.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "            # Print the model architecture\n",
    "            print(model.summary())\n",
    "            \n",
    "- functional API Input()\n",
    "    - good for merging models\n",
    "    \n",
    "            # For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "                    m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "                    m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "\n",
    "            # For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "            m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "            m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "\n",
    "            # Merge model outputs and define a functional model\n",
    "            merged = keras.layers.add([m1_layer2, m2_layer2])\n",
    "            model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "\n",
    "            # Print a model summary\n",
    "            print(model.summary())\n",
    "            \n",
    "#### training and evaluation models\n",
    "- fit(features, labels, batch_size, epochs, validation_split) arguments\n",
    "- add metrics='accuracy' to the compile step\n",
    "- perform model.evaluation(test_set)\n",
    "\n",
    "        # Set the optimizer, loss function, and metrics\n",
    "        model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Add the number of epochs and the validation split\n",
    "        model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.1)\n",
    "        \n",
    "        # Evaluate the small model using the train data\n",
    "        small_train = small_model.evaluate(train_features, train_labels)\n",
    "\n",
    "        # Evaluate the small model using the test data\n",
    "        small_test = small_model.evaluate(test_features, test_labels)\n",
    "        \n",
    "#### Estimators API model training\n",
    "\n",
    "- less flexible\n",
    "- high level module\n",
    "- enforces best practices\n",
    "- faster deployment\n",
    "- many models to select from\n",
    "- steps\n",
    "    - define feature columns\n",
    "    - load and transform data\n",
    "    - define estimator\n",
    "    - apply train operation\n",
    "    \n",
    "            # Define feature columns for bedrooms and bathrooms\n",
    "            bedrooms = feature_column.numeric_column(\"bedrooms\")\n",
    "            bathrooms = feature_column.numeric_column(\"bathrooms\")\n",
    "\n",
    "            # Define the list of feature columns\n",
    "            feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "            def input_fn():\n",
    "                # Define the labels\n",
    "                labels = np.array(housing.price)\n",
    "                # Define the features\n",
    "                features = {'bedrooms':np.array(housing['bedrooms']), \n",
    "                            'bathrooms':np.array(housing['bathrooms'])}\n",
    "                return features, labels\n",
    "                \n",
    "            # Define the model and set the number of steps\n",
    "            model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "            model.train(input_fn, steps=1)\n",
    "            \n",
    "            # Define the model and set the number of steps\n",
    "            model = estimator.LinearRegressor(feature_columns=feature_list)\n",
    "            model.train(input_fn, steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
