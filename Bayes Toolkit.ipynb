{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "Maximum likelihood estimation is a method that determines values for the parameters of a model. The parameter values are found such that they maximise the likelihood that the process described by the model produced the data that were actually observed.\n",
    "\n",
    "**Two assumptions we made are used so often in Machine Learning that they have a special name together as an entity : \"The i.i.d. assumption\" i.e. Independent and Identically distributed samples.**\n",
    "\n",
    "\\begin{align}\n",
    "    = \\underset{\\theta}{\\operatorname{argmax}}  \\underset{i}{\\operatorname{\\sum}}  \\log  P(x_i | \\theta)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum A Posteriori (MAP)\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "   \\theta_{MAP} = \\underset{\\theta}{\\operatorname{argmax}}  \\underset{i}{\\operatorname{\\sum}}  \\log  P(x_i | \\theta)P(\\theta)\n",
    "    \\text{ - (According to log properties)}\n",
    "\\end{align}\n",
    "\n",
    "So this is our MAP equation. Comparing this with MLE , the key difference is the prior  P(θ) , otherwise they are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Prior Distributions\n",
    "\n",
    "In Bayesian probability theory, if the posterior distributions p(θ | X) are in the same probability distribution family as the prior probability distribution p(θ), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function\n",
    "\n",
    "#### Parameterizations\n",
    "Let C(n, k) denote the binomial coefficient(n, k).\n",
    "\n",
    "- The Bernoulli distribution has probability of success $p$\n",
    "- The beta distribution has PDF: $f(p) = Γ(α + β) pα-1(1-p)β-1 / (Γ(α) Γ(β))$\n",
    "- The geometric distribution has only one parameter, p, and has PMF: $f(x) = p (1-p)x$\n",
    "- The binomial distribution with parameters n and p has PMF: $f(x) = C(n, x) px(1-p)n–x$\n",
    "- The negative binomial distribution with parameters r and p has PMF: $f(x) = C(r + x – 1, x) pr(1-p)x$\n",
    "- The exponential distribution parameterized in terms of the rate λ has PDF: $f(x) = λ exp(-λ x)$\n",
    "- The gamma distribution parameterized in terms of the rate has PDF: $f(x) = βα xα-1exp(-β x) / Γ(α)$\n",
    "- The Poisson distribution has one parameter λ and PMF $f(x) = exp(-λ) λx/ x!$\n",
    "- The normal distribution parameterized in terms of precision $τ (τ = 1/σ2)$ has PDF:\n",
    "$$f(x) = (τ/2π)1/2 exp( -τ(x – μ)2/2 )$$\n",
    "\n",
    "#### Posterior parameters\n",
    "For each sampling distribution, assume we have data $x1, x2, …, xn$\n",
    "\n",
    "- If the sampling distribution for x is binomial(m, p) with m known, and the prior distribution is beta(α, β), the posterior distribution for p is $beta(α + Σxi, β + mn – Σx_i)$. The Bernoulli is the special case of the binomial with m = 1.\n",
    "- If the sampling distribution for x is negative binomial(r, p) with r known, and the prior distribution is beta(α, β), the posterior distribution for p is $beta(α + nr, β + Σxi)$. The geometric is the special case of the negative binomial with r = 1.\n",
    "- If the sampling distribution for x is gamma(α, β) with α known, and the prior distribution on β is gamma(α0, β0), the posterior distribution for β is $gamma(α0 + nα, β0 + Σxi)$. The exponential is a special case of the gamma with α = 1.\n",
    "- If the sampling distribution for x is Poisson(λ), and the prior distribution on λ is gamma(α0, β0), the posterior on λ is $gamma(α0 + Σxi, β0 + n)$.\n",
    "- If the sampling distribution for x is normal(μ, τ) with τ known, and the prior distribution on μ is normal(μ0, τ0), the posterior distribution on μ is $normal((μ0 τ0 + τ Σxi)/(τ0 + nτ), τ0 + nτ)$.\n",
    "- If the sampling distribution for x is normal(μ, τ) with μ known, and the prior distribution on τ is gamma(α, β), the posterior distribution on τ is $gamma(α + n/2, (n-1)S2)$ where S2 is the sample variance.\n",
    "- If the sampling distribution for x is lognormal(μ, τ) with τ known, and the prior distribution on μ is normal(μ0, τ0), the posterior distribution on μ is $normal((μ0 τ0 + τ Πxi)/(τ0 + nτ), τ0 + nτ)$.\n",
    "- If the sampling distribution for x is lognormal(μ, τ) with μ known, and the prior distribution on τ is gamma(α, β), the posterior distribution on τ is $gamma(α + n/2, (n-1)S2)$ where S2 is the sample variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE with Normal Distributions\n",
    "\n",
    "We know the parameters used to desribe a normal distribution are  (μ and σ2)(μ and σ2) . Where  μμ  is the mean and sigma squared identifies the variance in the data.\n",
    "\n",
    "#### MLE in Python\n",
    "\n",
    "    #for normal distribution\n",
    "    from scipy.stats import norm # for generating sample data and fitting distributions\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.style.use('seaborn')\n",
    "    import numpy as np\n",
    "\n",
    "    ##### get data and label\n",
    "\n",
    "    #returns the mean and std\n",
    "    param = norm.fit(sample)\n",
    "    param[0], param[1]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classification\n",
    "\n",
    "MAP is the basis of **Naive Bayes (NB) Classifier**. It is a simple algorithm that uses the integration of maximum likelihood estimation techniques for classification.\n",
    "\n",
    "The fundamental Naive Bayes assumption is that each feature makes an **independent** and **equal** (i.e. are identical) contribution to the outcome. This is known as the **i.i.d assumption**. \n",
    "\n",
    "### Types of Naive Bayes Algorithm\n",
    "\n",
    "Naive Bayes Algorithm works with a number of data distributions for classification tasks. Here are three popular distributions that would routinely come across while doing data analysis. \n",
    "\n",
    "#### Gaussian Naive Bayes\n",
    "\n",
    "When data features values are continuous (i.e. real numbers), NAive Bayes makes the the assumption that the values associated with each class are distributed according to Gaussian/Normal Distribution.\n",
    "\n",
    "If in our data, an attribute say $x$ contains continuous data. We first segment the data by the class and then compute mean $\\mu_{y}$ and Variance ${\\sigma_{y}}^{2}$  of each class.\n",
    "\n",
    "$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$\n",
    "\n",
    "We shall see this approach in practice in the upcoming labs where we take a deep dive into Gaussian Naive Bayes. \n",
    "\n",
    "\n",
    "#### MultiNomial Naive Bayes\n",
    "\n",
    "MultiNomial Naive Bayes is preferred to use on data that is multinomially distributed. \n",
    "\n",
    "\n",
    ">In probability theory, the **Multinomial distribution** is a generalization of the binomial distribution. For example, it models the probability of counts for rolling a k-sided die n times. For n independent trials each of which leads to a success for exactly one of k categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories. (wiki)\n",
    "\n",
    "It is one of the standard classic algorithms, used often with text categorization (classification). Each event in text classification represents the occurrence of a word in a document. [Visit here](https://syncedreview.com/2017/07/17/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation/) for an example on this. \n",
    "\n",
    "#### Bernoulli Naive Bayes\n",
    "\n",
    "Bernoulli Naive Bayes is used on the data that is distributed according to multivariate Bernoulli distributions.i.e., multiple features can be there, but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. So, it requires features to be binary valued. In the context of text data , one can think of categorizing incoming emails as ham / spam etc. Have a quick look at the detailed slides [HERE](http://www.inf.ed.ac.uk/teaching/courses/inf2b/learnSlides/inf2b13-learnlec07-nup.pdf) to see this in action. We shall be developing a similar experiment towards the end of this section. \n",
    "\n",
    "The Bernoulli and Multinomial text models created in Naive Bayes following a \"Bag of Words\" approach perform with similar level of accuracy as more high end classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Data for Normality\n",
    "\n",
    "#### Kolmogorov-Smirnov Test\n",
    "null hypothesis: data is normal. If p-value less than .05, reject null hypothesis\n",
    "\n",
    "https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.kstest.html\n",
    "\n",
    "#one sample test\n",
    "scipy.stats.kstest(data, cdf, args=(), N=20, alternative='two-sided', mode='approx')\n",
    "#two sample test\n",
    "scipy.stats.ks_2samp(data1, data2)[source]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes and Information Retrieval\n",
    "\n",
    "Being on the intersection of machine learning and information retrieval, several issues arise when applying the naive Bayes classifier. David D. Lewis gives an overview of these in the paper [\"Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval\"](https://link.springer.com/content/pdf/10.1007%2FBFb0026666.pdf).\n",
    "\n",
    "We would like you to read through the article, without getting lost in the details and the math. Try to focus on the application areas and the advantages/disadvantages of certain models. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
