{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Basics\n",
    "\n",
    "Networks come in all shapes and sizes.\n",
    "\n",
    "- We can add more features (nodes) in the input layer.\n",
    "- We can add more nodes in the hidden layer. Also, we can simply add more hidden layers. This is what turns a neural network in a \"deep\" neural network (hence, deep learning)\n",
    "- We can have several nodes in the output layer.\n",
    "\n",
    "And there is one more thing that makes deep learning extremely powerful: unlike many other statistical and machine learning techniques, deep learning can deal extremely well with **unstructured data**.\n",
    "\n",
    "Types or Neural networks:\n",
    "- Standard neural networks\n",
    "- Convolutional neural networks (input = images, video)\n",
    "- Recurrent neural networks (input = audio files, text, time series data)\n",
    "- Generative adversarial networks\n",
    "\n",
    "#### Logistic regression as a neural network (no hidden layers)\n",
    "We'll need some expression here in order to make a prediction.\n",
    "The parameters here are $w \\in  \\mathbb{R}^n$ and $b \\in \\mathbb{R}$. Some expression to get to $\\hat y$ could be $\\hat y = w^T x + b$. The problem here is, however, that this type of expression does not ensure that the eventual outcome $ \\hat y$ will be between zero and one, and it could be much bigger than one or even negative!\n",
    "\n",
    "#### activation functions\n",
    "**sigmoid function**\n",
    "Recall that the mathematical expression of the sigmoid is $ a=\\dfrac{1}{1+ \\exp(-z)}$, and this outputs activation values somewhere between 0 and 1.\n",
    "    \n",
    "        def sigmoid(x, derivative=False):\n",
    "        f = 1 / (1 + np.exp(-x))\n",
    "        if (derivative == True):\n",
    "            return f * (1 - f)\n",
    "        return f\n",
    "\n",
    "**tanh**\n",
    "The hyperbolic tangent (or tanh) function goes between -1 and +1, and is in fact a shifted version of the sigmoid function, with formula $ a=\\dfrac{\\exp(z)- \\exp(-z)}{\\exp(z)+ \\exp(-z)}$. For intermediate layers, the tanh function generally performs pretty well because, with values between -1 and +1, the means of the activations coming out are closer to zero! \n",
    "\n",
    "    def tanh(x, derivative=False):\n",
    "        f = np.tanh(x)\n",
    "        if (derivative == True):\n",
    "            return (1 - (f ** 2))\n",
    "        return np.tanh(x)\n",
    "        \n",
    "**ReLU** (Rectified Linear Unit function)\n",
    "This is probably the most popular activation function, along with the tanh! The fact that the activation is exactly 0 when $z <0$  is slightly cumbersome when taking derivatives though. $a=\\max(0,z)$\n",
    "\n",
    "    def relu(x, derivative=False):\n",
    "        f = np.zeros(len(x))\n",
    "        if (derivative == True):\n",
    "            for i in range(0, len(x)):\n",
    "                if x[i] > 0:\n",
    "                    f[i] = 1  \n",
    "                else:\n",
    "                    f[i] = 0\n",
    "            return f\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = x[i]  \n",
    "            else:\n",
    "                f[i] = 0\n",
    "        return f\n",
    "\n",
    "**leaky ReLU**\n",
    "\n",
    "The leaky ReLU solves the derivative issue by allowing for the activation to be slightly negative when $z <0$ ! $a=\\max(0.001*z,z)$\n",
    "\n",
    "    def leaky_relu(x, leakage = 0.05, derivative=False):\n",
    "        f = np.zeros(len(x))\n",
    "        if (derivative == True):\n",
    "            for i in range(0, len(x)):\n",
    "                if x[i] > 0:\n",
    "                    f[i] = 1  \n",
    "                else:\n",
    "                    f[i] = leakage\n",
    "            return f\n",
    "        for i in range(0, len(x)):\n",
    "            if x[i] > 0:\n",
    "                f[i] = x[i]  \n",
    "            else:\n",
    "                f[i] = x[i]* leakage\n",
    "        return f\n",
    "        \n",
    "**arctan**\n",
    "The inverse tangent (arctan) function has a lot of the same qualities that tanh has, but the range roughly goes from -1.6 to 1.6, and  the slope is more gentle than the one we saw using the tanh function.\n",
    "\n",
    "    def arctan(x, derivative=False):\n",
    "        if (derivative == True):\n",
    "            return 1/(1+np.square(x))\n",
    "        return np.arctan(x)\n",
    "\n",
    "    z = np.arange(-10,10,0.2)\n",
    "    \n",
    "#### Loss and Cost Function\n",
    "The **loss function** is used to measure the inconsistency between the predicted value $(\\hat y)$ and the actual label $y$. In logistic regression the loss function is defined as\n",
    "$\\mathcal{L}(\\hat y, y) = - ( y \\log (\\hat y) + (1-y) \\log(1-\\hat y))$. The advantage of this loss function expression is that the optimization space here is convex, which makes optimizing using gradient descent easier. The loss function is defined over 1 particular training sample. \n",
    "\n",
    "The **cost function** takes the average loss over all the samples: $J(w,b) = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})$\n",
    "When you train your logistic regression model, the purpose is to find parameters $w$ and $b$ such that your cost function is minimized!\n",
    "\n",
    "#### Forward propagation\n",
    "Initialize $J= 0$, $dw_1= 0$, $dw_2= 0$, $db= 0$. \n",
    "\n",
    "For each training sample $1,...,l$ you'll need to compute:\n",
    "\n",
    "$ z^{(i)} = w^T x^ {(i)} +b $\n",
    "\n",
    "$\\hat y^{(i)} = \\sigma (z^{(i)})$\n",
    "\n",
    "$dz^{(i)} = \\hat y^{(i)}- y^{(i)}$\n",
    "\n",
    "#### Backward propagation (after updating values with forward propagation)\n",
    "$J_{+1} = - [y^{(i)} \\log (\\hat y^{(i)}) + (1-y^{(i)}) \\log(1-\\hat y^{(i)})$\n",
    "\n",
    "$dw_{1, +1}^{(i)} = x_1^{(i)} * dz^{(i)}$\n",
    "\n",
    "$dw_{2, +1}^{(i)} = x_2^{(i)} * dz^{(i)}$\n",
    "\n",
    "$db_{+1}^{(i)} =  dz^{(i)}$\n",
    "\n",
    "$\\dfrac{J}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{db}{m}$\n",
    "\n",
    "#### Update weights\n",
    "$w_1 := w_1 - \\alpha dw_1$\n",
    "\n",
    "$w_2 := w_2 - \\alpha dw_2$\n",
    "\n",
    "$b := b - \\alpha db$\n",
    "\n",
    "repeat until convergence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks Process Overview\n",
    "\n",
    "To summarize the process once more, we begin by defining a model architecture which includes the number of hidden layers, activation functions (sigmoid or relu) and the number of units in each of these.   \n",
    "\n",
    "We then initialize parameters for each of these layers (typically randomly). After the initial parameters are set, forward propagation evaluates the model giving a prediction, which is then used to evaluate a cost function. Forward propogation involves evaluating each layer and then piping this output into the next layer. \n",
    "\n",
    "Each layer consists of a linear transformation and an activation function.  The parameters for the linear transformation in **each** layer include $W^l$ and $b^l$. The output of this linear transformation is represented by $Z^l$. This is then fed through the activation function (again, for each layer) giving us an output $A^l$ which is the input for the next layer of the model.  \n",
    "\n",
    "After forward propogation is completed and the cost function is evaluated, backpropogation is used to calculate gradients of the initial parameters with respect to this cost function. Finally, these gradients are then used in an optimization algorithm, such as gradient descent, to make small adjustments to the parameters and the entire process of forward propogation, back propogation and parameter adjustments is repeated until the modeller is satisfied with the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Deep Neural Network from Scratch\n",
    "   \n",
    "    import numpy as np\n",
    "    import h5py\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    %matplotlib inline\n",
    "    plt.rcParams['figure.figsize'] = (5.0, 5.0) \n",
    "    plt.rcParams['image.interpolation'] = 'nearest'\n",
    "    plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "    np.random.seed(123)\n",
    "    \n",
    "#### Initialization in an L-layer Neural Network\n",
    "    def initialize_parameters(n_0, n_1, n_2):\n",
    "    np.random.seed(123) \n",
    "    W1 = np.random.randn(n_1, n_0) * 0.05 \n",
    "    b1 = np.zeros((n_1, 1))\n",
    "    W2 =  np.random.randn(n_2, n_1) * 0.05 \n",
    "    b2 = np.zeros((n_2, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "    \n",
    "#### create a dictionary of parameters for W and b given a list of layer dimensions.\n",
    "    #Simply randomly initialize values in accordance to the shape each parameter should have.\n",
    "    #Use random seed 123 (as provided)\n",
    "    def initialize_parameters_deep(list_layer_dimensions):\n",
    "\n",
    "        np.random.seed(123)\n",
    "        parameters = {}\n",
    "        L = len(list_layer_dimensions)           \n",
    "\n",
    "        for l in range(1, L):\n",
    "            parameters['W' + str(l)] = np.random.randn(list_layer_dimensions[l], list_layer_dimensions[l-1])*0.05\n",
    "            parameters['b' + str(l)] = np.zeros((list_layer_dimensions[l], 1))\n",
    "\n",
    "        return parameters\n",
    "    \n",
    "#### forward propagation through linear activation\n",
    "    def linear_activation_forward(A_prev, W, b, activation):\n",
    "        Z = np.dot(W, A_prev) + b #Your code here; see the linear transformation above for how to compute Z\n",
    "        linear_cache = (A_prev, W, b)\n",
    "        activation_cache = Z\n",
    "\n",
    "        #Here we define two possible activation functions\n",
    "        if activation == \"sigmoid\":\n",
    "            A = 1/(1+np.exp(-Z))\n",
    "        elif activation == \"relu\":\n",
    "            A = np.maximum(0,Z)\n",
    "        assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "        cache = (linear_cache, activation_cache)\n",
    "\n",
    "        return A, cache\n",
    "        \n",
    "#### continue forward propagation through L layer\n",
    "    def L_model_forward(X, parameters):\n",
    "        #Initialize a cache list to keep track of the caches\n",
    "        caches = [] #Your code here\n",
    "        A = X\n",
    "        L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "        # Implement the RELU activation L-1 times. Add \"cache\" to the \"caches\" list.\n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            A, cache = linear_activation_forward(A_prev, parameters['W'+ str(l)], parameters['b' + str(l)], activation = \"relu\")        \n",
    "            caches.append(cache)\n",
    "        #Implement the sigmoid function for the last layer. Add \"cache\" to the \"caches\" list.\n",
    "        AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "        caches.append(cache)\n",
    "\n",
    "        assert(AL.shape == (1,X.shape[1]))\n",
    "\n",
    "        return AL, caches\n",
    "        \n",
    "#### create cost function\n",
    "    def compute_cost(AL, Y):\n",
    "\n",
    "        m = Y.shape[1]\n",
    "\n",
    "        cost = -(1/m)* np.sum((Y*np.log(AL))+ (1-Y)*np.log(1-AL))\n",
    "        cost = np.squeeze(cost)      #turn [[17]] into 17\n",
    "\n",
    "        return cost\n",
    "    \n",
    "#### start backward propagation with a linear backward function  \n",
    "    def linear_backward(dZ, cache):\n",
    "        A_prev, W, b = cache #Unpacking our complex object\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        dW = (1/m) * np.dot(dZ,A_prev.T)\n",
    "        db = (1/m) * np.sum(dZ, axis =1, keepdims = True)\n",
    "        dA_prev = np.dot(W.T , dZ) #Your code here; see the formulas above\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "        \n",
    "#### combine activation to linear backward function\n",
    "    def linear_activation_backward(dA, cache, activation):\n",
    "        linear_cache, activation_cache = cache\n",
    "        Z= activation_cache\n",
    "\n",
    "        if activation == \"sigmoid\": \n",
    "            s = 1/(1+np.exp(-Z))\n",
    "            dZ = dA * s * (1-s)\n",
    "            dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "        elif activation == \"relu\":\n",
    "            dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "            dZ[Z <= 0] = 0\n",
    "            dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "        \n",
    "#### commence backword propagation\n",
    "    def L_model_backward(AL, Y, caches):\n",
    "        grads = {}\n",
    "        L = len(caches) # the number of layers\n",
    "        m = AL.shape[1]\n",
    "        Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "        # Initializing the backpropagation\n",
    "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "        # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "        current_cache = caches[L-1]\n",
    "        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "\n",
    "        # Loop from l=L-2 to l=0\n",
    "        for l in reversed(range(L-1)):\n",
    "            # (RELU -> LINEAR) gradients\n",
    "            # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"relu\") #Your code here; use the helper function defined above\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "            grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "        return grads \n",
    "    \n",
    "#### update parameters with updated weights\n",
    "    def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "        L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "\n",
    "        for l in range(L):\n",
    "            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Recognition using Deep Learning from Scratch\n",
    "    \n",
    "#### import images\n",
    "    import matplotlib.image as mpimg\n",
    "    filename = 'data/validation/santa/00000448.jpg'\n",
    "    img=mpimg.imread(filename)\n",
    "    plt.imshow(img)\n",
    "    print(img.shape)\n",
    "    plt.show()\n",
    "    \n",
    "#### keras preprocessing through image downgrade\n",
    "    import time\n",
    "    import matplotlib.pyplot as plt\n",
    "    import scipy\n",
    "    from PIL import Image\n",
    "    from scipy import ndimage\n",
    "    from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "    %matplotlib inline\n",
    "    plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "    plt.rcParams['image.interpolation'] = 'nearest'\n",
    "    plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "    np.random.seed(1)\n",
    "    \n",
    "#### set directory path\n",
    "    train_data_dir = 'data/train'\n",
    "    test_data_dir = 'data/validation'\n",
    "\n",
    "#### get all the data in the directory data/validation (132 images), and reshape them\n",
    "    test_generator = ImageDataGenerator().flow_from_directory(\n",
    "            test_data_dir, \n",
    "            target_size=(64, 64), batch_size=132) \n",
    "\n",
    "#### get all the data in the directory data/train (790 images), and reshape them\n",
    "    train_generator = ImageDataGenerator().flow_from_directory(\n",
    "            train_data_dir, \n",
    "            target_size=(64, 64), batch_size=790)\n",
    "\n",
    "#### create the data sets\n",
    "    train_images, train_labels = next(train_generator)\n",
    "    test_images, test_labels = next(test_generator)\n",
    "    \n",
    "#### Explore your dataset again\n",
    "    m_train = train_images.shape[0]\n",
    "    num_px = train_images.shape[1]\n",
    "    m_test = test_images.shape[0]\n",
    "\n",
    "    print (\"Number of training examples: \" + str(m_train))\n",
    "    print (\"Number of testing examples: \" + str(m_test))\n",
    "    print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "    print (\"train_images shape: \" + str(train_images.shape))\n",
    "    print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "    print (\"test_images_orig shape: \" + str(test_images.shape))\n",
    "    print (\"test_labels shape: \" + str(test_labels.shape))\n",
    "    \n",
    "#### Reshape the training and test examples \n",
    "    train_img = train_images.reshape(train_images.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "    test_img = test_images.reshape(test_images.shape[0], -1).T\n",
    "\n",
    "    # Standardize data to have feature values between 0 and 1.\n",
    "    train_x = train_img/255.\n",
    "    test_x = test_img/255.\n",
    "\n",
    "    print (\"train_img's shape: \" + str(train_img.shape))\n",
    "    print (\"test_img's shape: \" + str(test_img.shape))\n",
    "\n",
    "#### Reshape the labels\n",
    "    train_labels_final = train_labels.T[[1]]\n",
    "    test_labels_final = test_labels.T[[1]]\n",
    "\n",
    "    print (\"train_labels_final's shape: \" + str(train_labels_final.shape))\n",
    "    print (\"test_labels_final's shape: \" + str(test_labels_final.shape))\n",
    "    \n",
    "#### putting the functions together\n",
    "    def L_layer_model(X, Y, layers_dims, learning_rate = 0.005, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "        np.random.seed(1)\n",
    "        costs = []                         \n",
    "\n",
    "        # Parameters initialization. (≈ 1 line of code)\n",
    "        parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "        # Loop (gradient descent)\n",
    "        for i in range(0, num_iterations):\n",
    "\n",
    "            # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "            AL, caches = L_model_forward(X, parameters) #Your code here; use the previous helper functions\n",
    "        \n",
    "            # Compute cost.\n",
    "            cost = compute_cost(AL, Y) #Your code here; use the previous helper functions\n",
    "\n",
    "            # Backward propagation.\n",
    "            grads = L_model_backward(AL, Y, caches) #Your code here; use the previous helper functions\n",
    "\n",
    "            # Update parameters.\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)  #Your code here; use the previous helper functions\n",
    "\n",
    "            # Print the cost every 100 training example\n",
    "            if print_cost and i % 100 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            if print_cost and i % 100 == 0:\n",
    "                costs.append(cost)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        return parameters\n",
    "        \n",
    "#### run function    \n",
    "    parameters = L_layer_model(train_img, train_labels_final, layers_dims, num_iterations = 1000, print_cost = True)\n",
    "    \n",
    "    #make predictions\n",
    "    def predict(X, parameters, y=None):\n",
    "\n",
    "        m = X.shape[1]\n",
    "        n = len(parameters) // 2\n",
    "\n",
    "        # Forward propagation\n",
    "        probs, caches = L_model_forward(X, parameters)\n",
    "\n",
    "        # convert probs to 0/1 predictions\n",
    "        for i in range(0, probs.shape[1]):\n",
    "            if probs[0,i] > 0.50:\n",
    "                probs[0,i] = 1\n",
    "            else:\n",
    "                probs[0,i] = 0\n",
    "\n",
    "        #print (\"predictions: \" + str(probs)); print (\"true labels: \" + str(y))\n",
    "        if type(y) != type(None):\n",
    "            print(\"Accuracy: \"  + str(np.sum((probs == y)/m)))\n",
    "\n",
    "        return probs\n",
    "        \n",
    "#### print misslabeled images\n",
    "    def print_mislabeled_images(classes, X, y, p):\n",
    "        a = p + y\n",
    "        mislabeled_indices = np.asarray(np.where(a == 1))\n",
    "        plt.rcParams['figure.figsize'] = (90.0, 90.0) # set default size of plots\n",
    "        num_images = len(mislabeled_indices[0])\n",
    "        for i in range(num_images):\n",
    "            index = mislabeled_indices[1][i]\n",
    "\n",
    "            plt.subplot(2, num_images, i + 1)\n",
    "            plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
    "            plt.axis('off')\n",
    "            \n",
    "    print_mislabeled_images(list(train_generator.class_indices), test_img, test_labels_final, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Basics\n",
    "\n",
    "    #packages to import\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    from keras import optimizers\n",
    "\n",
    "#### Deciding on the network architecture\n",
    "    model = models.Sequential()\n",
    "\n",
    "#### Adding layers\n",
    "Once we have initialized a network object as shown above, we can then add layers to the network which includes the number of layers we wish to add, as well as which activiation function we hope to use. For example, when coding from scratch, we previously used the sigmoid and ReLu activation functions.   \n",
    "\n",
    "The `Dense` method indicates that this layer will be fully connected. There are other layer architectures that we will discuss further in upcoming labs and lessons.\n",
    "\n",
    "Finally, the `input_shape` parameter is often optional. That is, in successive layers, Keras implies the required shape of the layer to be added based on the shape of the previous layer.\n",
    "\n",
    "    model.add(layers.Dense(units, activation, input_shape))\n",
    "    \n",
    "#### Compiling the model\n",
    "Once we have defined the network architecture and added layers to that network, we then compile the model before then training that model on our data.  \n",
    "\n",
    "    model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "                  loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#### Training the model\n",
    "* **Sample**: one element of a dataset.  \n",
    "    * *Example*: one image is a sample in a convolutional network  \n",
    "    * *Example*: one audio file is a sample for a speech recognition model  \n",
    "    \n",
    "* **Batch**: a set of N samples. The samples in a batch are processed independently, in parallel. If training, a batch results in only one update to the model.  \n",
    "* A batch generally approximates the distribution of the input data better than a single input. The larger the batch, the better the approximation; however, it is also true that the batch will take longer to process and will still result in only one update. For inference (evaluate/predict), it is recommended to pick a batch size that is as large as you can afford without going out of memory (since larger batches will usually result in faster evaluation/prediction).\n",
    "* **Epoch**: an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation.\n",
    "* When using validation_data or validation_split with the fit method of Keras models, evaluation will be run at the end of every epoch.\n",
    "* Within Keras, there is the ability to add callbacks specifically designed to be run at the end of an epoch. Examples of these are learning rate changes and model checkpointing (saving).\n",
    "\n",
    "        history = model.fit(x_train,\n",
    "                            y_train,\n",
    "                            epochs=20,\n",
    "                            batch_size=512,\n",
    "                            validation_data=(x_val, y_val))\n",
    "                            \n",
    "#### Plotting \n",
    "When we fit the model as shown above, we not only update the model object itself, we are also returned a history associated with the model. (Hence our variable name.) With this, we can retrieve further information regarding how the model training progressed from epoch to epoch. To do this, you can access the history attribute of the returned object. Given our variable naming above, we would thus have:\n",
    "\n",
    "```history.history```\n",
    "\n",
    "This will return a dictionary of the metrics we indicated when compiling the model. By default, the loss criteria will always be included as well. So in our example, this dictionary will have 2 keys, one for the loss, and one for the accuracy. If you wish to plot learning curves for the loss or accuracy versus the epochs, you can then simply retrieve these lists. For example:\n",
    "\n",
    "    ```history.history['loss']```\n",
    "\n",
    "would return a list of the loss at each epoch.\n",
    "\n",
    "#### Making Predictions\n",
    "\n",
    "As with sci-kit learn and other prebuilt packages, making predictions from a trained model is relatively straightforward. To do this, you can simply use the `predict` method built into the model object. For example:  \n",
    "    ```{python}\n",
    "    y_hat = model.predict(x)\n",
    "    ```\n",
    "    \n",
    "#### Evaluating the Model\n",
    "\n",
    "Now that the model has been trained, our predictions are applying that model to the data. Similarly, we can use the `evaluate` method in order to compute the loss and other specified metrics for our trained model.\n",
    "\n",
    "For example,   \n",
    "\n",
    "```model.evaluate(X_train, X_train_labels)``` will return the final loss associated with the model for the training data as well as any other metrics that were specified during compilation.\n",
    "\n",
    "Similarly, \n",
    "\n",
    "```model.evaluate(X_test, X_test_labels)``` will return the final loss associated with the model for the test data as well as any other specified metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning NN\n",
    "\n",
    "#### Hyperparameters\n",
    "- number of hidden units\n",
    "- number of layers\n",
    "- learning rate alpha\n",
    "- activation function\n",
    "\n",
    "**Hyperparameter level of performance**\n",
    "Most important:\n",
    "- $\\alpha$\n",
    "\n",
    "Important next:\n",
    "- $\\beta$ (momentum)\n",
    "- Number of hidden units\n",
    "- mini-batch-size\n",
    "\n",
    "Finally:\n",
    "- Number of layers\n",
    "- Learning rate decay\n",
    "\n",
    "Almost never tuned:\n",
    "- $\\beta_1$, $\\beta_2$, $\\epsilon$ (Adam)\n",
    "\n",
    "Things to do:\n",
    "\n",
    "- don't use a grid, because hard to say in advance which hyperparameters will be important\n",
    "\n",
    "#### Training, Test and Validation Sets\n",
    "In short, we'll use 3 sets when running, selecting and validating a model:\n",
    "- You train algorithms on the training set\n",
    "- You'll use a validation set to decide which one will be your final model after parameter tuning\n",
    "- After having chosen the final model (and having evaluated long enough), you'll use the test set to get an unbiased estimate of the classification performance (or whatever your evaluation metric will be).\n",
    "\n",
    "With big data, your dev and test sets don't necessarily need to be 20-30% of all the data. You can choose test and hold-out sets that are of size 1-5%. eg. 96% train, 2% hold-out, 2% test set.\n",
    "\n",
    "If your dataset is small, you can perform K-fold cross-validation on the training set. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`.\n",
    "\n",
    "    random.seed(123)\n",
    "    val = X_train[:1000]\n",
    "    train_final = X_train[1000:]\n",
    "    label_val = y_train[:1000]\n",
    "    label_train_final = y_train[1000:]\n",
    "    \n",
    "#### Normalization\n",
    "- normalized inputs speed up training\n",
    "- prevents exploding or vanishing gradients by having same scale values\n",
    "- Done by:\n",
    "    1. Subtracting the mean\n",
    "    2. dividing by the standard deviation\n",
    "\n",
    "#### Rules of Thumb Regarding Bias / Variance\n",
    "| High Bias? (training performance) | high variance? (validation performance)  |\n",
    "|---------------|-------------|\n",
    "| Use a bigger network|    More data     |\n",
    "| Train longer | Regularization   |\n",
    "| Look for other existing NN architextures |Look for other existing NN architextures |\n",
    "\n",
    "#### Regularization\n",
    "**use regularization when overfitting occurs**\n",
    "\n",
    "- **L1-regularization** is where you just add a term:\n",
    "\n",
    "$$ \\dfrac{\\lambda}{m}||w||_1$$ (could also be 2 in the denominator)\n",
    "\n",
    "- **L2-regularization** is the most common type of regularization.\n",
    "- L2-regularization is called weight decay, because regularization will make your load smaller:\n",
    "\n",
    "$\\bigr(1- \\dfrac{\\alpha\\lambda}{m}\\bigr)$.\n",
    "\n",
    "Intuition for regularization: the weight matrices will be penalized from being too large. Actually, the network will be forced to almost be simplified.\n",
    "\n",
    "Also: eg, tanh function, if $w$ small, the activation function will be mostly operating in the linear region and not \"explode\" as easily.\n",
    "\n",
    "- **Dropout Regularization** For each node, drop a coin and drop them out (you can also alter the dropout probability to be different from 0.5).\n",
    "- In different iterations through the training set, different nodes will be zeroed out!\n",
    "- **When making predictions, don't do dropout!**\n",
    "\n",
    "- **Early stopping** Overfitting happens when the model is overtrained. Early stopping reduces the number of epochs which reduces overfitting\n",
    "- Need to visualize loss and accuracy plots and find epoch where loss is minimized and accuracy is maximized.\n",
    "\n",
    "#### Optimization\n",
    "In addition, we could even use an alternative convergence algorithm instead of gradient descent. One issue with gradient descent is that it oscillates to a fairly big extent, because the derivative is bigger in the vertical direction. There are other algorithms that run faster:\n",
    "\n",
    "**Gradient Descent with Momentum**\n",
    "Compute an exponentially weighthed average of the gradients and use that gradient instead. The intuitive interpretation is that this will successively dampen oscillations, improving convergence.\n",
    "\n",
    "Momentum:\n",
    "compute dW and db on the current minibatch.\n",
    "\n",
    "Combute $V_{dw} = \\beta V_{dw} + (1-\\beta)dW$ and\n",
    "\n",
    "Combute $V_{db} = \\beta V_{db} + (1-\\beta)db$\n",
    "\n",
    "--> moving average for the derivatives of W and b\n",
    "\n",
    "$W:= W- \\alpha Vdw$\n",
    "\n",
    "$b:= b- \\alpha Vdb$\n",
    "\n",
    "This averages out gradient descent, and will \"dampen\" oscillations\n",
    "Generally, $\\beta=0.9$ is a good hyperparameter value.\n",
    "\n",
    "**RMSprop**\n",
    "RMSprop: \"root mean square\" prop.\n",
    "\n",
    "    model.compile(optimizer= \"rmsprop\" ,loss='mse',metrics=['mse'])\n",
    "\n",
    "Slow down learning on one direction and speed up in another one.\n",
    "\n",
    "On each iteration, use exponentially weithed average again:\n",
    "exponentially weighted average of the squares of the derivatives\n",
    "\n",
    "$S_{dw} = \\beta S_{dw} + (1-\\beta)dW^2$\n",
    "\n",
    "$S_{db} = \\beta S_{dw} + (1-\\beta)db^2$\n",
    "\n",
    "$W:= W- \\alpha \\dfrac{dw}{\\sqrt{S_{dw}}}$ and\n",
    "\n",
    "$b:= b- \\alpha \\dfrac{db}{\\sqrt{S_{db}}}$\n",
    "\n",
    "In the direction where we want to learn fast, the corresponding S will be small, so dividing by a small number. On the other hand, in the direction where we will want to learn slow, the corresponding S will be relatively large, and updates will be smaller. \n",
    "\n",
    "Often, add small $\\epsilon$ in the denominator to make sure that you don't end up dividing by 0.\n",
    "\n",
    "**Adam Optimization Algorithm**\n",
    "\"Adaptive Moment Estimation\", basically using the first and second moment estimations.\n",
    "\n",
    "    model.compile(optimizer= \"Adam\" ,loss='mse',metrics=['mse'])\n",
    "\n",
    "Works very well in many situations!\n",
    "\n",
    "Taking momentum and RMSprop and putting it together!\n",
    "\n",
    "Initialize:\n",
    "\n",
    "$V_{dw}=0, S_{dw}=0, V_{db}=0, S_{db}=0$.\n",
    "\n",
    "each iteration:\n",
    "Compute $dW, db$ using the current mini-batch\n",
    "\n",
    "$V_{dw} = \\beta_1 V_{dw} + (1-\\beta_1)dW$, $V_{db} = \\beta_1 V_{db} + (1-\\beta_1)db$ \n",
    "\n",
    "$S_{dw} = \\beta_2 S_{dw} + (1-\\beta_2)dW^2$, $S_{db} = \\beta_2 S_{db} + (1-\\beta_2)db^2$ \n",
    "\n",
    "Is like momentum and then RMSprop. We need to perform a correction! This is sometimes also done in RSMprop, but definitely here too.\n",
    "\n",
    "\n",
    "$V^{corr}_{dw}= \\dfrac{V_{dw}}{1-\\beta_1^t}$, $V^{corr}_{db}= \\dfrac{V_{db}}{1-\\beta_1^t}$\n",
    "\n",
    "$S^{corr}_{dw}= \\dfrac{S_{dw}}{1-\\beta_2^t}$, $S^{corr}_{db}= \\dfrac{S_{db}}{1-\\beta_2^t}$\n",
    "\n",
    "$W:= W- \\alpha \\dfrac{V^{corr}_{dw}}{\\sqrt{S^{corr}_{dw}+\\epsilon}}$ and\n",
    "\n",
    "$b:= b- \\alpha \\dfrac{V^{corr}_{db}}{\\sqrt{S^{corr}_{db}+\\epsilon}}$ \n",
    "\n",
    "**Learning rate decay**\n",
    "Learning rate decreases across epochs.\n",
    "\n",
    "    sgd = optimizers.SGD(lr=0.03, decay=0.0001, momentum=0.9)\n",
    "\n",
    "$\\alpha = \\dfrac{1}{1+\\text{decay_rate * epoch_nb}}* \\alpha_0$\n",
    "\n",
    "other methods:\n",
    "\n",
    "$\\alpha = 0.97 ^{\\text{epoch_nb}}* \\alpha_0$ (or exponential decay)\n",
    "\n",
    "or\n",
    "\n",
    "$\\alpha = \\dfrac{k}{\\sqrt{\\text{epoch_nb}}}* \\alpha_0$\n",
    "\n",
    "or\n",
    "\n",
    "Manual decay!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNN)\n",
    "\n",
    "CNNs have certain features that identify patterns in images because of  \"convolution operation\" including:\n",
    "\n",
    "- Dense layers learn global patterns in their input feature space\n",
    "\n",
    "- Convolution layers learn local patterns, and this leads to the following interesting features:\n",
    "    - Unlike with densely connected networks, when a convolutional neural network recognizes a patterns let's say, in the upper-right corner of a picture, it can recognize it anywhere else in a picture. \n",
    "    - Deeper convolutional neural networks can learn spatial hierarchies. A first layer will learn small local patterns, a second layer will learn larger patterns using features of the first layer patterns, etc. \n",
    "     \n",
    "Because of these properties, CNNs are great for tasks like:\n",
    "- Image classification\n",
    "- Object detection in images\n",
    "- Picture neural style transfer\n",
    "\n",
    "**In Keras, function for the convolution step is Conv2D. The convolutional operation applies a filter (typically 3x3 or 5x5) to each possible 3x3 or 5x5 region of the original image**\n",
    "\n",
    "### Padding\n",
    "\n",
    "There are some issues with using filters on images including: \n",
    "\n",
    "- The image shrinks with each convolution layer: you're throwing away information in each layer! For example:\n",
    "    - Starting from a 5 x 5 matrix, and using a 3 x 3 matrix, you end up with a 3 x 3 image. \n",
    "    - Starting from a 10 x 10 matrix, and using a 3 x 3 matrix, you end up with a 8 x 8 image. \n",
    "    - etc.\n",
    "- The pixels around the edges are used much less in the outputs due to the filter. \n",
    "\n",
    "Fortunately, padding solves both of these problems! Just one layer of pixels around the edges preserves the image size when having a 3 x 3 filter. We can also use bigger filters, but generally the dimensions are odd!\n",
    "\n",
    "Some further terminology regarding padding that you should be aware of includes:\n",
    "\n",
    "- \"Valid\" - no padding\n",
    "- \"Same\" - padding such that output is same as the input size\n",
    "\n",
    "By adding padding to our 5x5 image, (now a 6x6 image by adding a border of pixels) we can add padding so that each pixel of our original 5x5 image can be the center of a 3x3 convolution window filter.\n",
    "\n",
    "### Strided convolutions\n",
    "\n",
    "The stride is how the convolution filter is moved over the original image. In our above example, we moved the filter one pixel to the right starting from the upper left hand corner, and then began to do this again after moving the filter one pixel down. Alternatively, by changing the stride, we could move our filter by 2 pixels each time, resulting in a smaller number of possible locations for the filter.\n",
    "\n",
    "Strided convolutions are rarely used in practice but a good feature to be aware of for some models.\n",
    "\n",
    "### RGB in CNN\n",
    "\n",
    "Instead of 5 x 5 grayscale, let's imagine a 7 x 7 RGB image, which boils down to having a 7 x 7 x 3 tensor. (The image itelf is compromised by a 7 by 7 matrix of pixels, each with 3 numerical values for the RGB values.) From there, you will need to use a filter that has the third dimension equal to 3 as well, let's say, 3 x 3 x 3 (a 3D \"cube\").\n",
    "\n",
    "This allows us to detect, eg only horizontal edges in the blue channel (filter on the red and green channel all equal to 0).\n",
    "\n",
    "Then, in each layer, you can convolve with several 3D filters. Then, you stack every output result together, and that way you end up having a 5 x 5 x (number of filters) shape.\n",
    "\n",
    "### Pooling Layer\n",
    "\n",
    "The last element in a CNN architecture (before fully connected layers as we have previously discussed in other neural networks) is the pooling layer. This layer is meant to substantially downsample the previous convolutional layers. The idea behind this is that the previous convolutional layers will find patterns such as edges or other basic shapes present in the pictures. From there, pooling layers such as Max pooling (the most common) will take a summary of the convolutions from a larger section. In practice Max pooling (taking the max of all convolutions from a larger area of the original image) works better then average pooling as we are typically looking to detect whether a feature is present in that region. Downsampling is essential in order to produce viable execution times in the model training.\n",
    "\n",
    "Max pooling has some important hyperparameters:\n",
    "- f (filter size)\n",
    "- S (stride)\n",
    "\n",
    "Common hyperparameters include: f=2, s=2 and f=3, s=2, this shrinks the size of the representations.\n",
    "If a feature is detected anywhere in the quadrants, a high number will appear. so max pooling preserves this feature.\n",
    "\n",
    "### Fully Connected Layers in Your CNN.\n",
    "\n",
    "Once you have addded a number of convolutional layers and pooling layers, add fully connected (dense) layers as we did before in previous neural network models. This now allows the network to learn a final decision function based on these transformed informative inputs generating from the convolutional and pooling layers.\n",
    "\n",
    "### Pretrained CNN\n",
    "\n",
    " A pretrained network is a network which was previously ran on a large, general data set, and saved. The advantage is that the hierarchical features learned by this network can act as a generic model, and can be used for a wide variety of computer vision tasks, even if your new problem involves completely different classes of images.\n",
    " \n",
    " Keras has several pretrained models available. Here is a list of pretrained image classification models. All these models are available in `keras.applications` and were pretrained on the ImageNet dataset, a data set with 1.4 Million labeled images and 1,000 different classes.\n",
    "\n",
    "- DenseNet\n",
    "- InceptionResNetV2\n",
    "- InceptionV3\n",
    "- MobileNet\n",
    "- NASNet\n",
    "- ResNet50\n",
    "- VGG16\n",
    "- VGG19\n",
    "- Xception\n",
    "\n",
    "You can find an overview here too: https://keras.io/applications/\n",
    "\n",
    "You can simply import the desired pretrained model, and use it as a function with 2 arguments: **weights and include_top**. Use \"imagenet\" in weights in order to use the weights that were obtained when training on the ImageNet data set. You can chose to iclude the top of the model (whether or not to include the fully-connected layer at the top of the network), through the argument include_top.\n",
    "\n",
    "#### Feature Extration (pretrained CNN)\n",
    "Feature extraction with convolutional neural networks means that you take the convolutional base of a pretrained network, run new data through it, and train a new classifier on top of the output (a new densely connected classifier). Why use convolutional base but *new* dense classifier? Generally, patterns learned by the convolutional layers are more generalizable.\n",
    "\n",
    "Note that, if your dataset differs a lot from the dataset used when pretraining, it might even be worth only using part of the convolutional base (see \"fine-tuning\")\n",
    "\n",
    "Also, with feature extraction, there are two ways running the model:\n",
    "- You can run the convolutional base over your dataset, save its output, then use this data as input to a standalone, densely connected network. This solution is pretty fast to run, and you need to run the convolutional base first for every input image. The problem here is, however, that you can't use data augmentation as we've seen it before.\n",
    "- You can extend the convolutional base by adding dense layers on top, and running everything altogether on the input data. This way, you can use data augmentation, but as every input image goes through the convolutional base every time, this technique is much more time-consuming. It's almost impossible to do this without a GPU.\n",
    "\n",
    "        #add pretrained CNN as top layer\n",
    "        model = models.Sequential()\n",
    "        model.add(cnn_base)\n",
    "        #add dense layers for a classifier\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(132, activation='relu'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        #freeze convolutional base\n",
    "        cnn_base.trainable = False\n",
    "        #perform sanity check \n",
    "        #You can check whether a layer is trainable (or alter its setting) through the layer.trainable attribute:\n",
    "        for layer in model.layers:\n",
    "            print(layer.name, layer.trainable)\n",
    "\n",
    "        #Similarly, we can check how many trainable weights are in the model:\n",
    "        print(len(model.trainable_weights))\n",
    "\n",
    "#### Fine-tuning  (pretrained CNN)\n",
    "\n",
    "Fine tuning is similar to feature extraction in that you reuse the convolution base and retrain the dense, fully connected classifier layers to output a new prediction. In addition, fine tuning also works by retraining the frozen weights for the convolutional base. This allows these weights to be tweaked for the current scenario, hence the name. To do this, you'll freeze part of the model while tuning specific layers.\n",
    "\n",
    "**must use feature extraction before using fine tuning**\n",
    "    \n",
    "    #unfreeze convolutional base\n",
    "    cnn_base.trainable = True\n",
    "    #select (fine tune) layers to be frozen/unfrozen\n",
    "    cnn_base.trainable = True\n",
    "    set_trainable = False\n",
    "    for layer in cnn_base.layers:\n",
    "        if layer.name == 'block5_conv1':\n",
    "            set_trainable = True\n",
    "        if set_trainable:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN - Sequence Models\n",
    "\n",
    "A Sequence Model is a general term for a special class of Deep Neural Networks that work with a time series of data as an input. The series of data is any set of data where we want the model to consider the data one point at a time, in order. This means that they are great for problems where the order of the data matters--for instance, stock price data or text. In both cases, the data only makes sense in order.\n",
    "\n",
    "#### Use Cases\n",
    "**_Text Classification_**\n",
    "One of the most common applications of RNNs is for plain old text classification. Recall that all the models that we've used so far for text generation have been incapable of focusing on the order of the words, which means that they're likely to miss out on more advanced pieces of information such as connotation, context, sarcasm, etc. However, since RNNs examine the words one at a time and remember what they've seen at each time step, they're able to capture this information quite effectively in most cases!\n",
    "\n",
    "**_Sequence Generation_**\n",
    "Sequence generation is probably some of the most fun that you can have with Neural Networks, because they excel at coming up with wacky, almost-human sounding names for things when fed the right data. \n",
    "\n",
    "**_Sequence to Sequence Model_**\n",
    "If you've ever used Google Translate before, then you've already interacted with a **_Sequence to Sequence Model_**. These models learn to map an input sequence to an output sequence, usually through an **_Encoder-Decoder_** architecture.\n",
    "\n",
    "#### RNN Architecture\n",
    "A basic Recurrent Neural Network is just a neural network that passes it's output from a given example back into itself as input for the next example.\n",
    "\n",
    "##### Back Propagation\n",
    "One interesting aspect of working with RNNs is that they use a modified form of back propagation called **_Back Propagation Through Time (BPTT)_**. Because the model is trained on sequence data, it has the potential to be right or wrong at every point in that sequence. This means that we need to adjust the model's weights at each time point to effectively learn from sequence data. Because the model starts at the most recent output, and then works backwards to calculate the loss and update the weights at each time step, the model is said to be going \"back in time\" to learn.  Since we have to update the every single weight at every single time step, that means that BPTT is much more computationally expensive than traditional back propagation. For instance, if a single data point is a sequence with 1000 time steps, then our model will perform a full round of back propagation for each of the 1000 points in that single sequence. \n",
    "\n",
    "##### Truncated Back Prop Through Time\n",
    "This algorithm increases performance by breaking a big sequence of 1000 into 50 sequences of 20. This significantly improves training time over regular BPTT, but is still significantly slower than vanilla back propagation. \n",
    "\n",
    "#### Vanishing and Exploding Gradients\n",
    "\n",
    "One of the biggest problems with standard Recurrent Neural Networks is that they get Saturated. The problem with this it that they use a sigmoid or tanh activation function, and there are large areas of each function where the derivative is very, very close to 0. When gradients are close to 0 because the values are extremely low, this is called **Vanishing Gradient**. Similarly, networks can also get the point where the gradients are much, much too large, resulting in massive weight updates that cause the model to thrash between 1 extremely wrong answer and another. When this happens, it is called **Exploding Gradient**. In practice, we can easily solve Exploding Gradients by just \"clipping\" the weight updates by bounding them at a maximum value. However, there's no good solution for Vanishing Gradients! This is where the modern versions of RNNs come in. In practice, when building models for Sequence Data, people rarely use traditional RNN architectures anymore. Instead they make use of LSTMs and GRUs. Both of these models can be thought of as special types of neurons that can be used in an RNN. Although they work a little differently, they have the same strength--the ability to forget information! By constantly updating their internal state, they can learn what is important to remember, and when it is okay to forget it.\n",
    "\n",
    "### Gradient Recurring Network (GRU)\n",
    "\n",
    "**_Gated Recurrent Units_**, or **_GRUs_**, are a special type of cell that passes along it's internal state at each time step. However, not every part of the internal state is passed along--only the important stuff! GRUs make use of two \"gate\" functions: a **_Reset Gate_**, which determines what should be removed from the cell's internal state before passing itself along to the next time step, and an **_Update Gate_**, which determines how much of the state from the previous time step should be used in the current time step.\n",
    "\n",
    "    #sample GRU model\n",
    "    gru_model = Sequential()\n",
    "    gru_model.add(Embedding(20000, 128))\n",
    "    gru_model.add(GRU(50, return_sequences=True))\n",
    "    gru_model.add(GlobalMaxPool1D())\n",
    "    gru_model.add(Dropout(0.5))\n",
    "    gru_model.add(Dense(50, activation='relu'))\n",
    "    gru_model.add(Dropout(0.5))\n",
    "    gru_model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "### Long Short Term Memory Cells (LSTMs)\n",
    "\n",
    "**_Long Short Term Memory Cells_**, or **_LSTMs_**, are another sort of specialized neurons for use in RNNs that are able to effectively learn what to remember and what to forget in sequence models. \n",
    "\n",
    "LSTMs are generally like GRUs, except that they use 3 gates instead of 2. LSTMs have: \n",
    "\n",
    "* an **_Input Gate_**, which determines how much of the cell state that was passed along should be kept\n",
    "* an **_Forget Gate_**, which determines how much of the current state should be forgotten\n",
    "* an **_Output Gate_**, which determines how much of the current state should be exposed to the next layers in the network\n",
    "\n",
    "**BUILDING MODELS USING BOTH (GRU, LSTM) METHODS ARE BEST PRACTICE AS ONE DOES NOT NECESSARILY OUTPERFORM THE OTHER**\n",
    "\n",
    "    #sample LSTM model\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Embedding(20000, 128))\n",
    "    lstm_model.add(LSTM(50, return_sequences=True))\n",
    "    lstm_model.add(GlobalMaxPool1D())\n",
    "    lstm_model.add(Dropout(0.5))\n",
    "    lstm_model.add(Dense(50, activation='relu'))\n",
    "    lstm_model.add(Dropout(0.5))\n",
    "    lstm_model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "### Bidirectional Layers\n",
    "\n",
    "A Bidirectional RNN is just like a regular RNN, but with a twist--half of the neurons start by at the beginnig of the data and work towards the end one step at a time, while the other half start at the end of the data and work towards the beginning at the same pace!\n",
    "\n",
    "Bidirectional RNNs excel at things like speech recognition and other NLP tasks. Typically, Bidirectional RNN Layers combined with LSTM cells are a great first place to start when tackling NLP tasks. However, _they do come with the drawback of increased complexity and computational requirements_, since each bidirectional layer is essentially double the size, since an equal amount of neurons are needed for each direction. This means that if we create a bidirectional layer of 50 LSTM neurons, then our model actually has 100 LSTM cells for that layer--50 for front-to-back, and 50 for back-to-front. This size increase can definitely slow down training times, because using things like LSTM cells are already quite time intensive. However, *_when it comes to performance with things like human speech, bidirectional models are often best-in-class_*!\n",
    "\n",
    "    #sample LSTM model with Bidirectional Layer added\n",
    "    from keras.layers import LSTM, Dense, Bidirectional\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(10, return_sequences=True),\n",
    "                            input_shape=(5, 10)))\n",
    "    model.add(Bidirectional(LSTM(10)))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Juniors",
   "language": "python",
   "name": "jrenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.328px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
