{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheat Sheet (pyspark)\n",
    "https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "\n",
    "#### The three Vs\n",
    "\n",
    "##### VOLUME¶\n",
    "Volume refers to the amount of data generated through websites, portals and online applications in a data-driven business. Especially for online retailers, volume encompasses the available data that are out there and need to be assessed for relevance.\n",
    "\n",
    "##### VELOCITY¶\n",
    "Velocity refers to the speed with which data is generated, and as internet speeds have increased and the number of users has increased, the velocity has also increased substantially.\n",
    "\n",
    "##### VARIETY¶\n",
    "Variety in Big Data refers to all the structured and unstructured data that has the possibility of getting generated either by humans or by machines. Structured data is whatever data you could store in a spreadsheet. It can easily be cataloged and summary statistics can be calculated for it. Unstructured data are raw things like texts, tweets, pictures, videos, emails, voice mails, hand-written text, ECG reading, and audio recordings. Humans can only make sense of data that is structured, and it is usually up to data scientists to create some organization and structure to unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel and Distributed Computing with Map-Reduce\n",
    "\n",
    "MapReduce is a programming paradigm that enables the ability to scale across hundreds or thousands of servers for big data analytics. The underlying concept can be somewhat difficult to grasp, because this paradigm differs from the traditional programming practices. This lesson aims to present a simple yet intuitive account of MapReduce that we shall put into practice in upcoming labs. \n",
    "\n",
    "*In a nutshell, the term \"MapReduce\" refers to two distinct tasks. The first is the __Map__ job, which takes one set of data and transforms it into another set of data, where individual elements are broken down into tuples __(key/value pairs)__, while the __Reduce__ job takes the output from a map as input and combines those data tuples into a smaller set of tuples.*\n",
    "\n",
    "#### Distributed Processing Systems\n",
    ">A distributed processing system is a group of computers in a network working in tandem to accomplish a task\n",
    "\n",
    "#### Parallel Processing\n",
    "With parallel computing:\n",
    "\n",
    "* a larger problem is broken up into smaller pieces\n",
    "* every part of the problem follows a series of instructions\n",
    "* each one of the instructions is executed simultaneously on different processors\n",
    "* all of the answers are collected from the small problems and combined into one final answer\n",
    "\n",
    "#### MapReduce process\n",
    "\n",
    "##### 1. MAP Task ((Splitting & Mapping)\n",
    "The dataset that needs processing must first be transformed into <key:value> pairs and split into fragments, which are then assigned to map tasks. Each computing cluster is assigned a number of map tasks, which are subsequently distributed among its nodes. In this example, let's assume that we are using 5 nodes (a server with 5 different worker.\n",
    "\n",
    "First, split the data from one file or files into however many nodes are being used.\n",
    "\n",
    "We will then use the map function to create key value pairs represented by:   \n",
    "*{animal}* , *{# of animals per zoo}* \n",
    "\n",
    "After processing of the original key:value pairs, some __intermediate__ key:value pairs are generated. The intermediate key:value pairs are __sorted by their key values__ to create a new list of key:value pairs.\n",
    "\n",
    "##### 2. Shuffling\n",
    "This list from the map task is divided into a new set of fragments that sorts and shuffles the mapped objects into an order or grouping that will make it easier to reduce them. __The number these new fragments, will be the same as the number of the reduce tasks__. \n",
    "\n",
    "##### 3. REDUCE Task (Reducing)\n",
    "Now, every properly shuffled segment will have a reduce task applied to it. After the task is completed, the final output is written onto a file system. The underlying file system is usually HDFS (Hadoop Distributed File System). \n",
    "\n",
    "It's important to note that MapReduce will generally only be powerful when dealing with large amounts of data. When using on a small dataset, it will be faster to perform operations not in the MapReduce framework.\n",
    "\n",
    "There are two groups of entities in this process to ensuring that the map reduce task gets done properly:\n",
    "\n",
    "__Job Tracker__: a \"master\" node that informs the other nodes which map and reduce jobs to complete\n",
    "\n",
    "__Task Tracker__: the \"worker\" nodes that complete the map and reduce operations\n",
    "\n",
    "There are different names for these components depending on the technology used, but there will always be a master node that informs worker nodes what tasks to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Context\n",
    "\n",
    "#### SparkConf\n",
    "- configures a Spark Context including Java properties\n",
    "\n",
    "        pyspark.SparkConf(loadDefaults=True, _jvm=None, _jconf=None)\n",
    "\n",
    "#### Create a local spark context with pyspark\n",
    "    import pyspark\n",
    "    sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "#### Display the type of the Spark Context\n",
    "    type(sc)\n",
    "\n",
    "#### Use Python's dir(obj) to get a list of all attributes of SparkContext\n",
    "    dir(sc)\n",
    "\n",
    "#### Use Python's help ( help(object) ) function to get information on attributes and methods for sc object. \n",
    "    help(sc)\n",
    "\n",
    "#### Check the number of cores being used\n",
    "    print (\"Default number of cores being used:\", sc.defaultParallelism) \n",
    "\n",
    "#### Check for the current version of Spark\n",
    "    print (\"Current version of Spark:\", sc.version)\n",
    "    \n",
    "#### Check the name of application currently running in spark environment\n",
    "    sc.appName\n",
    "    \n",
    "#### Access complete configuration settings (including all defaults) for the current spark context \n",
    "    sc._conf.getAll()\n",
    "    \n",
    "#### Shut down SparkContext\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame\n",
    "- The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDD\n",
    "- To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection.\n",
    "\n",
    "        # Import SparkSession from pyspark.sql\n",
    "        from pyspark.sql import SparkSession\n",
    "\n",
    "        # Create my_spark\n",
    "        my_spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "        # Print my_spark\n",
    "        print(my_spark)\n",
    "#### catalog\n",
    "- lists all data inside your cluster\n",
    "\n",
    "        print(spark.catalog.listTables())\n",
    "        \n",
    "#### queries\n",
    "- SQL query format\n",
    "\n",
    "        # create a SQL query\n",
    "        query = \"FROM flights SELECT * LIMIT 10\"\n",
    "        # Get the first 10 rows of flights\n",
    "        flights10 = spark.sql(query)\n",
    "        # Show the results\n",
    "        flights10.show()\n",
    "#### Spark Cluster to Pandas DataFrame\n",
    "- convert with the .toPandas() method\n",
    "\n",
    "        # create a query\n",
    "        query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
    "        # Run the query\n",
    "        flight_counts = spark.sql(query)\n",
    "        # Convert the results to a pandas DataFrame\n",
    "        pd_counts = flight_counts.toPandas()\n",
    "        # Print the head of pd_counts\n",
    "        print(pd_counts.head())\n",
    "#### DataFrame to Spark Cluster\n",
    "- the .createDataFrame() method will convert a DF to a Spark Cluster\n",
    "        \n",
    "        spark_temp = spark.createDataFrame(pd_temp)\n",
    "#### Temporary View\n",
    "- will allow spark to create a temporary cluster to use spark commands on a DataFrame\n",
    "\n",
    "        spark_temp.createOrReplaceTempView('new_table_name')\n",
    "#### Direct read-in of datasources\n",
    "- allows direct conversion to a spark dataframe for several file types\n",
    "\n",
    "        #  file path to .csv\n",
    "        file_path = \"/usr/local/share/datasets/airports.csv\"\n",
    "\n",
    "        # Read in the airports data\n",
    "        airports = spark.read.csv(file_path, header=True)\n",
    "\n",
    "        # Show the data\n",
    "        airports.show()\n",
    "#### creating columns\n",
    "- In Spark you can do this using the .withColumn() method, which takes two arguments. First, a string with the name of your new column, and second the new column input\n",
    "- Spark dataframes are immutable, so adding new columns means reassigning the df.\n",
    "\n",
    "        df = df.withColumn(\"newCol\", df.column.mean()) #using the mean of another column to create values for new column \n",
    "#### filtering results\n",
    "- similar to the WHERE clause in SQL\n",
    "- two ways of filtering:\n",
    "    - passing a string value will filter out the values listed\n",
    "    - passing in the direct df.column with the filter will create a new column will boolean values.\n",
    "    \n",
    "            # Filter flights by passing a string\n",
    "            long_flights1 = flights.filter(\"distance > 1000\")\n",
    "\n",
    "            # Filter flights by passing a column of boolean values\n",
    "            long_flights2 = flights.filter(flights.distance > 1000)\n",
    "#### select columns\n",
    "- similar to SQL SELECT statement\n",
    "- takes multiple arguments - one for each column you want to select. These arguments can either be the column name as a string (one for each column) or a column object (using the df.colName syntax). When you pass a column object, you can perform operations like addition or subtraction on the column to change the data contained in it\n",
    "- can also use column-wise operations \n",
    "\n",
    "        # Select columns using column strings\n",
    "        selected1 = flights.select('tailnum', 'origin', 'dest')\n",
    "\n",
    "        # Select columns using df.column format\n",
    "        temp = flights.select(flights.origin, flights.dest, flights.carrier)\n",
    "        #using operations to create a column\n",
    "        avg_speed = (flights.distance/(flights.air_time/60))\n",
    "        \n",
    "#### alias\n",
    "- renames a column when selecting\n",
    "\n",
    "        #rename column to avg_speed\n",
    "        avg_speed = (flights.distance/(flights.air_time/60)).alias(\"avg_speed\")\n",
    "#### aggregating\n",
    "- you can aggregate with common methods by using the .groupBy() method\n",
    "- creating groups using the .groupby() allows them to be part of a pyspark.sql.GroupedData class\n",
    "        \n",
    "        #using filter and groupby to fingd the shortest flight from PDX\n",
    "        flights.filter(flights.origin == 'PDX').groupBy().min(\"distance\").show()\n",
    "- you can also use the .agg() function which allows use of any of the pyspark.sql.functions library\n",
    "\n",
    "        #import functions\n",
    "        import pyspark.sql.functions as F\n",
    "        #find standard deviation of a columns\n",
    "        df.column.agg(F.stddev()).show()\n",
    "#### joining\n",
    "- performed using the df.join() method\n",
    "    \n",
    "        joined_df = df.join(joining_df, on='joining_column', how='how_to_join')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "Resilient Distributed Datasets (RDD) are fundamental data structures of Spark. An RDD is, essentially, the Spark representation of a set of data, spread across multiple machines, with APIs to let you act on it. An RDD could come from any datasource, e.g. text files, a database, a JSON file etc.\n",
    "\n",
    "#### create RDD\n",
    "    rdd = sc.parallelize(data,numSlices=10) #creates 10 partitions\n",
    "    print(type(rdd))\n",
    "    \n",
    "#### Get # of partitions\n",
    "    rdd.getNumPartitions()\n",
    "    \n",
    "#### Basic actions\n",
    "    rdd.count() #returns the total count of items in the RDD\n",
    "    rdd.first() #returns the first item in the RDD\n",
    "    rdd.take() #returns the first n items in the RDD\n",
    "    rdd.top() #returns the top n items\n",
    "    rdd.collect() #returns everything from your RDD\n",
    "    \n",
    "#### Mapping function to data (creates tuples of paired data)\n",
    "    def sales_tax(num):\n",
    "        return num * 0.92\n",
    "\n",
    "    revenue_minus_tax = price_items.map(sales_tax)\n",
    "    \n",
    "#### Applying lambda function to data\n",
    "    discounted = revenue_minus_tax.map(lambda x : x*0.9)\n",
    "    \n",
    "#### chain methods in spark\n",
    "    price_items.map(sales_tax).map(lambda x : x*0.9).top(15)\n",
    "    \n",
    "#### See the full lineage of all the operations that have been performed on an RDD\n",
    "    discounted.toDebugString()\n",
    "    \n",
    "#### Flatmap (creates a list of data - all same level)\n",
    "    flat_mapped = price_items.flatMap(lambda x : (x, x*0.92*0.9 ))\n",
    "\n",
    "#### A filter method is a specialized form of a map function that only returns the items that match a certain criteria\n",
    "    selected_items = discounted.filter(lambda x: x>300)\n",
    "    \n",
    "####  Use a reduce method with a lambda function to to add up all of the values in the RDD\n",
    "    selected_items.reduce(lambda x,y :x + y)\n",
    "    \n",
    "#### reduceByKey to perform reducing operations while grouping by keys.\n",
    "    total_spent = sales_data.reduceByKey(lambda x,y :x + y)\n",
    "    \n",
    "####  sortBy method on the RDD to rank the users from highest spending to least spending.\n",
    "    total_spent.sortBy(lambda x: x[1],ascending = False).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning in Spark\n",
    "\n",
    "### ml and mllib machine learning libraries\n",
    "\n",
    "[mllib](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html)\n",
    "- mllib library is built upon the RDDs \n",
    "[ml](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html)\n",
    "- ml library is built on Spark DataFrames\n",
    "\n",
    "\n",
    "    \n",
    "#### ml library\n",
    "\n",
    "- used sklearn as an inspiration for their implementation of a machine learning library. As a result, many of the methods and functionalities look similar, but there are some crucial distinctions. There two main classes\n",
    "\n",
    "`Transformer`\n",
    "- An algorithm that transforms one pyspark DataFrame into another DataFrame.\n",
    "- uses the .transform() method\n",
    "- returns a new dataframe, usually with a new column appended\n",
    "- examples of classes: Bucketizer, PCA\n",
    "\n",
    "`Estimator`\n",
    "- An algorithm that can be fit onto a pyspark DataFrame that can then be used as a Transformer.\n",
    "- uses the .fit() method\n",
    "- returns a model object\n",
    "- examples of calsses: StringIndexerColum, RandomForestModel\n",
    "\n",
    "#### ml library machine learning steps\n",
    "\n",
    "##### ensure proper data types\n",
    "- only takes numbers (decimals, whole numbers) called 'doubles','integers' in Spark\n",
    "- Spark will make a guess on datatypes, if not recognized as a double or integer, you can use the .cast() method in combination of .withColumn() method to convert the datatype (\"integer\", \"double\")\n",
    "\n",
    "        #create new column with numerical data\n",
    "        original_df = original_df.withColumn(\"col_name\", df.col_name.cast('datatype')\n",
    "##### handle categorical variables\n",
    "- there are two steps to take to one-hot-encode categorical variables:\n",
    "- step 1: use StringIndexer to convert distinct variables to individual numbered columns\n",
    "\n",
    "        #create stringindexer\n",
    "        indexer = StringIndexer(inputCol='col_name', outputCol='new_cols_name')\n",
    "- step 2: use OneHotEncoder to OHE the columns\n",
    "        \n",
    "        #create ohe\n",
    "        encoder = OneHotEncoder(inputCol='col_name', outputCol='new_cols_name'\n",
    "##### assemble a vector\n",
    "- Spark modeling requires all columns containing features to be combined in one column\n",
    "- done using VectorAssembler\n",
    "\n",
    "        #assemble matrix into a vector\n",
    "        assembler = VectorAssembler(inputCols=['list', 'of', 'col', 'names'], outputCol='feature')\n",
    "##### create a pipeline\n",
    "- combines all estimators and transformers\n",
    "- wraps the process so you can reuse the named pipeline\n",
    "\n",
    "        #import pipeline\n",
    "        from pyspark.ml import Pipeline\n",
    "        #make pipeline\n",
    "        pipeline = Pipeline(stages=[indexer, encoder, assembler])\n",
    "##### fit and transform data\n",
    "\n",
    "        #fit and transform\n",
    "        piped_data = pipeline.fit(model_data).transform(model_data)\n",
    "##### split data into train and test sets\n",
    "\n",
    "        #split data\n",
    "        training, test = piped_data.randomSplit([.7,.3])\n",
    "##### run selected model\n",
    "- there are many models to choose from, ensure proper package is imported and instantiated\n",
    "##### select evaluation method\n",
    "- select evaluation via pyspark.ml.evaluation\n",
    "        \n",
    "        evaluator = selectedEvaluationMethod()\n",
    "##### hyperparameter tuning\n",
    "- performed using the pyspark.ml.tuning library\n",
    "- if several parameters will be tuned, use ParamGridBuilder() method\n",
    "\n",
    "        #import tuning package\n",
    "        import pyspark.ml.tuning as tune\n",
    "        #create grid\n",
    "        grid = ParamGridBuilder()\n",
    "        #replace the grid with all additions\n",
    "        grid = grid.addGrid(model.parameter, [0,1,2])\n",
    "        grid = grid.addGrid(model.parameter, [0,1,2])\n",
    "        #build the grid\n",
    "        grid = grid.build()\n",
    "##### create the validator        \n",
    "- the tuning module also contains the CrossValidator class which will implement your selected evaluator\n",
    "\n",
    "        #create crossvalidator\n",
    "        cv = tune.CrossValidator(estimator=model_name,\n",
    "                                 estimatorParamMaps=grid,\n",
    "                                 evaluator=evaluator)\n",
    "##### fit training data\n",
    "    best_model = model_name.fit(training)\n",
    "##### predict test data\n",
    "    results = best_model.transform(test)\n",
    "##### evaluate model\n",
    "    print(evaluator.evaluate(test_results))\n",
    "\n",
    "#### Sample ml library Pipeline\n",
    "\n",
    "    from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "    #set pipeline parameters\n",
    "    string_indexer = StringIndexer(inputCol='month',outputCol='month_num',handleInvalid='keep')\n",
    "    one_hot_encoder = OneHotEncoderEstimator(inputCols=['month_num'],outputCols=['month_vec'])\n",
    "    vector_assember = VectorAssembler(inputCols=features,outputCol='features')\n",
    "    random_forest = RandomForestRegressor(featuresCol='features',labelCol='area')\n",
    "    stages =  [string_indexer, one_hot_encoder, vector_assember,random_forest]\n",
    "\n",
    "    pipeline = Pipeline(stages=stages) #instantiate pipeline\n",
    "\n",
    "    params = ParamGridBuilder()\\\n",
    "    .addGrid(random_forest.maxDepth, [5,10,15])\\\n",
    "    .addGrid(random_forest.numTrees, [20,50,100])\\\n",
    "    .build() #performs gridsearch on set parameters\n",
    "\n",
    "    reg_evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='area',metricName = 'mae') #evaluates model\n",
    "\n",
    "    cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params,evaluator=reg_evaluator)\n",
    "\n",
    "    cross_validated_model = cv.fit(spark_df) #fits model\n",
    "\n",
    "    cross_validated_model.avgMetrics #returns best metrics based on metricName\n",
    "\n",
    "    #shows selected predictions\n",
    "    predictions = cross_validated_model.transform(spark_df)\n",
    "    predictions.select('prediction','area').show(300)\n",
    "\n",
    "    cross_validated_model.bestModel.stages #checking best model by stage\n",
    "\n",
    "    optimal_rf_model = cross_validated_model.bestModel.stages[3] #looking at stage 3 of process\n",
    "    optimal_rf_model.fe\n",
    "\n",
    "    optimal_rf_model.featureImportances #checking feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Spark Word Count Function\n",
    "\n",
    "    stopWordList = ['', 'the','a','in','of','on','at','for','by','i','you','me'] \n",
    "    def wordCount(filename, stopWordlist):\n",
    "        output = sc.textFile(filename)\n",
    "        words1 = lines.flatMap(lambda x: x.split(' '))\n",
    "        words2 = words1.map(lambda x: (x.lower(), 1))\n",
    "        wordCount = words2.reduceByKey(lambda x,y: x+y)\n",
    "        freqWords = wordCount.filter(lambda x:  x[1] >= 5 )\n",
    "        stopWords = freqWords.filter(lambda x:  x[0] in stopWordList) \n",
    "        output = stopWords.collect()\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
