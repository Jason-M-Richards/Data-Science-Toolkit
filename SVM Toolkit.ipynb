{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators¶\n",
    "svm.LinearSVC([penalty, loss, dual, tol, C, …])\tLinear Support Vector Classification.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC\n",
    "\n",
    "svm.LinearSVR([epsilon, tol, C, loss, …])\tLinear Support Vector Regression.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR\n",
    "\n",
    "svm.NuSVC([nu, kernel, degree, gamma, …])\tNu-Support Vector Classification.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC\n",
    "\n",
    "svm.NuSVR([nu, C, kernel, degree, gamma, …])\tNu Support Vector Regression.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR\n",
    "\n",
    "svm.OneClassSVM([kernel, degree, gamma, …])\tUnsupervised Outlier Detection.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM\n",
    "\n",
    "svm.SVC([C, kernel, degree, gamma, coef0, …])\tC-Support Vector Classification.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "\n",
    "svm.SVR([kernel, degree, gamma, coef0, tol, …])\tEpsilon-Support Vector Regression.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR\n",
    "\n",
    "svm.l1_min_c(X, y[, loss, fit_intercept, …])\tReturn the lowest bound for C such that for C in (l1_min_C, infinity) the model is guaranteed not to be empty.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building SVM Using Scikit-learn\n",
    "\n",
    "#### Generate two data sets in scikit-learn¶\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(221)\n",
    "\n",
    "plt.title(\"Two blobs\")\n",
    "X_1, y_1 = make_blobs(n_features = 2, centers = 2, cluster_std=1.25, random_state = 123)\n",
    "\n",
    "plt.scatter(X_1[:, 0], X_1[:, 1], c = y_1, s=25)\n",
    "\n",
    "plt.subplot(222)\n",
    "\n",
    "plt.title(\"Two blobs with more noise\")\n",
    "\n",
    "X_2, y_2 = make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=3,  random_state = 123)\n",
    "\n",
    "plt.scatter(X_2[:, 0], X_2[:, 1], c = y_2, s=25)\n",
    "\n",
    "plt.subplot(223)\n",
    "\n",
    "\n",
    "#### SVM to first dataset\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "clf.fit(X_1, y_1)\n",
    "\n",
    "Save the first feature (on the horizontal axis) as X_11 and the second feature (on the ver\n",
    "tical axis) as X_12.\n",
    "\n",
    "X_11= X_1[:,0]\n",
    "\n",
    "X_12= X_1[:,1]\n",
    "\n",
    "clf.coef_\n",
    "\n",
    "\n",
    "#### Store the min and maximum values X_11 and X_12 operate in.\n",
    "\n",
    "X11_min, X11_max = X_11.min() - 1, X_11.max() + 1\n",
    "\n",
    "X12_min, X12_max = X_12.min() - 1, X_12.max() + 1\n",
    "\n",
    "#### Create a grid using the numpy function linspace, which creates a numpy array with evenly spaced numbers over a specified interval. Specify num = 10 \n",
    "\n",
    "x11_coord = np.linspace(X11_min, X11_max, 10)\n",
    "\n",
    "x12_coord = np.linspace(X12_min, X12_max, 10)\n",
    "\n",
    "#### Create decision boundary using np.meshgrid with the two arguments equal to the np.linspace objects created for X11 and X12.\n",
    "\n",
    "X12_C, X11_C = np.meshgrid(x12_coord, x11_coord)\n",
    "\n",
    "#### Create a numpy array (100,2) that concatenates the coordinates for X11 and X12 together in one numpy object. Use np.c_ and make sure to use .ravel() first. Use np.shape() on your resulting object first to verify the resulting shape.\n",
    "\n",
    "x11x12 = np.c_[X11_C.ravel(), X12_C.ravel()]\n",
    "\n",
    "np.shape(x11x12)\n",
    "\n",
    "#### Get a decision boundary for this particular data set. Using your (100,2) numpy array and calling clf.decision_function() on it, the decision function returns the distance to the samples that you generated using meshgrid. Make sure you change your shape in a way that you get a (10,10) numpy array.\n",
    "\n",
    "df1 = clf.decision_function(x11x12)\n",
    "\n",
    "df1 = df1.reshape(X12_C.shape)\n",
    "\n",
    "#### Plot our data again with the result of svm in it.\n",
    "\n",
    "plt.scatter(X_11, X_12, c = y_1)\n",
    "###### what comes next uses same axes as scatterplot\n",
    "axes = plt.gca()\n",
    "\n",
    "##### three lines for hyperplane and boundaries\n",
    "axes.contour(X11_C, X12_C, df1, \n",
    "colors=[\"blue\",\"black\",\"blue\"], levels= [-1, 0, 1], linestyles=[':', '-', ':'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "###### support vector coordinates\n",
    "clf.support_vectors_\n",
    "\n",
    "#### Highlight your support vectors in new plot\n",
    "\n",
    "plt.scatter(X_11, X_12, c = y_1)\n",
    "\n",
    "axes = plt.gca()\n",
    "\n",
    "axes.contour(X11_C, X12_C, df1, colors= \"black\", levels= [-1, 0, 1], linestyles=[':', '-', ':'])\n",
    "\n",
    "axes.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], facecolors='blue') \n",
    "\n",
    "plt.show()\n",
    "\n",
    "#### SVM when data not linearly separable (dataset 2) using SVC function\n",
    "\n",
    "plt.scatter(X_2[:, 0], X_2[:, 1], c=y_2, s=25)\n",
    "\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "clf.fit(X_2, y_2)\n",
    "\n",
    "X_21= X_2[:,0]\n",
    "\n",
    "X_22= X_2[:,1]\n",
    "\n",
    "X21_min, X21_max = X_21.min() - 1, X_21.max() + 1\n",
    "\n",
    "X22_min, X22_max = X_22.min() - 1, X_22.max() + 1\n",
    "\n",
    "x21_coord = np.linspace(X21_min, X21_max, 10)\n",
    "\n",
    "x22_coord = np.linspace(X22_min, X22_max, 10)\n",
    "\n",
    "X22_C, X21_C = np.meshgrid(x22_coord, x21_coord)\n",
    "\n",
    "x21x22 = np.c_[X21_C.ravel(), X22_C.ravel()]\n",
    "\n",
    "df2 = clf.decision_function(x21x22)\n",
    "\n",
    "df2= df2.reshape(X21_C.shape)\n",
    "\n",
    "plt.scatter(X_21, X_22, c = y_2)\n",
    "\n",
    "axes = plt.gca()\n",
    "\n",
    "axes.contour(X21_C, X22_C, df2, colors=[\"blue\",\"black\",\"blue\"], levels= [-1, 0, 1], linestyles=[':', '-', ':'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#### Changing hyperparameter to adjust for misclassification\n",
    "\n",
    "###### C-value hyperparameter increased to decrease boundary width\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C = 5000000) \n",
    "\n",
    "clf.fit(X_2, y_2)\n",
    "\n",
    "###### code same as above to compile and plot new visual with adjusted hyperparameter\n",
    "\n",
    "Other options in Scikit Learn\n",
    "When you dig deeper in Scikit Learn, you'll notice that there are several ways to get to linear SVM's for classification:\n",
    "\n",
    "#### What does One-vs-one mean? what does One-vs-all mean?\n",
    "\n",
    "One-vs-one means that with $n$ classes, $\\dfrac{(n)*(n-1)}{2}$ boundaries are constructed!\n",
    "\n",
    "One-vs-all means that when there are $n$ classes, $n$ boundaries are created.\n",
    "\n",
    "#### Classifying four classes\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "\n",
    "plt.title(\"Four blobs\")\n",
    "\n",
    "X, y = make_blobs(n_samples=100, n_features=2, centers=4, cluster_std=1.6,  random_state = 123)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c = y, s=25);\n",
    "\n",
    "#### Try four different models and plot the results using subplots where:\n",
    "\n",
    "- The first one is a regular SVC (C=1)\n",
    "- The second one is a regular SVC with C=0.1\n",
    "- The third one is a NuSVC with nu= 0.7\n",
    "- The fourth one is a LinearSVC (no arguments)\n",
    "\n",
    "X1= X[:,0]\n",
    "\n",
    "X2= X[:,1]\n",
    "\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 200)\n",
    "\n",
    "x2_coord = np.linspace(X2_min, X2_max, 200)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "\n",
    "clf1 = svm.SVC(kernel = \"linear\",C=1) \n",
    "\n",
    "clf1.fit(X, y)\n",
    "\n",
    "Z1 = clf1.predict(x1x2).reshape(X1_C.shape)\n",
    "\n",
    "clf2 = svm.SVC(kernel = \"linear\",C=0.1) \n",
    "\n",
    "clf2.fit(X, y)\n",
    "\n",
    "Z2 = clf2.predict(x1x2).reshape(X1_C.shape)\n",
    "\n",
    "clf3 = svm.NuSVC(kernel = \"linear\",nu=0.7) \n",
    "\n",
    "clf3.fit(X, y)\n",
    "\n",
    "Z3 = clf3.predict(x1x2).reshape(X1_C.shape)\n",
    "\n",
    "clf4 = svm.LinearSVC() \n",
    "\n",
    "clf4.fit(X, y)\n",
    "\n",
    "Z4 = clf4.predict(x1x2).reshape(X1_C.shape)\n",
    "\n",
    "#### Plotting Figures \n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.subplot(221)\n",
    "\n",
    "plt.title(\"SVC, C=1\")\n",
    "\n",
    "axes = plt.gca()\n",
    "\n",
    "axes.contourf(X1_C, X2_C, Z1, alpha = 1)\n",
    "\n",
    "plt.scatter(X1, X2, c = y, edgecolors = 'k')\n",
    "\n",
    "axes.scatter(clf1.support_vectors_[:, 0], clf1.support_vectors_[:, 1], facecolors='blue', edgecolors= 'k') \n",
    "\n",
    "plt.subplot(222)\n",
    "\n",
    "plt.title(\"SVC, C=0.1\")\n",
    "\n",
    "axes = plt.gca()\n",
    "\n",
    "axes.contourf(X1_C, X2_C, Z2, alpha = 1)\n",
    "\n",
    "plt.scatter(X1, X2, c = y, edgecolors = 'k')\n",
    "\n",
    "axes.scatter(clf2.support_vectors_[:, 0], clf2.support_vectors_[:, 1], facecolors='blue', edgecolors= 'k') \n",
    "\n",
    "plt.subplot(223)\n",
    "\n",
    "plt.title(\"NuSVC, nu=0.5\")\n",
    "\n",
    "axes = plt.gca()\n",
    "\n",
    "axes.contourf(X1_C, X2_C, Z3, alpha = 1)\n",
    "\n",
    "plt.scatter(X1, X2, c = y, edgecolors = 'k')\n",
    "axes.scatter(clf3.support_vectors_[:, 0], clf3.support_vectors_[:, 1], facecolors='blue', edgecolors= 'k') \n",
    "\n",
    "plt.subplot(224)\n",
    "\n",
    "plt.title(\"LinearSVC\")\n",
    "\n",
    "axes = plt.gca()\n",
    "\n",
    "axes.contourf(X1_C, X2_C, Z4, alpha = 1)\n",
    "\n",
    "plt.scatter(X1, X2, c = y, edgecolors = 'k')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#### Coefficients of the decision boundaries\n",
    "\n",
    "print(clf2.coef_)\n",
    "\n",
    "print(clf4.coef_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Margin Classifier (Raw) (for clearly defined boundaries)\n",
    "\n",
    "###### label classes from dataset\n",
    "\n",
    "class_1 = X[labels==0]\n",
    "\n",
    "class_2 = X[labels==1]\n",
    "\n",
    "import cvxpy as cp\n",
    "\n",
    "d = 2  \n",
    "\n",
    "m = 50 \n",
    "\n",
    "n = 50  \n",
    "\n",
    "###### Define the variables\n",
    "w = cp.Variable(d)\n",
    "\n",
    "b = cp.Variable()\n",
    "\n",
    "###### Define the constraints\n",
    "x_constraints = [w.T * class_1[i] + b >= 1  for i in range(m)]\n",
    "y_constraints = [w.T * class_2[i] + b <= -1 for i in range(n)]\n",
    "\n",
    "###### Sum the constraints\n",
    "constraints = x_constraints +  y_constraints \n",
    "\n",
    "###### Define the objective. Hint: use cp.norm\n",
    "obj = cp.Minimize(cp.norm(w,2))\n",
    "\n",
    "###### Add objective and constraint in the problem\n",
    "prob = cp.Problem(obj, constraints)\n",
    "\n",
    "###### Solve the problem\n",
    "prob.solve()\n",
    "print(\"Problem Status: %s\"%prob.status)\n",
    "\n",
    "###### if problem status says \"infeasible\", use soft margin classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Boundaries\n",
    "\n",
    "def plotBoundaries(x, y, w, b):\n",
    "\n",
    "##### Takes in a set of datapoints x and y for two clusters,\n",
    "    d1_min = np.min([x[:,0],y[:,0]])\n",
    "    d1_max = np.max([x[:,0],y[:,0]])\n",
    "###### Line form: (-a[0] * x - b ) / a[1]\n",
    "    d2_at_mind1 = (-w[0]*d1_min - b ) / w[1]\n",
    "    d2_at_maxd1 = (-w[0]*d1_max - b ) / w[1]\n",
    "    sup_up_at_mind1 = (-w[0]*d1_min - b + 1 ) / w[1]\n",
    "    sup_up_at_maxd1 = (-w[0]*d1_max - b + 1 ) / w[1]\n",
    "    sup_dn_at_mind1 = (-w[0]*d1_min - b - 1 ) / w[1]\n",
    "    sup_dn_at_maxd1 = (-w[0]*d1_max - b - 1 ) / w[1]\n",
    "\n",
    "###### Plot the clusters!\n",
    "    plt.scatter(x[:,0],x[:,1],color='purple')\n",
    "    plt.scatter(y[:,0],y[:,1],color='yellow')\n",
    "    plt.plot([d1_min,d1_max],[d2_at_mind1 ,d2_at_maxd1],color='black')\n",
    "    plt.plot([d1_min,d1_max],[sup_up_at_mind1,sup_up_at_maxd1],'-.',color='blue')\n",
    "    plt.plot([d1_min,d1_max],[sup_dn_at_mind1,sup_dn_at_maxd1],'-.',color='blue')\n",
    "    plt.ylim([np.floor(np.min([x[:,1],y[:,1]])),np.ceil(np.max([x[:,1],y[:,1]]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Margin Classifier (Raw)\n",
    "\n",
    "import cvxpy as cp\n",
    "\n",
    "d = 2\n",
    "\n",
    "m = 50 \n",
    "\n",
    "n = 50  \n",
    "\n",
    "###### Define the variables\n",
    "w = cp.Variable(d)\n",
    "\n",
    "b = cp.Variable()\n",
    "\n",
    "ksi_1 = cp.Variable(m)\n",
    "\n",
    "ksi_2 = cp.Variable(n)\n",
    "\n",
    "C=0.01\n",
    "\n",
    "###### Define the constraints\n",
    "x_constraints = [w.T * class_1[i] + b >= 1 - ksi_1[i]  for i in range(m)]\n",
    "\n",
    "y_constraints = [w.T * class_2[i] + b <= -1 + ksi_2[i] for i in range(n)]\n",
    "\n",
    "ksi_1_constraints = [ksi_1 >= 0  for i in range(m)]\n",
    "\n",
    "ksi_2_constraints = [ksi_2 >= 0  for i in range(n)]\n",
    "\n",
    "###### Sum the constraints\n",
    "constraints = x_constraints +  y_constraints + ksi_1_constraints + ksi_2_constraints\n",
    "\n",
    "###### Define the objective. Hint: use cp.norm. Add in a C hyperparameter and assume 1 at first\n",
    "obj = cp.Minimize(cp.norm(w,2)+ C * (sum(ksi_1)+ sum(ksi_2)))\n",
    "\n",
    "###### Add objective and constraint in the problem\n",
    "prob = cp.Problem(obj, constraints)\n",
    "\n",
    "###### Solve the problem\n",
    "prob.solve()\n",
    "\n",
    "print(\"Problem Status: %s\"%prob.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Kernals\n",
    "\n",
    "#### Create two non-linear datasets\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "\n",
    "plt.title(\"Four blobs\")\n",
    "\n",
    "X_3, y_3 = make_blobs(n_samples=100, n_features=2, centers=4, cluster_std=1.6, random_state = 123)\n",
    "plt.scatter(X_3[:, 0], X_3[:, 1], c = y_3, s=25)\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.title(\"Two interleaving half circles\")\n",
    "\n",
    "X_4, y_4 = make_moons(n_samples=100, shuffle = False , noise = 0.3, random_state=123)\n",
    "plt.scatter(X_4[:, 0], X_4[:, 1], c = y_4, s=25)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#### RBF Kernal (Radial Basis Function)\n",
    "\n",
    "##### Radial basis function kernel has 2 hyperparameters: C and gamma.\n",
    "\n",
    "###### Create a loop that builds a model for each of the combinations\n",
    "C_range =  np.array([0.1, 1, 10])  # [0.01, 10]\n",
    "\n",
    "gamma_range =  np.array([0.1, 1, 100]) # [1, 100] \n",
    "\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "\n",
    "details = []\n",
    "\n",
    "for C in C_range:\n",
    "    for gamma in gamma_range:\n",
    "        clf = svm.SVC(C=C, gamma=gamma)\n",
    "        clf.fit(X_4, y_4)\n",
    "        details.append((C, gamma, clf))\n",
    "\n",
    "#### Prepare your data for plotting\n",
    "X1= X_4[:,0]\n",
    "\n",
    "X2= X_4[:,1]\n",
    "\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 500)\n",
    "\n",
    "x2_coord = np.linspace(X2_min, X2_max, 500)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "\n",
    "#### Plot the prediction results in 9 subplots  \n",
    "plt.figure(figsize=(11, 11))\n",
    "\n",
    "for (k, (C, gamma, clf)) in enumerate(details):\n",
    "###### evaluate the predictions in a grid\n",
    "    Z = clf.predict(x1x2)  \n",
    "    Z = Z.reshape(X1_C.shape)\n",
    "\n",
    "###### visualize decision function for these parameters\n",
    "    plt.subplot(3, 3, k + 1)\n",
    "    plt.title(\"gam= %r, C= %r, score = %r\"  % (gamma, C, round(clf.score(X_4,y_4), 2)))\n",
    "\n",
    "###### visualize parameter's effect on decision function\n",
    "    plt.contourf(X1_C, X2_C, Z, alpha = 1)\n",
    "    plt.scatter(X_4[:, 0], X_4[:, 1], c=y_4,  edgecolors='gray')\n",
    "    plt.axis('tight')\n",
    "\n",
    "#### Repeat but use decision_function instead of predict\n",
    "\n",
    "#### Plot the prediction results in 9 subplots  \n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for (k, (C, gamma, clf)) in enumerate(details):\n",
    "###### evaluate the decision functions in a grid\n",
    "    Z = clf.decision_function(x1x2)  \n",
    "    Z = Z.reshape(X1_C.shape)\n",
    "\n",
    "###### visualize decision function for these parameters\n",
    "    plt.subplot(3, 3, k + 1)\n",
    "    plt.title(\"gam= %r, C= %r, score = %r\"  % (gamma, C, round(clf.score(X_4,y_4), 2)))\n",
    "\n",
    "###### visualize parameter's effect on decision function\n",
    "    plt.contourf(X1_C, X2_C, Z, alpha = 1)\n",
    "    plt.scatter(X_4[:, 0], X_4[:, 1], c=y_4,  edgecolors='gray')\n",
    "    plt.axis('tight')\n",
    "\n",
    "#### Polynomial kernel\n",
    "##### Polynomial kernel has 3 hyperparameters:\n",
    "\n",
    "$\\gamma$, which can be specified using keyword gamma\n",
    "\n",
    "$r$, which can be specified using keyword coef0\n",
    "\n",
    "$d$, which can be specified using keyword degree\n",
    "\n",
    "$r= 0.1$ and $2$\n",
    "$\\gamma= 0.1$ and $1$\n",
    "$d= 3$ and $4$\n",
    "\n",
    "\n",
    "##### Note that decision_function() cannot be used on a classifier with more than two classes.\n",
    "\n",
    "#### Create a loop that builds a model for each combination\n",
    "\n",
    "r_range =  np.array([0.1, 2])  # [0.01, 10]\n",
    "\n",
    "gamma_range =  np.array([0.1, 1]) # [1, 100] \n",
    "\n",
    "d_range = np.array([3, 4])\n",
    "\n",
    "param_grid = dict(gamma=gamma_range, degree = d_range, coef0 = r_range)\n",
    "\n",
    "details = []\n",
    "\n",
    "for d in d_range:\n",
    "    for gamma in gamma_range:\n",
    "         for r in r_range:\n",
    "            clf = svm.SVC(kernel = \"poly\", coef0 = r , gamma=gamma, degree= d)\n",
    "            clf.fit(X_3, y_3)\n",
    "            details.append((r, d, gamma, clf))\n",
    "\n",
    "#### Prepare your data for plotting\n",
    "X1= X_3[:,0]\n",
    "\n",
    "X2= X_3[:,1]\n",
    "\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 500)\n",
    "\n",
    "x2_coord = np.linspace(X2_min, X2_max, 500)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "\n",
    "#### Plot the prediction results \n",
    "\n",
    "plt.figure(figsize=(12, 14))\n",
    "\n",
    "for (k, (r, d,gamma, clf)) in enumerate(details):\n",
    "###### evaluate the predictions in a grid\n",
    "    Z = clf.predict(x1x2)  \n",
    "    Z = Z.reshape(X1_C.shape)\n",
    "\n",
    "###### visualize decision function for these parameters\n",
    "    plt.subplot(4, 2, k + 1)\n",
    "    plt.title(\"d= %r, gam= %r, r = %r , score = %r\"  % (d, gamma,r, round(clf.score(X_3,y_3), 2)))\n",
    "\n",
    "###### visualize parameter's effect on decision function\n",
    "    plt.contourf(X1_C, X2_C, Z, alpha = 1)\n",
    "    plt.scatter(X_3[:, 0], X_3[:, 1], c=y_3,  edgecolors='gray')\n",
    "    plt.axis('tight')\n",
    "\n",
    "#### The Sigmoid Kernel\n",
    "\n",
    "##### Sigmoid kernel has 2 hyperparameters:\n",
    "\n",
    "$\\gamma$, which can be specified using keyword gamma\n",
    "\n",
    "$r$, which can be specified using keyword coef0\n",
    "\n",
    "#### Create a loop that builds a model for each combination\n",
    "r_range =  np.array([0.01, 1, 10])  \n",
    "\n",
    "gamma_range =  np.array([0.001, 0.01, 0.1]) \n",
    "\n",
    "param_grid = dict(gamma=gamma_range,coef0 = r_range)\n",
    "\n",
    "details = []\n",
    "\n",
    "for gamma in gamma_range:\n",
    "     for r in r_range:\n",
    "        clf = svm.SVC(kernel = \"sigmoid\", coef0 = r , gamma=gamma)\n",
    "        clf.fit(X_3, y_3) \n",
    "        details.append((r, gamma, clf))\n",
    "\n",
    "#### Prepare your data for plotting\n",
    "X1= X_3[:,0]\n",
    "\n",
    "X2= X_3[:,1]\n",
    "\n",
    "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n",
    "\n",
    "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n",
    "\n",
    "x1_coord = np.linspace(X1_min, X1_max, 500)\n",
    "\n",
    "x2_coord = np.linspace(X2_min, X2_max, 500)\n",
    "\n",
    "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n",
    "\n",
    "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n",
    "\n",
    "# Plot the prediction results \n",
    "plt.figure(figsize=(12, 14))\n",
    "\n",
    "for (k, (r, gamma, clf)) in enumerate(details):\n",
    "###### evaluate the predictions in a grid\n",
    "    Z = clf.predict(x1x2)  \n",
    "    Z = Z.reshape(X1_C.shape)\n",
    "\n",
    "###### visualize decision function for these parameters\n",
    "    plt.subplot(3, 3, k + 1)\n",
    "    plt.title(\" gam= %r, r = %r , score = %r\"  % (gamma,r, round(clf.score(X_3,y_3), 2)))\n",
    "\n",
    "###### visualize parameter's effect on decision function\n",
    "    plt.contourf(X1_C, X2_C, Z, alpha = 1)\n",
    "    plt.scatter(X_3[:, 0], X_3[:, 1], c=y_3,  edgecolors='gray')\n",
    "    plt.axis('tight')\n",
    "\n",
    "##### Note: The polynomial kernel is very sensitive to the hyperparameter settings. Especially setting a \"wrong\" gamma can have a dramatic effect on the model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real world data set with higher dimensions\n",
    "\n",
    "##### Note: cannot visually represent data with more than 3 dimensions\n",
    "\n",
    "import statsmodels as sm\n",
    "\n",
    "import sklearn.preprocessing as preprocessing\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "salaries = pd.read_csv(\"salaries_final.csv\", index_col = 0)\n",
    "\n",
    "salaries.head()\n",
    "\n",
    "\n",
    "target = pd.get_dummies(salaries.Target, drop_first=True)\n",
    "\n",
    "xcols = salaries.columns[:-1]\n",
    "\n",
    "data = pd.get_dummies(salaries[xcols], drop_first=True)\n",
    "\n",
    "#### Build Linear SVC\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size = 0.25, random_state=123)\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "clf = svm.SVC(probability=True)\n",
    "\n",
    "clf.fit(data_train, target_train['>50K'])\n",
    "\n",
    "total =(time.time() - start_time)\n",
    "\n",
    "total/60\n",
    "\n",
    "clf.predict_proba(data_test)\n",
    "\n",
    "clf.score(data_test, target_test['>50K'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
