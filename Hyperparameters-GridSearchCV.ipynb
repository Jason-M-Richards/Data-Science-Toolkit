{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set grid parameters\n",
    "###### logistic regression params\n",
    "    lr_param_grid = {'C': [.001,.001,.01,.1,1,10], 'penalty': [l1,l2]}\n",
    "###### svc params\n",
    "    svc_param_grid = {'C': [1,10,100,1000], 'gamma': [.0001,.001,.01], 'kernel': ['linear', 'rbf', 'sigmoid']}\n",
    "###### decision tree params\n",
    "    dt_param_grid = {'max_depth': [1,5,10], 'min_samples_split': [.1,1 ,10], 'min_samples_leaf': [.1,.5,5], 'max_features': [2,6,10]}\n",
    "###### xgb params\n",
    "    xgb_param_grid = {\"learning_rate\": [.01,0.1,1],'max_depth': [2,6,10],'min_child_weight': [5,10,15],'n_estimators': [50, 100, 250]}\n",
    "###### adaboost params\n",
    "    ab_param_grid = {\"learning_rate\": [.01,0.1,1],'algorithm': ['SAMME', 'SAMME.R'],'n_estimators': [50, 100, 250]}\n",
    "###### gradient boost params\n",
    "    gb_param_grid = {'learning_rate': [.01,0.1,1],'max_depth': [2,6,10],'min_samples_leaf': [1,3,6],'min_samples_split': [1,2,4],'n_estimators': [50, 100, 250]}\n",
    "###### random forest params\n",
    "    rf_param_grid = {'max_depth': [2,6,10],'min_samples_leaf': [1,3,6],'min_samples_split': [1,2,4],'n_estimators': [5, 10, 25]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "\n",
    "Steps:\n",
    "- determine estimator\n",
    "- define the hyperparameters to tune\n",
    "- define a range of values of your hyperparameters\n",
    "- decide cross-validation scheme\n",
    "- define a score function\n",
    "- include useful info. or functions\n",
    "- setting 'refit' parameter = True allows direct use of the new parameters to fit the data\n",
    "\n",
    "        # Create a Random Forest Classifier with specified criterion\n",
    "        rf_class = RandomForestClassifier(criterion='entropy')\n",
    "\n",
    "        # Create the parameter grid\n",
    "        param_grid = {'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']} \n",
    "\n",
    "        # Create a GridSearchCV object\n",
    "        grid_rf_class = GridSearchCV(\n",
    "            estimator=rf_class,\n",
    "            param_grid=param_grid,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=4,\n",
    "            cv=5,\n",
    "            refit=True, return_train_score=True)\n",
    "        print(grid_rf_class)\n",
    "        \n",
    "### CV properties     \n",
    "\n",
    "    # Read the cv_results property into a dataframe & print it out\n",
    "    cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "    print(cv_results_df)\n",
    "\n",
    "    # Extract and print the column with a dictionary of hyperparameters used\n",
    "    column = cv_results_df.loc[:, ['params']]\n",
    "    print(column)\n",
    "\n",
    "    # Extract and print the row that had the best mean test score\n",
    "    best_row = cv_results_df[cv_results_df['rank_test_score'] == 1 ]\n",
    "    print(best_row)\n",
    "    \n",
    "    # Print out the ROC_AUC score from the best-performing square\n",
    "    best_score = grid_rf_class.best_score_\n",
    "    print(best_score)\n",
    "\n",
    "    # Create a variable from the row related to the best-performing square\n",
    "    cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "    best_row = cv_results_df.loc[[grid_rf_class.best_index_]]\n",
    "    print(best_row)\n",
    "\n",
    "    # Get the n_estimators parameter from the best-performing square and print\n",
    "    best_n_estimators = grid_rf_class.best_params_[\"n_estimators\"]\n",
    "    print(best_n_estimators)\n",
    "    \n",
    "       # See what type of object the best_estimator_ property is\n",
    "    print(type(grid_rf_class.best_estimator_))\n",
    "\n",
    "    # Create an array of predictions directly using the best_estimator_ property\n",
    "    predictions = grid_rf_class.best_estimator_.predict(X_test)\n",
    "\n",
    "    # Take a look to confirm it worked, this should be an array of 1's and 0's\n",
    "    print(predictions[0:5])\n",
    "\n",
    "    # Now create a confusion matrix \n",
    "    print(\"Confusion Matrix \\n\", confusion_matrix(y_test, predictions))\n",
    "\n",
    "    # Get the ROC-AUC score\n",
    "    predictions_proba = grid_rf_class.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    print(\"ROC-AUC Score \\n\", roc_auc_score(y_test, predictions_proba)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV\n",
    "- similar to GridSearch, but takes a range of values and the number of iterations is set, rather than taking every combination\n",
    "- parameter differences\n",
    "    - n_iter = # of random iterations\n",
    "    - param_distributions = same as param_grid\n",
    "\n",
    "\n",
    "            # Create the parameter grid\n",
    "            param_grid = {'learning_rate': np.linspace(0.1,2,150), 'min_samples_leaf': list(range(20,65))} \n",
    "\n",
    "            # Create a random search object\n",
    "            random_GBM_class = RandomizedSearchCV(\n",
    "                estimator = GradientBoostingClassifier(),\n",
    "                param_distributions = param_grid,\n",
    "                n_iter = 10,\n",
    "                scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)\n",
    "\n",
    "            # Fit to the training data\n",
    "            random_GBM_class.fit(X_train, y_train)\n",
    "\n",
    "### Properties          \n",
    "           \n",
    "           # Print the values used for both hyperparameters\n",
    "            print(random_GBM_class.cv_results_['param_learning_rate'])\n",
    "            print(random_GBM_class.cv_results_['param_min_samples_leaf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informed Search\n",
    "- sequential model building and learning\n",
    "\n",
    "### coarse to fine tuning\n",
    "- random search\n",
    "- review results\n",
    "- grid search in the smaller area (could also be more random searches)\n",
    "- continue until optimal score obtained\n",
    "\n",
    "### bayesian tuning\n",
    "- pick hyperparameter combination\n",
    "- build model\n",
    "- get new evidence\n",
    "- update beliefs and chose better hyperparameters next round\n",
    "- popular for larger tasks\n",
    "- Hyperopt package\n",
    "    - needs uniform distributions, so a hp.quniform or hp.uniform is recommended\n",
    "    - an objective function needs to be created to calculate loss per iteration\n",
    "\n",
    "            \n",
    "            from hyperopt import hp, space, tpe, fmin\n",
    "            # Set up space dictionary with specified hyperparameters\n",
    "            space = {'max_depth': hp.quniform('max_depth', 2, 10, 2),'learning_rate': hp.uniform('learning_rate', 0.001,0.9)}\n",
    "\n",
    "            # Set up objective function\n",
    "            def objective(params):\n",
    "                params = {'max_depth': int(params['max_depth']),'learning_rate': params['learning_rate']}\n",
    "                gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) \n",
    "                best_score = cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=2, n_jobs=4).mean()\n",
    "                loss = 1 - best_score\n",
    "                return loss\n",
    "\n",
    "            # Run the algorithm\n",
    "            best = fmin(fn=objective,space=space, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest)\n",
    "            print(best)\n",
    "            \n",
    "### genetic algorithms\n",
    "- create some models\n",
    "- pick the best by scoring function\n",
    "- create new models similar to the best ones\n",
    "- add randomness so local optimum isn't reached\n",
    "- repeat\n",
    "- TPOT library\n",
    "    - generations - # of cycles\n",
    "    - population_size = # of models to keep\n",
    "    - offspring_size = # of offspring in each \n",
    "    - mutation_rate = proportion of pipelines to apply randomness\n",
    "    - crossover_rate = proportion of pipelines to breed each iteration\n",
    "    - scoring = objective function\n",
    "    - cv = cross validation\n",
    "    \n",
    "            # Assign the values outlined to the inputs\n",
    "            number_generations = 3\n",
    "            population_size = 4\n",
    "            offspring_size = 3\n",
    "            scoring_function = 'accuracy'\n",
    "\n",
    "            # Create the tpot classifier\n",
    "            tpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size,\n",
    "                                      offspring_size=offspring_size, scoring=scoring_function,\n",
    "                                      verbosity=2, random_state=2, cv=2)\n",
    "\n",
    "            # Fit the classifier to the training data\n",
    "            tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "            # Score on the test set\n",
    "            print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamter Visualizations for Classification\n",
    "\n",
    "#### Max Tree Depth\n",
    "    max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for max_depth in max_depths:\n",
    "       dt = DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)\n",
    "       dt.fit(x_train, y_train)\n",
    "       train_pred = dt.predict(x_train)\n",
    "       false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "       roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "       ###### Add auc score to previous train results\n",
    "       train_results.append(roc_auc)\n",
    "       y_pred = dt.predict(x_test)\n",
    "       false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "       roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "       ###### Add auc score to previous test results\n",
    "       test_results.append(roc_auc)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(max_depths, train_results, 'b', label='Train AUC')\n",
    "    plt.plot(max_depths, test_results, 'r', label='Test AUC')\n",
    "    plt.ylabel('AUC score')\n",
    "    plt.xlabel('Tree depth')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#### Minimum Sample Split\n",
    "    min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for min_samples_split in min_samples_splits:\n",
    "       dt = DecisionTreeClassifier(criterion='entropy', min_samples_split=min_samples_split)\n",
    "       dt.fit(x_train, y_train)\n",
    "       train_pred = dt.predict(x_train)\n",
    "       false_positive_rate, true_positive_rate, thresholds =    roc_curve(y_train, train_pred)\n",
    "       roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "       train_results.append(roc_auc)\n",
    "       y_pred = dt.predict(x_test)\n",
    "       false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "       roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "       test_results.append(roc_auc)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(min_samples_splits, train_results, 'b', label='Train AUC')\n",
    "    plt.plot(min_samples_splits, test_results, 'r', label='Test AUC')\n",
    "    plt.xlabel('Min. Sample splits')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#### Minimum Sample Leaf\n",
    "    min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for min_samples_leaf in min_samples_leafs:\n",
    "       dt = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=min_samples_leaf)\n",
    "       dt.fit(x_train, y_train)\n",
    "       train_pred = dt.predict(x_train)\n",
    "       false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "       roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "       train_results.append(roc_auc)\n",
    "       y_pred = dt.predict(x_test)\n",
    "       false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "       roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "       test_results.append(roc_auc)\n",
    "    plt.figure(figsize=(12,6))    \n",
    "    plt.plot(min_samples_leafs, train_results, 'b', label='Train AUC')\n",
    "    plt.plot(min_samples_leafs, test_results, 'r', label='Test AUC')\n",
    "    plt.ylabel('AUC score')\n",
    "    plt.xlabel('Min. Sample Leafs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#### Maximum Features\n",
    "    max_features = list(range(1,x_train.shape[1]))\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for max_feature in max_features:\n",
    "       dt = DecisionTreeClassifier(criterion='entropy', max_features=max_feature)\n",
    "       dt.fit(x_train, y_train)\n",
    "       train_pred = dt.predict(x_train)\n",
    "       false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "       roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "       train_results.append(roc_auc)\n",
    "       y_pred = dt.predict(x_test)\n",
    "       false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "       roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "       test_results.append(roc_auc)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(max_features, train_results, 'b', label='Train AUC')\n",
    "    plt.plot(max_features, test_results, 'r', label='Test AUC')\n",
    "    plt.ylabel('AUC score')\n",
    "    plt.xlabel('max features')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#### Tree Depth (Regressor using R2 and MSE)\n",
    "    max_depths = np.linspace(1, 30, 30, endpoint=True)\n",
    "    mse_results = []\n",
    "    r2_results = []\n",
    "    for max_depth in max_depths:\n",
    "        regressor = DecisionTreeRegressor(max_depth=max_depth, random_state=45)\n",
    "        regressor.fit(x_train, y_train)\n",
    "        y_pred = regressor.predict(x_test)\n",
    "        score = performance(y_test, y_pred)\n",
    "        r2_results.append(score[0])\n",
    "        mse_results.append(score[1])\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(max_depths, r2_results, 'b', label='R2')\n",
    "    plt.xlabel('Tree Depth')\n",
    "    plt.ylabel('R-squared')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(max_depths, mse_results, 'r', label='MSE')\n",
    "    plt.xlabel('Tree Depth')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
