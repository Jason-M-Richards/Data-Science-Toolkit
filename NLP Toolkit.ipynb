{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working With Text, Simplified\n",
    "\n",
    "Generally, projects that work with text data follow the same overall pattern as any other projects. The main difference is that text projects usually require a bit more cleaning and preprocessing than regular data, in order to get the text into a format that's usable for modeling. \n",
    "\n",
    "Here are some of the ways that NLTK can make our lives easier when working with text data:\n",
    "\n",
    "* **_Stop Word Removal_**. NLTK contains a full library of stop words, making it easy to remove the words that don't matter from our data.  \n",
    "<br>  \n",
    "* **_Filtering and Cleaning_**. NLTK provides simple, easy ways to create and filter Frequency Distributions, as well providing mutliple ways to clean, stem, lemmatize, or tokenize datasets.\n",
    "<br>  \n",
    "* **_Feature Selection and Feature Engineering_**. NLTK contains tools to quickly generate features such as bigrams and ngrams. It major libraries such as the **_Penn Tree Bank_** to allow quick feature engineering, such as generating Part-of-Speech tags, or sentence polarity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions (Regex) and Use Cases for NLP\n",
    "\n",
    "Regex is especially useful for Natural Language Processing. By definition, just about any text document you work with on an NLP task is going to be one that contains a large amount of text. One of the more common NLP-specific use cases for regex is to use regex during the tokenization stage to define the rules for where we should split strings into separate tokens. As an example, NLTK's basic `word_tokenize` function would split a word that contains an apostrophe into 3 separate tokens--`'they're'` gets broken into `[\"they\", \"'\", \"re\"]`. This is because the word tokenizer has instructions to just grab sequences of letters as the basic tokens, and an apostrophe isn't a letter. When preprocessing text data, it's quite common to use some small regex patterns to create a more intelligent tokenization scheme to avoid problems like this, so that our tokenizer treats words like `'they're'` as a single token.\n",
    "\n",
    "### Creating Basic Patterns\n",
    "\n",
    "Regex is only as good as the **_Patterns_** we create. We can use these patterns to find, or to replace text. There are many, many things we can do with regex, and covering them all is outside the scope of this lesson. Instead, we'll just focus on some of the more useful, basic patterns that allow us to begin using regex to work with text data. \n",
    "\n",
    "Let's take a look at a basic regex pattern, to get a feel for what they look like. \n",
    "\n",
    "```python\n",
    "import re\n",
    "sentence = 'he said that she said \"hello\".'\n",
    "pattern = 'he'\n",
    "p = re.compile(sentence)\n",
    "p.findall() # Output will be ['he', 'he, 'he']\n",
    "```\n",
    "\n",
    "We define a pattern by just writing in a python string. We can then use the regular expressions library, `re`, to compile this pattern. Once we have a compiled pattern, we just need to pass in a string and the pattern will find every instance of that pattern in the string. \n",
    "\n",
    "For people new to regex, the results from the pattern above might be surprising at first. The pattern successfully matches the word 'he', but it also matches the letters 'he' that are found inside of the words 'she' and 'hello'.  Subsequences inside of larger sequences are fair game to regex. If we just wanted to match the word 'he', we would need to specify that the pattern needs to start and end with a space, or use of **_anchors_** for things like word boundaries. \n",
    "\n",
    "### Ranges, Groups, and Quantifiers\n",
    "\n",
    "Obviously, we don't want to have to explicitly type every valid match for any search into our pattern. That would defeat the purpose. Luckily, we don't have to type every possible uppercase letter to match on uppercase letters. Instead, we can use a **_Range_** such as `[A-Z]`. This will match any uppercase letter. Ranges are always inside of square brackets. We can put many things inside of ranges at the same time, and regex will match on any of them. For instance, of if we wanted to find any uppercase letter, lowercase letter, or digit, we could use `[A-Za-z0-9]`. \n",
    "\n",
    "\n",
    "### Character Classes\n",
    "\n",
    "Character classes are a special case of ranges. Since it's quite a common task to use ranges to do things like match on words or numbers, regex actually includes character classes as a shortcut. For instance, we could use `\\d` to match any digit--this is equivalent to using `[0-9]`. We could also use `\\w` to match on any word. In the same vein, we can use `\\D` to get anything that _isn't_ a digit, or `\\W` to match on everything that isn't a word. There are a few other types of character classes as well. For a full list, check out the cheat sheet below!\n",
    "\n",
    "### Groups and Quantifiers\n",
    "\n",
    "Groups are kind of like ranges, but they specify an exact pattern to match on. Groups are denoted by parentheses. Whereas `[A-Z0-9]` matches on any uppercase letter or any digit, `(A-Z0-9)` will only match on the sequence `'A-Z0-9'` exactly. This becomes much more useful when paired with **_Quantifiers_**, which allows us to specify how many times a group should happen in a row. If we want to specify an exact number of times, we can use curly braces. For instance, a group followed by `{3}` will only match on patterns that have that group repeated exactly 3 times. The most common quantifiers are usually:\n",
    "\n",
    "* `*` (0 or more times)\n",
    "* `+` (1 or more times)\n",
    "* `?` (0 or 1 times)\n",
    "\n",
    "In this way, we can fill a grouping with any pattern, tell and specify the number of times we can expect to see that pattern. When we include things like ranges, groupings, and quantifiers together, it becomes easy to write a pattern that can match complex things, like email addresses--take a look at the example provided below, and see if you can figure out how it works!\n",
    "\n",
    "`'([A-Za-z]+)@([A-Za-z]+)\\.com'` \n",
    "\n",
    "This pattern matches basic email addresses like 'joe@gmail.com', but not 'john.doe@gmail.com', or 'joe@stanford.edu'. Take a look at the pattern again--how would you need to modify the pattern in order for it to match either of those, as well?\n",
    "\n",
    "### Cheat Sheet Link\n",
    "\n",
    "https://www.cheatography.com/davechild/cheat-sheets/regular-expressions/\n",
    "\n",
    "#### Sample Regex for Phone Number\n",
    "\n",
    "    pattern = '(\\(\\d{3}\\) (\\d{3}-\\d{4}))'\n",
    "    p = re.compile(pattern)\n",
    "    digits = p.findall(file)\n",
    "    digits\n",
    "#### Sample Regex for Price\n",
    "\n",
    "    pattern = '(\\$\\d+\\.?\\d*)'\n",
    "    p = re.compile(pattern)\n",
    "    digits = p.findall(file)\n",
    "    digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Approaches to NLP Preprocessing\n",
    "\n",
    "As you've likely noticed by now, working with text data comes with **_a lot_** of ambiguity. When all we start with is an arbitrarily-sized string of words, there's no clear answer as to what sorts of features we should engineer, or even where we should start! The goal of this lesson is to provide a framework for working with text data, and help us figure out exactly what sorts of features we should create when working with text data. \n",
    "\n",
    "In this lesson, we'll focus on the following topics:\n",
    "* Feature Extraction\n",
    "* Tokenization\n",
    "* Bag of Words\n",
    "* Stopword Removal\n",
    "* Frequency Distributions\n",
    "* Stemming and Lemmatization\n",
    "* Bigrams, Ngrams, and Mutual Information Score\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "#### number of characters\n",
    "\n",
    "    num_char=len(text)\n",
    "    \n",
    "#### number of words\n",
    "\n",
    "- assuming text split by spaces\n",
    "\n",
    "        words = text.split()\n",
    "        print(len(words))\n",
    "\n",
    "#### average word count\n",
    "\n",
    "    # Function that returns number of words in a string\n",
    "    def count_words(string):\n",
    "        # Split the string into words\n",
    "        words = string.split()\n",
    "    \n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "    # Create a new feature word_count\n",
    "    ted['word_count'] = ted['transcript'].apply(count_words)\n",
    "\n",
    "    # Print the average word count of the talks\n",
    "    print(ted['word_count'].mean())\n",
    "\n",
    "#### average character count\n",
    "\n",
    "    # Create a feature char_count\n",
    "    tweets['char_count'] = tweets['content'].apply(len)\n",
    "\n",
    "    # Print the average character count\n",
    "    print(tweets['char_count'].mean())\n",
    "\n",
    "#### special character count\n",
    "\n",
    "    # Function that returns numner of hashtags in a string\n",
    "    def count_hashtags(string):\n",
    "     # Split the string into words\n",
    "        words = string.split()\n",
    "\n",
    "     # Create a list of words that are hashtags\n",
    "        hashtags = [word for word in words if word.startswith('#')]\n",
    "\n",
    "     # Return number of hashtags\n",
    "        return(len(hashtags))\n",
    "        \n",
    "#### readability score\n",
    "\n",
    "- uses Textatistic package\n",
    "- calculates Flesch and Gunning Fog reading ease scores\n",
    "\n",
    "##### Flesch\n",
    "        # Import Textatistic\n",
    "        from textatistic import Textatistic\n",
    "\n",
    "        # Compute the readability scores \n",
    "        readability_scores = Textatistic(sisyphus_essay).scores\n",
    "\n",
    "        # Print the flesch reading ease score\n",
    "        flesch = readability_scores['flesch_score']\n",
    "        print(\"The Flesch Reading Ease is %.2f\" % (flesch))\n",
    "        \n",
    "##### Gunning Fog\n",
    "\n",
    "    # Import Textatistic\n",
    "    from textatistic import Textatistic\n",
    "\n",
    "    # List of excerpts\n",
    "    excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "    # Loop through excerpts and compute gunning fog index\n",
    "    gunning_fog_scores = []\n",
    "    for excerpt in excerpts:\n",
    "      readability_scores = Textatistic(excerpt).scores\n",
    "      gunning_fog = readability_scores['gunningfog_score']\n",
    "      gunning_fog_scores.append(gunning_fog)\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "- turning strings into smaller chunks\n",
    "- many theories and rules\n",
    "- nltk most common kit\n",
    "- SpaCy also available\n",
    "\n",
    "#### word_tokenize\n",
    "\n",
    "    # Import necessary modules\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    # Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "    tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "    # Make a set of unique tokens in the entire scene: unique_tokens\n",
    "    unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "#### sent_tokenize\n",
    "\n",
    "- tokenize a document into sentences\n",
    "\n",
    "        # Import necessary modules\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "\n",
    "        # Split scene_one into sentences: sentences\n",
    "        sentences = sent_tokenize(scene_one)\n",
    "\n",
    "#### regex_tokenize\n",
    "\n",
    "- tokenize string based on regex pattern\n",
    "\n",
    "        # Import the necessary modules\n",
    "        from nltk.tokenize import regexp_tokenize\n",
    "        # Define a regex pattern to find hashtags: pattern1\n",
    "        pattern1 = r\"#\\w+\"\n",
    "        # Use the pattern on the first tweet in the tweets list\n",
    "        hashtags = regexp_tokenize(tweets[0], pattern1)\n",
    "        print(hashtags)\n",
    "\n",
    "#### TweetTokenizer\n",
    "\n",
    "- special class for tweet tokenization which allows to separate hashtags, mentions and repeated exclamation points.\n",
    "\n",
    "        from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "        # Use the TweetTokenizer to tokenize all tweets into one list\n",
    "        tknzr = TweetTokenizer()\n",
    "        all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "        print(all_tokens)\n",
    "        \n",
    "#### SpaCy tokenizing\n",
    "\n",
    "    import spacy\n",
    "\n",
    "    # Load the en_core_web_sm model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Create a Doc object\n",
    "    doc = nlp(gettysburg)\n",
    "\n",
    "    # Generate the tokens\n",
    "    tokens = [token.text for token in doc]\n",
    "    print(tokens)\n",
    "        \n",
    "### Bag of Words\n",
    "\n",
    "- basic method for finding topics in text\n",
    "- need to tokenize first\n",
    "- counts up all the token\n",
    "- context of corpus is lost (consider n-grams)\n",
    "\n",
    "        # Import Counter\n",
    "        from collections import Counter\n",
    "\n",
    "        # Tokenize the article: tokens\n",
    "        tokens = word_tokenize(article)\n",
    "\n",
    "        # Convert the tokens into lowercase: lower_tokens\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "        # Create a Counter with the lowercase tokens: bow_simple\n",
    "        bow_simple = Counter(lower_tokens)\n",
    "\n",
    "        # Print the 10 most common tokens\n",
    "        print(bow_simple.most_common(10))\n",
    "\n",
    "### Removing Stop Words\n",
    "\n",
    "When working with text data, one of the first steps to try is to remove the **_Stop Words_** from the text. One common feature of text data (regardless of language!) is the inclusion of stop words for grammatical structure. Words such as \"a\", \"and\", \"but\", and \"or\" are examples of stop words. While a sentence would be both grammatically incorrect and hard to understand without them, from a modeling standpoint, stop words provide little to no actual value. If we create a **_Frequency Distribution_** to see the number of times each word is used in a corpus, we'll almost always find that the top spots are dominated by stop words, which tell us nothing about the actual content of the corpus. Removing stop words allows us to reduce the overall dimensionality of our dataset (which is always a good thing), while also distilling the overall vocabulary of our Bag-Of-Words down only to the words that really matter. \n",
    "\n",
    "_NLTK_ makes it extremely easy to remove stopwords. The library includes a full corpus of all stopwords for all the languages NLTK supports. Since we usually only want the stopwords relevant to the language our text data is in, NLTK even makes it easy to filter out the unneeded stop words and grab only the ones that pertain to our problem. \n",
    "\n",
    "The following example shows how we can get all the stopwords for English from NLTK:\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "# It is generally a good idea to also remove punctuation\n",
    "import string\n",
    "\n",
    "# Now we have a list that includes all english stopwords, as well as all punctuation\n",
    "stopwords_list += list(string.punctuation)\n",
    "```\n",
    "\n",
    "Once we have a list of stopwords, we can easily remove them from our text data after we've tokenized our data. Recall that we can easily tokenize text data using NLTK's `word_tokenize` function. Once we have a list of word tokens, all we need to do is use a list comprehension, and omit any tokens that can be found in our stopwords list.  For example:\n",
    "\n",
    "```python\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(some_text_data)\n",
    "\n",
    "# It is usually a good idea to lowercase all tokens during this step, as well\n",
    "stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "\n",
    "## Frequency Distributions\n",
    "\n",
    "Once we have tokenized our data and removed all the stop words, the next step is usually to explore our text data through a **_Frequency Distribution_**. This is just a fancy way of saying that we create a histogram that tells us the total number of times each word is used in a given corpus. \n",
    "\n",
    "Once we have tokenized our text data, we can use NLTK to easily create a Frequency Distribution using `nltk.FreqDist()`. A Frequency Distribution is analagous to a python dictionary, with a few more bells and whistles attached to make it easier to use for NLP tasks. Each key is a word token, and each value is the corresponding number of times that token appeared in the tokenized corpus given to the `FreqDist` object at instantiation. \n",
    "\n",
    "We can easily filter a FreqDist to see the most common words by using the built-in method, as seen below:\n",
    "\n",
    "```python\n",
    "from  nltk import FreqDist\n",
    "freqdist = FreqDist(tokens)\n",
    "\n",
    "# get the 200 most common words \n",
    "most_common = freqdist.most_common(200)\n",
    "```\n",
    "\n",
    "Once we have the most common words, we can easily use this to filter out the text and reduce the dimensionality of particularly large datasets, as needed. \n",
    "\n",
    "### Stemming and Lemmatization\n",
    "\n",
    "Consider the words 'run', 'running', 'ran', and 'runs'. If we create a basic frequency distribution, each of these words will be treated as a separate token. After all, they are different words. However, we know that they pretty much mean the same thing. Counting these words as individual separate tokens can sometimes hurt our model by needlessly increasing dimensionality, and hiding important information from our model. Although we instinctively know that those four words are all talking about the same action, our model will default to thinking that they are four completely different concepts. The way we deal with this is to remove suffixes through techniques such as **_Stemming_** or **_Lemmatization_**.\n",
    "\n",
    "People often get stemming and lemmatization confused, because they are extremely similar. They generally accomplish the same task, but they use different means to do so. \n",
    "\n",
    "**_Stemming_** follows a predetermined set of rules to reduce a word to its _stem_.  Words like 'running' and 'runs' will be reduced down to 'run', because the stemmer contains rules that understands how to deal with suffixes such as '-ing' and '-s'. The best stemmer currently available is the **_Porter Stemmer_**. For code samples demonstrating how to use it, check out NLTK's documentation for the [Porter Stemmer](http://www.nltk.org/howto/stem.html).\n",
    "\n",
    "**_Lemmatization_** differs from stemming in that it reduces each word down to a linguistically valid **_lemma_**, or root word. It does this through stored linguistic mappings. Lemmatization is generally more complex, but also more accurate. This is because the rules that guide things like the Porter Stemmer are good, but far from perfect. For example, Stemmers commonly deal with the suffix `-ed` by just  dropping it from the word. This usually works, until it runs into an edge case like the word 'agreed'. When stemmed, 'agreed' becomes 'agre'. Lemmatization does not make this mistake, because it contains a mapping for the word that tells it what 'agreed' should be reduced down to. Generally, most lemmatizers make use of the famous **_WordNet_** lexical database. \n",
    "\n",
    "#### NLTK lemmatization\n",
    "\n",
    "```python\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizer.lemmatize('feet') # foot\n",
    "lemmatizer.lemmatize('running') # run\n",
    "```\n",
    "\n",
    "#### SpaCy lemmatization\n",
    "\n",
    "    import spacy\n",
    "\n",
    "    # Load the en_core_web_sm model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Create a Doc object\n",
    "    doc = nlp(gettysburg)\n",
    "\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "    # Convert lemmas into a string\n",
    "    print(' '.join(lemmas))\n",
    "\n",
    "### N-grams and Mutual Information Score\n",
    "\n",
    "- ngrams create collections of words from a corpus based on the n(number of words) can be 2 (bigram) 3(trigram)...etc... \n",
    "- Would split \"the dog played outside\" into (bigram) `('the', 'dog'), ('dog', 'played'), ('played', 'outside')`. \n",
    "- we can apply a frequency filter to only keep bigrams that show up more than a set number of times\n",
    "    - experts tend to apply a minimum frequency filter of 5. \n",
    "- **_Pointwise Mutual Information Score_**. This is a statistical measure from information theory that generally measures the mutual dependence between two words in a bigram\n",
    "- NLTK provides an easy way to perform MIS\n",
    "- can cause a dimensionality problem\n",
    "- best to keep ngrams at or below three in most cases\n",
    "- sklearns CountVectorizer package has an argument ngram_range\n",
    "\n",
    "        # Generating ngrams\n",
    "        vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "### Vectorization Strategies\n",
    "\n",
    "Once we cleaned and tokenized our text data, we can convert it to vectors. However, there are a few different ways we can do this. Depending on our goals and our dataset, some may be more useful than others. \n",
    "\n",
    "#### Count Vectorization\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "- count the number of times each word appears\n",
    "- If working with a single document, we just create a single vector, where each element in the vector corresponds to the count of a unique word in the document. \n",
    "- If working with multiple documents, we would store everything in a DataFrame, with each column representing a unique word, while each row represents the the count vector for a given document. \n",
    "\n",
    "        # Import CountVectorizer\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "        # Create CountVectorizer object\n",
    "        vectorizer = CountVectorizer()\n",
    "\n",
    "        # Generate matrix of word vectors\n",
    "        bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "        # Print the shape of bow_matrix\n",
    "        print(bow_matrix.shape)\n",
    "\n",
    "        # Convert bow_matrix into a DataFrame\n",
    "        bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "        # Map the column names to vocabulary \n",
    "        bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "        # Print bow_df\n",
    "        print(bow_df)\n",
    "\n",
    "#### TF-IDF Vectorization\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "sklearn.feature_extraction.text.TfidfVectorizer\n",
    "\n",
    "- stands for **_Term Frequency-Inverse Document Frequency_**.  \n",
    "- combination of two individual metrics, which are the TF and IDF, respectively\n",
    "- used when we have multiple documents\n",
    "- places more weight on rare words that show up and less weight to less import words that show up more\n",
    "\n",
    "        # Import TfidfVectorizer\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "        # Create TfidfVectorizer object\n",
    "        vectorizer = TfidfVectorizer()\n",
    "\n",
    "        # Generate matrix of word vectors\n",
    "        tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "        # Print the shape of tfidf_matrix\n",
    "        print(tfidf_matrix.shape)\n",
    "\n",
    "### Cosine Score\n",
    "\n",
    "- value between 0 and 1 when working with text\n",
    "- robust for document length\n",
    "\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        # Initialize an instance of tf-idf Vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "        # Generate the tf-idf vectors for the corpus\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "        # Compute and print the cosine similarity matrix\n",
    "        cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "        print(cosine_sim)\n",
    "        \n",
    "### Linear Kernal\n",
    "- performs the same operation of cosine similarity on tfidf vectorized data\n",
    "\n",
    "        # Compute cosine similarity matrix\n",
    "        cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "        # Print cosine similarity matrix\n",
    "        print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim Dictionary\n",
    "- create a dictionary of words stired as a list of tuples that can be reused\n",
    "\n",
    "        # Import Dictionary\n",
    "        from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "        # Create a Dictionary from the articles: dictionary\n",
    "        dictionary = Dictionary(articles)\n",
    "\n",
    "        # Select the id for \"computer\": computer_id\n",
    "        computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "        # Use computer_id with the dictionary to print the word\n",
    "        print(dictionary.get(computer_id))\n",
    "\n",
    "        # Create a MmCorpus: corpus\n",
    "        corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "        # Print the first 10 word ids with their frequency counts from the fifth document\n",
    "        print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting Tokenized Words\n",
    "\n",
    "- using matplotlib to visualize word counts in a corpus\n",
    "\n",
    "        # Split the script into lines: lines\n",
    "        lines = holy_grail.split('\\n')\n",
    "\n",
    "        # Replace all script lines for speaker\n",
    "        pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "        lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "        # Tokenize each line: tokenized_lines\n",
    "        tokenized_lines = [regexp_tokenize(s, r'\\w+') for s in lines]\n",
    "\n",
    "        # Make a frequency list of lengths: line_num_words\n",
    "        line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "        # Plot a histogram of the line lengths\n",
    "        plt.hist(line_num_words)\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition NER) with NLTK\n",
    "\n",
    "- NLP task to identify important named entities in the text\n",
    "    - people, places, organizations\n",
    "    - dates, states, works of art\n",
    "    - other categories\n",
    "- can be used alongside topic identification\n",
    "- answers 5 w's\n",
    "\n",
    "            # Tokenize the article into sentences: sentences\n",
    "            sentences = sent_tokenize(article)\n",
    "\n",
    "            # Tokenize each sentence into words: token_sentences\n",
    "            token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "            # Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "            pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "            # Create the named entity chunks: chunked_sentences\n",
    "            chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "            # Test for stems of the tree with 'NE' tags\n",
    "            for sent in chunked_sentences:\n",
    "                for chunk in sent:\n",
    "                    if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "                        print(chunk)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "            # Create the defaultdict: ner_categories\n",
    "            ner_categories = defaultdict(int)\n",
    "\n",
    "            # Create the nested for loop\n",
    "            for sent in chunked_sentences:\n",
    "                for chunk in sent:\n",
    "                    if hasattr(chunk, 'label'):\n",
    "                        ner_categories[chunk.label()] += 1\n",
    "            \n",
    "            # Create a list from the dictionary keys for the chart labels: labels\n",
    "            labels = list(ner_categories.keys())\n",
    "\n",
    "            # Create a list of the values: values\n",
    "            values = [ner_categories.get(v) for v in labels]\n",
    "\n",
    "            # Create the pie chart\n",
    "            plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "            # Display the chart\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy NER\n",
    "\n",
    "- similar to gensim with different implementations\n",
    "- Focus on creating NLP pipelines to generate models and corpora\n",
    "- open source, other packages (Dispiacy - visual tool)\n",
    "- more robust than nltk\n",
    "\n",
    "\n",
    "        # Import spacy\n",
    "        import spacy\n",
    "\n",
    "        # Instantiate the English model: nlp\n",
    "        nlp = spacy.load('en', tagger=False, parser=False, matcher=False)\n",
    "\n",
    "        # Create a new document: doc\n",
    "        doc = nlp(article)\n",
    "\n",
    "        # Print all of the found entities and their labels\n",
    "        for ent in doc.ents:\n",
    "            print(ent.label_, ent.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with polyglot\n",
    "\n",
    "- wide variety of language support\n",
    "- transliteration (swapping characters from one language to another)\n",
    "- detects language used\n",
    "\n",
    "        # Create a new text object using Polyglot's Text class: txt\n",
    "        txt = Text(article)\n",
    "\n",
    "        # Print each of the entities found\n",
    "        for ent in txt.entities:\n",
    "            print(ent)\n",
    "\n",
    "        # Print the type of ent\n",
    "        print(type(ent))\n",
    "        \n",
    "        # Create the list of tuples: entities\n",
    "        entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "        # Print entities\n",
    "        print(entities)\n",
    "        \n",
    "        # Initialize the count variable: count\n",
    "        count = 0\n",
    "\n",
    "        # Iterate over all the entities\n",
    "        for ent in txt.entities:\n",
    "            # Check whether the entity contains 'Márquez' or 'Gabo'\n",
    "            if \"Márquez\" in ent or \"Gabo\" in ent:\n",
    "                # Increment count\n",
    "                count += 1\n",
    "\n",
    "        # Print count\n",
    "        print(count)\n",
    "\n",
    "        # Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "        percentage = count / len(txt.entities)\n",
    "        print(percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP on Macbeth\n",
    "\n",
    "    #import packages\n",
    "    import nltk\n",
    "    from nltk.corpus import gutenberg, stopwords\n",
    "    from nltk.collocations import *\n",
    "    from nltk import FreqDist\n",
    "    from nltk import word_tokenize\n",
    "    import string\n",
    "    import re\n",
    "    #look at available corpuae\n",
    "    file_ids = gutenberg.fileids()\n",
    "    file_ids\n",
    "    #get macbeth and verify fist 1000 words\n",
    "    macbeth_text = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "    print(macbeth_text[:1000])\n",
    "    Preprocessing the Data¶\n",
    "\n",
    "        #Preprocessing Data\n",
    "        pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\" #removes all numbers and finds words with apostrophes\n",
    "        macbeth_tokens_raw = nltk.regexp_tokenize(macbeth_text, pattern)#tokenize data\n",
    "        macbeth_tokens = macbeth_tokens = [word.lower() for word in macbeth_tokens_raw] #lowercase all\n",
    "\n",
    "        #Create a Frequency Distribution\n",
    "        macbeth_freqdist = FreqDist(macbeth_tokens)\n",
    "        macbeth_freqdist.most_common(50)\n",
    "\n",
    "        #Remove stopwords\n",
    "        stopwords_list = stopwords.words('english') #gets all english stopwords\n",
    "        stopwords_list += list(string.punctuation) #adds punctuation to stopwords\n",
    "        stopwords_list += ['0','1','2','3','4','5','6','7','8','9'] adds all numbers to stopwords\n",
    "        macbeth_words_stopped = [word for word in macbeth_tokens if word not in stopwords_list]\n",
    "       \n",
    "        #Recreate a frequency distribution\n",
    "        macbeth_stopped_freqdist = FreqDist(macbeth_words_stopped)\n",
    "        macbeth_stopped_freqdist.most_common(50)\n",
    "        # check vocabulary length\n",
    "        len(macbeth_stopped_freqdist)\n",
    "        \n",
    "        #Normalize the frequency distribution as a percentage\n",
    "        total_word_count = sum(macbeth_stopped_freqdist.values())\n",
    "        macbeth_top_50 = macbeth_stopped_freqdist.most_common(50)\n",
    "        print(\"Word\\t\\t\\tNormalized Frequency\")\n",
    "        for word in macbeth_top_50:\n",
    "            normalized_frequency = word[1] / total_word_count\n",
    "            print(\"{} \\t\\t\\t {:.4}\".format(word[0], normalized_frequency))\n",
    "            \n",
    "        #Create Bigram\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures() #function to creat bigrams\n",
    "        macbeth_finder = BigramCollocationFinder.from_words(macbeth_words_stopped) #instantiate w/ text\n",
    "        macbeth_scored = macbeth_finder.score_ngrams(bigram_measures.raw_freq) #grabs scores\n",
    "        macbeth_scored[:50] \n",
    "        \n",
    "        #Get Pointwise Mutual Information Scores\n",
    "        macbeth_pmi_finder = BigramCollocationFinder.from_words(macbeth_words_stopped) #create another finder\n",
    "        macbeth_pmi_finder.apply_freq_filter(5) #apply frequency filter at 5\n",
    "        macbeth_pmi_scored = macbeth_pmi_scored = macbeth_pmi_finder.score_ngrams(bigram_measures.pmi) #calculate pmi scores\n",
    "        macbeth_pmi_scored[:50] #grab first 50 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Free Grammar and POS Tagging\n",
    "\n",
    "#### CFG meaning\n",
    "Speech contains an underlying \"deep structure\" that we recognize, regardless of the actual content of the the sentence. We don't actually need any context about what the sentence is actually about to determine if the grammar is correct\n",
    "\n",
    "#### Sentence Structure and Parsing\n",
    "One way that we can help a computer understand how to interpret a sentence is to create a CFG for it to use when parsing. The CFG defines the rules of how sentences can exist. \n",
    "\n",
    "#### Sample Sentence Structure\n",
    "* `S -> NP VP` A sentence (S) consists of a Noun Phrase (NP) followed by a Verb Phrase (VP).\n",
    "* `PP -> P NP` A Prepositional Phrase (PP) consists of a Preposition (P) followed by a Noun Phrase (NP)\n",
    "* `NP -> Det N | Det N PP | 'I'` A Noun Phrase (NP) can consist of:\n",
    "    * a Determiner (Det) followed by a Noun (N), or (as denoted by `|`) \n",
    "    * a Determiner (Det) followed by a Noun (N), followed by a Prepositional Phrase (PP), or\n",
    "    * The token `'I'`.\n",
    "* `VP -> V NP | VP PP` A Verb Phrase can consist of:\n",
    "    * a Verb (V) followed by a Noun Phrase (NP) or\n",
    "    * a Verb Phrase (VP) followed by a Prepositional Phrase (PP)\n",
    "* `Det -> 'an' | 'my'` Determiners are the tokens 'an' or 'my'\n",
    "* `N -> 'elephant' | 'pajamas'` Nouns are the tokens 'elephant' or 'pajamas'\n",
    "* `V -> 'shot'` Verbs are the token 'shot'\n",
    "* `P -> 'in'` Prepositions are the token 'in'\n",
    "\n",
    "#### Generating POS Tags\n",
    "\n",
    "- document should be tokenized before performing POS tagging\n",
    "\n",
    "##### NLTK\n",
    "\n",
    "- POS list at https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    \n",
    "        #generating part of speech tags to tokens\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "        nltk.pos_tag(tokenized_sent)\n",
    "    \n",
    "##### SpaCy\n",
    "- POS list at https://spacy.io/api/annotation\n",
    "\n",
    "        # Load the en_core_web_sm model\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "        # Create a Doc object\n",
    "        doc = nlp(lotf)\n",
    "\n",
    "        # Generate tokens and pos tags\n",
    "        pos = [(token.text, token.pos_) for token in doc]\n",
    "        print(pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "#### Questions to ask before dealing with text data\n",
    "* Do we remove stop words, or not?  \n",
    "<br>  \n",
    "* Do we stem or lemmatize our text data, or leave the words as is?  \n",
    "<br>  \n",
    "* Is basic tokenization enough, or do we need to support special edge cases through the use of regex?  \n",
    "<br>  \n",
    "* Do we use the entire vocabulary, or just limit the model to a subset of the most frequently used words? If so, how many?  \n",
    "<br>  \n",
    "* Do we engineer other features, such as bigrams, or POS tags, or Mutual Information Scores?  \n",
    "<br>  \n",
    "* What sort of vectorization should we use in our model? Boolean Vectorization? Count Vectorization? TF-IDF? More advanced vectorization strategies such as Word2Vec? \n",
    "#### Feature Engineering in Text Classification\n",
    "Experiment and treat the entire project as an iterative process! When working with text data, don't be afraid to try modeling on alterative forms of the text data, such as bigrams or ngrams. Similarly, explore how adding in additional features such as POS tags or mutual information scores affect the overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification for Article Class Prediction\n",
    "    #get packages\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize, FreqDist\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.datasets import fetch_20newsgroups\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    #set classes to predict and remove unnecessary parts of the articles\n",
    "    categories = ['alt.atheism', 'comp.windows.x', 'rec.sport.hockey', 'sci.crypt', 'talk.politics.guns']\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers','quotes'))\n",
    "    newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers','quotes'))\n",
    "    \n",
    "    #set target, labels and relevant data\n",
    "    data = newsgroups_train.data\n",
    "    target = newsgroups_train.target\n",
    "    label_names = newsgroups_train.target_names\n",
    "    \n",
    "    #create stopwords list\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    punc_list = list(string.punctuation)\n",
    "    stopwords_list += punc_list\n",
    "    other_list = [\"''\", '\"\"','...', '`']\n",
    "    stopwords_list += other_list\n",
    "    \n",
    "    #iterate through data to tokenize and remove stopwords\n",
    "    def process_article(article):\n",
    "        tokens = nltk.word_tokenize(article)\n",
    "        stopwords_removed = [token.lower() for token in tokens if token not in stopwords_list]\n",
    "        return stopwords_removed\n",
    "    #create new processed list\n",
    "    processed_data = list(map(process_article, data))\n",
    "    \n",
    "    #counting word totals\n",
    "    total_vocab = set()\n",
    "    for comment in processed_data:\n",
    "        total_vocab.update(comment)\n",
    "    len(total_vocab)\n",
    "    \n",
    "    #creating a frequency distribution for all articles\n",
    "    articles_concat = []\n",
    "    for article in processed_data:\n",
    "        articles_concat += article\n",
    "    articles_freqdist = FreqDist(articles_concat)\n",
    "    articles_freqdist.most_common(200)\n",
    "    \n",
    "    #need to go back and vectorize data\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_data_train = vectorizer.fit_transform(data)\n",
    "    tf_idf_data_test =vectorizer.fit_transform(newsgroups_test.data)\n",
    "    tf_idf_data_train.shape #get shape of data \n",
    "    \n",
    "    #run multinomial naive bayes and random forest classifier models\n",
    "    nb_classifier = MultinomialNB()\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "    nb_classifier.fit(tf_idf_data_train, target)\n",
    "    nb_train_preds = nb_classifier.predict(tf_idf_data_train)\n",
    "    nb_test_preds = nb_classifier.predict(tf_idf_data_test)\n",
    "    rf_classifier.fit(tf_idf_data_train, target)\n",
    "    rf_train_preds = rf_classifier.predict(tf_idf_data_train)\n",
    "    rf_test_preds = rf_classifier.predict(tf_idf_data_test)\n",
    "    nb_train_score = accuracy_score(target, nb_train_preds)\n",
    "    nb_test_score = accuracy_score(newsgroups_test.target, nb_test_preds)\n",
    "    rf_train_score = accuracy_score(target, rf_train_preds)\n",
    "    rf_test_score = accuracy_score(newsgroups_test.target, rf_test_preds)\n",
    "\n",
    "    print(\"Multinomial Naive Bayes\")\n",
    "    print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(nb_train_score, nb_test_score))\n",
    "    print(\"\")\n",
    "    print('-'*70)\n",
    "    print(\"\")\n",
    "    print('Random Forest')\n",
    "    print(\"Training Accuracy: {:.4} \\t\\t Testing Accuracy: {:.4}\".format(rf_train_score, rf_test_score))\n",
    "    \n",
    "_Needs further tuning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep NLP\n",
    "\n",
    "### Word Embeddings\n",
    "\n",
    "**_Word Embeddings_** are a type of vectorization strategy that computes word vectors from a text corpus by training a neural network, which results in a high-dimensional embedding space, where each word is in the corpus is a unique vector in that space. In this embedding space, the position of the vector relative to the other vectors captures semantic meaning. Embeddings, since vectorize, take up smaller space in order to process faster and more accurately.\n",
    "\n",
    "#### Word Embeddings with SpaCy\n",
    "\n",
    "    # Create Doc objects\n",
    "    mother_doc = nlp(mother)\n",
    "    hopes_doc = nlp(hopes)\n",
    "    hey_doc = nlp(hey)\n",
    "\n",
    "    # Print similarity between mother and hopes\n",
    "    print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "    # Print similarity between mother and hey\n",
    "    print(mother_doc.similarity(hey_doc))\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "At its core, Word2Vec is just another Deep Neural Network. It's not even a particularly complex neural network--The model contains an input layer, a single hidden layer, and and an output layer that uses the softmax activation function, meaning that the model is meant for multiclass classification. The model examines a window of words, which is a tunable parameter that we can set when working with the model. \n",
    "\n",
    "Instead of predicting the next word given a context, the model trains to predict the context surrounding a given word!  It turns out that by training to predict the context window for a given word, the neurons in the hidden layer end up learning the embedding space! This is the reason why the size of the word vectors output by a Word2Vec model are a parameter that we can set ourselves. \n",
    "\n",
    "If we want word vectors of size 300, then we just include 300 neurons in our hidden layer. If we want vectors of size 100, then we include 100 neurons, and so on. If there are 10,000 words and we want vectors of size 300, then this means the hidden layer will be of shape  [10000, 300]. To put it another way--each of the 10,000 words will have it's own unique vector of weights, which will be of size 300, since there are 300 neurons. \n",
    "\n",
    "Once we've trained the model, we don't actually need the output layer anymore--all that matters is the hidden layer, which will now act as a \"Lookup Table\" that allows us to quickly get the vector for any given word in the vocabulary.\n",
    "\n",
    "In the case of the Word2Vec model, the \"company\" a word keeps is quite literally the words in the context window around a given word, which the model is learning to predict! The more similar words are, the more sentences in which they are likely to share context windows! This is exactly what the model is learning, and this is why words that are similar end up near each other inside the embedding space. The ways that they are not similar also help the model learn to differentiate between them, since there will be patterns here as well. \n",
    "\n",
    "#### Training A Word2Vec Model\n",
    "\n",
    "To train a Word2Vec model, we first need to import the model from the `gensim` library and instantiate it. Upon instantiation, we'll need to provide the model with certain parameters such as:\n",
    "* the dataset we'll be training on\n",
    "* the `size` of the word vectors we want to learn \n",
    "* the `window` size to use when training the model\n",
    "* `min_count`, which corresponds to the minimum number of times a word must be used in the corpus in order to be included in the training (for instance, `min_count=5` would only learn word embeddings for words that appear 5 or more times throughout the entire training set)\n",
    "* `workers`, the number of threads to use for training, which can speed up processing (`4` is typically used, since most processors nowadays have at least 4 cores). \n",
    "\n",
    "Once we've instantiated the model, we'll still need to call the model's `.train()` function, and pass in the following parameters:\n",
    "\n",
    "* The same dataset that we passed in at instantation\n",
    "* The `total_examples`, which is the number of words in the model. You don't need to calculate this manually--instead, you can just pass in the instantiated model's `.corpus_count` attribute for this parameter.\n",
    "* The number of `epochs` to train the model for. \n",
    "\n",
    "The following example demonstrates how to instantiate and train a Word2Vec model:\n",
    "\n",
    "\n",
    "    from gensim.models import Word2Vec\n",
    "\n",
    "    #Let's assume we have our text corpus already tokenized and stored inside the variable 'data'--the regular text preprocessing steps still need to be handled before training a Word2Vec model!\n",
    "\n",
    "    model = Word2Vec(data, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    model.train(data, total_examples=model.corpus_count)\n",
    "\n",
    "#### Exploring the Embedding Space\n",
    "\n",
    "Once we have trained the model, we can easily explore the embedding space using the built-in methods and functionality provided by gensim's `Word2Vec` class. \n",
    "\n",
    "The actual Word2Vec model itself is quite large. Normally, we only need the actual vectors and the words that correspond to them, which are stored inside of `model.wv` as a `Word2VecKeyedVectors` object. To save time and space, it's usually easiest to just store the `model.wv` inside it's own variable, and then work directly with that. We can then use this model for various sorts of functionality, which we'll demonstrate below!\n",
    "\n",
    "   \n",
    "    wv = model.wv\n",
    "\n",
    "    #Get the most similar words to a given word\n",
    "    wv.most_similar('Cat')\n",
    "\n",
    "    #Get the least similar words to a given word. NOTE: We'll see in \n",
    "    #the next lab that this function doesn't always work the way we   \n",
    "    #might intuitively think it will!\n",
    "    wv.most_similar(negative='Cat')\n",
    "\n",
    "    #Get the word vector for a given word\n",
    "    wv['Cat']\n",
    "\n",
    "    #Get all the vectors for all the words!\n",
    "    wv.vectors\n",
    "\n",
    "    #Compute some word vector arithmetic, such as (king - man + woman) \n",
    "    #which should be roughly equal to 'queen'\n",
    "    wv.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "    \n",
    "### Text Classification\n",
    "\n",
    "The most common model to use for embedded word vectors is the **_GloVe_** (short for **_Global Vectors for Word Representation_**) model by the Stanford NLP Group.\n",
    "\n",
    "#### GloVe file\n",
    "\n",
    "For text classification purposes, loading the weights precludes the need for us to instantiate or train a Word2Vec model entirely--instead, we just need to:\n",
    "\n",
    "* Get the total vocabulary in our dataset\n",
    "* Download and unzip the GloVe file needed from the Stanford NLP Group's website\n",
    "* Read the GloVe file, and save only the vectors that correspond to the words that appear in the vocabulary of our dataset.\n",
    "\n",
    "#### Mean Word Embedding\n",
    "\n",
    "To get the vector representation for any arbitrarily-sized block of text, all we need to do is get the vector for every individual word that appears in that block of text, and average them together! The benefit of this is that no matter how big or small that block of text is, the **_Mean Word Embedding_** of that sentence will be the same size as all of the others, because the vectors we're averaging together all have the exact same dimensionality! This makes it a simple matter to get a block of text into a format that we can use with traditional Supervised Learning models such as Support Vector Machines or Gradient Boosted Trees.\n",
    "\n",
    "**_Best to perform Classification using a Pipeline so save on work_**\n",
    "\n",
    "#### Embedding Layer\n",
    "\n",
    "An **_Embedding Layer_** is just a layer that learns the word embeddings for our dataset on the fly, right there inside the Neural Network. Essentially, its a way to make use of all the benefits of Word2Vec, without worrying about finding a way to include a separately trained Word2Vec model's output into our Neural Networks\n",
    "\n",
    "You should make note of a couple caveats that come with using embedding layers in your Neural Network--namely:\n",
    "\n",
    "* The Embedding Layer must always be the first layer of the network, meaning that it should immediately follow the `Input()` layer\n",
    "* All words in the text should be integer-encoded, with each unique word encoded as it's own unique integer. \n",
    "* The size of the Embedding Layer must always be greater than the total vocabulary size of the dataset! The first parameter denotes the vocabulary size, while the second denotes the size of the actual word vectors\n",
    "* The size of the sequences passed in as data must be set when creating the layer (all data will be converted to padded sequences of the same size during the preprocessing step). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Fake News\n",
    "\n",
    "    # Import the necessary modules\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Print the head of df\n",
    "    print(df.head())\n",
    "\n",
    "    # Create a series to store the labels: y\n",
    "    y = df.label\n",
    "\n",
    "    # Create training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "    # Initialize a CountVectorizer object: count_vectorizer\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    # Transform the training data using only the 'text' column values: count_train \n",
    "    count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Transform the test data using only the 'text' column values: count_test \n",
    "    count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "    # Print the first 10 features of the count_vectorizer\n",
    "    print(count_vectorizer.get_feature_names()[:10])\n",
    "    \n",
    "    # Import TfidfVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    # Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "    # Transform the training data: tfidf_train \n",
    "    tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Transform the test data: tfidf_test \n",
    "    tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # Print the first 10 features\n",
    "    print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "    # Print the first 5 vectors of the tfidf training data\n",
    "    print(tfidf_train.A[:5])\n",
    "    \n",
    "    # Create the CountVectorizer DataFrame: count_df\n",
    "    count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "    # Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "    tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "    # Print the head of count_df\n",
    "    print(count_df.head())\n",
    "\n",
    "    # Print the head of tfidf_df\n",
    "    print(tfidf_df.head())\n",
    "\n",
    "    # Calculate the difference in columns: difference\n",
    "    difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "    print(difference)\n",
    "\n",
    "    # Check whether the DataFrames are equal\n",
    "    print(count_df.equals(tfidf_df))\n",
    "    \n",
    "    # Import the necessary modules\n",
    "    from sklearn import metrics\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "    # Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB()\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "    # Create the predicted tags: pred\n",
    "    pred = nb_classifier.predict(count_test)\n",
    "\n",
    "    # Calculate the accuracy score: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(score)\n",
    "\n",
    "    # Calculate the confusion matrix: cm\n",
    "    cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "    print(cm)\n",
    "    \n",
    "    # Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB()\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "    # Create the predicted tags: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "    # Calculate the accuracy score: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(score)\n",
    "\n",
    "    # Calculate the confusion matrix: cm\n",
    "    cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "    print(cm)\n",
    "    \n",
    "    # Create the list of alphas: alphas\n",
    "    alphas = np.arange(0,1,0.1)\n",
    "\n",
    "    # Define train_and_predict()\n",
    "    def train_and_predict(alpha):\n",
    "        # Instantiate the classifier: nb_classifier\n",
    "        nb_classifier = MultinomialNB(alpha=alpha)\n",
    "        # Fit to the training data\n",
    "        nb_classifier.fit(tfidf_train, y_train)\n",
    "        # Predict the labels: pred\n",
    "        pred = nb_classifier.predict(tfidf_test)\n",
    "        # Compute accuracy: score\n",
    "        score = metrics.accuracy_score(y_test, pred)\n",
    "        return score\n",
    "\n",
    "    # Iterate over the alphas and print the corresponding score\n",
    "    for alpha in alphas:\n",
    "        print('Alpha: ', alpha)\n",
    "        print('Score: ', train_and_predict(alpha))\n",
    "        print()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Recommender Function Based on Plots Using NLP \n",
    "\n",
    "- takes a movie title, cosine similarity matrix and indices series as arguments\n",
    "- extract pairwise cosine similarity scores for the movie\n",
    "- sorts scores in descending order\n",
    "- outputs titles corresponding to the highest scores\n",
    "\n",
    "       # Generate mapping between titles and index\n",
    "        indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "        def get_recommendations(title, cosine_sim, indices):\n",
    "            # Get index of movie that matches title\n",
    "            idx = indices[title]\n",
    "            # Sort the movies based on the similarity scores\n",
    "            sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "            # Get the scores for 10 most similar movies\n",
    "            sim_scores = sim_scores[1:11]\n",
    "            # Get the movie indices\n",
    "            movie_indices = [i[0] for i in sim_scores]\n",
    "            # Return the top 10 most similar movies\n",
    "            return metadata['title'].iloc[movie_indices]\n",
    "       \n",
    "       \n",
    "       # Initialize the TfidfVectorizer \n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "        # Initialize the TfidfVectorizer \n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "        # Construct the TF-IDF matrix\n",
    "        tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "        # Generate the cosine similarity matrix\n",
    "        cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "        # Generate recommendations \n",
    "        print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "476.716px",
    "left": "37px",
    "top": "53.454px",
    "width": "291.67px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
