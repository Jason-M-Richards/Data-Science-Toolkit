{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "Collaborative filtering is a method of making automatic predictions (i.e. filtering) about the interests of a user by collecting preferences or taste information from many users on the aggregate (i.e. collaborating). There are two main approaches to Collaborative Filter that we will learn about. The basic idea behind collaborative filtering model is:\n",
    "\n",
    "- Predict a numerical value expressing the predicted score of an item for a user. The predicted value should be  within the same scale that is used by all users for rating (i.e. number of stars or rating between 0-5)\n",
    "\n",
    "- Recommend a list of Top-N items that the active user will like the most based on the highest predicted ratings for the items that they have not yet seen\n",
    "\n",
    "This can be done with two different methods:\n",
    "\n",
    "* Memory-Based also known as Neighborhood-Based\n",
    "* Model-Based approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Based / Neighborhood-Based Collaborative Filtering\n",
    "\n",
    "Remember that the key idea behind Collaborative Filtering is that similar users share similar interests and that users tend to like items that are similar to one another. With neighborhood-based collaborative filtering methods, you're attempting to quantifying just how similar users and items are to one another and getting the top N recommendations based on that similarity metric.\n",
    "\n",
    "Let's look at the explicit utility matrix we saw in a previous lesson. Below the first utility matrix, we'll also have at a version of the matrix with _implicit_ data, which assumes that we do not have a rating for each movie, we only know whether or not someone has watched the movie. \n",
    "\n",
    "*Explicit Ratings*:\n",
    "\n",
    "|        | Toy Story | Cinderella | Little Mermaid | Lion King |\n",
    "|--------|-----------|------------|----------------|-----------|\n",
    "| Matt   |           | 2          |                | 5         |\n",
    "| Lore   | 2         |            | 4              |           |\n",
    "| Mike   |           | 5          | 3              | 2         |\n",
    "| Forest | 5         |            | 1              |           |\n",
    "| Taylor | 1         | 5          |                | 2         |\n",
    "\n",
    "*Implicit Ratings*\n",
    "\n",
    "|        | Toy Story | Cinderella | Little Mermaid | Lion King |\n",
    "|--------|-----------|------------|----------------|-----------|\n",
    "| Matt   |           | 1          |                | 1         |\n",
    "| Lore   | 1         |            | 1              |           |\n",
    "| Mike   |           | 1          | 1              | 1         |\n",
    "| Forest | 1         |            | 1              |           |\n",
    "| Taylor | 1         | 1          |                | 1         |\n",
    "\n",
    "When dealing with utility matrices, there are two different ways to go about determining similarity within the utility matrix. \n",
    "\n",
    "* Item-based: measure the similarity between the items that target users rates/ interacts with and other items\n",
    "* User-based: measure the similarity between target users and other users\n",
    "\n",
    "Before we dive into the differences between these two methods, let's look at what these similarity metrics are, and how they are related to the final score prediction.\n",
    "\n",
    "### Similarity Metrics:\n",
    "\n",
    "**Pearson Correlation**: Is a commonly used method for computing similarity. It ranges from [-1, 1] and it represents the linear correlation between two vectors. A correlation value of 0 represents no relationship, -1 represents high negative correlation and +1 represents high positive correlation. This similarity metric only takes into account those items that are rated by both individuals. The pearson correlation is great because it takes into account\n",
    "\n",
    "### $$ \\text{pearson correlation}(u,v) = \\frac{\\sum_{i \\in I_{uv}}{(r_{ui}- \\mu_{u})*(r_{vi}- \\mu_{v})}}{\\sqrt{\\sum_{i \\in I_{uv} }{(r_{ui}-\\mu_{u})^{2}  }}  * \\sqrt{\\sum_{i \\in I_{uv} }{(r_{vi}-\\mu_{v})^{2}  }}} $$\n",
    "\n",
    "\n",
    "**Cosine Similarity**: Determines how vectors are related to each other by measuring the cosine angle between two vectors. The value also ranges from [-1,1], with -1 meaning that the two vectors are diametrically opposed, 0 meaning the two vectors are perpendicular to one another, and 1 meaning that the vectors are the same. Here is the formula in the context of user similarity:\n",
    "\n",
    "### $$ \\text{cosine similarity}(u,v) = \\frac{\\sum_{i \\in I_{uv}}{r_{ui}*r_{vi}}}{\\sqrt{\\sum_{i \\in I_{uv} }{r_{ui}^{2}  }}  * \\sqrt{\\sum_{i \\in I_{uv} }{r_{ui}^{2}  }}} $$\n",
    "\n",
    "where u is a user and v is another user being compared to u. i represents each item being rated. I is the entire item set.\n",
    "\n",
    "**Jaccard Similarity**: Uses the number of preferences in common between two users into account. Importantly, it does not take the actual values of the ratings into account, only whether or not users have rated the same items. In other words, all explicit ratings are effectively turned into values of 1 when using the Jaccard Similarity metric.\n",
    "\n",
    "\n",
    "### $$ \\text{Jaccard Similarity}(u,v) = \\frac{I_{u} \\cup I_{v}}{I_{u} \\cap I_{v}}$$\n",
    "\n",
    "### Calculating a Predicted Rating\n",
    "\n",
    "Once these similarities have been calculated, the ratings are calculated essentially as a weighted average of the k most similar neighbors. For example, if trying to   is that the values of the individual ratings can be calculated as \n",
    "$$ r_{ij} = \\frac{\\sum_{k}{Similarities(u_i,u_k)r_{kj}}}{\\text{number of ratings}} $$\n",
    "\n",
    "\n",
    "\n",
    "#### Item-item filtering  \n",
    "When someone looks at the similarity of one vector of an items ratings from every user and compares it to every other item. Now, the most similar items can be recommended to those that a customer has liked. This is similar to content-based recommendation, except we are not looking at any actual characteristics of items. We are merely looking at who has liked an item and compared it to who has liked other items. Let's look at this in a table with the similarity metric as Jaccard Index. To start off with, let's compare Toy Story and Cinderella. The union of everyone that has liked both movies is 5 and the intersection of the two movies is 1 (we can see that Taylor liked both Toy Story and Cinderella. The rest of the similarities have been filled in.\n",
    "\n",
    "\n",
    "\n",
    "    |                | Toy Story | Cinderella | Little Mermaid | Lion King |\n",
    "    |----------------|-----------|------------|----------------|-----------|\n",
    "    | Toy Story      |           | 1 /  5     |    2 / 4       |   1/5     |\n",
    "    | Cinderella     | 1/5       |            |   1/5          |    1      |\n",
    "    | Little Mermaid | 2/4       |   1/5      |                |  1/5      |\n",
    "    | Lion King      | 1/5       |     1      |  1/5           |           |\n",
    "\n",
    "#### User-User filtering.\n",
    "The other method of collaborative filtering is to see similar customers are to one another. Once we've determined how similar customers are to one another, we can recommend items to them that are liked by the other customers that are most similar to them. Similar to above, here is a similarity table for each of the users, made by taking their jaccard similarity to one another. The process of calculating the Jaccard index is the same when comparing the users except now we are comparing how each user voted compared to one another.\n",
    "\n",
    "\n",
    "\n",
    "    |        | Matt | Lore | Mike | Forest | Taylor |\n",
    "    |--------|------|------|------|--------|--------|\n",
    "    | Matt   |      |  0   |  2/3 |  0     |   2/3  |\n",
    "    | Lore   |  0   |      | 2/4  |  2/2   |   2/4  |\n",
    "    | Mike   |  2/3 | 1/4  |      |  1/4   |   2/4  |\n",
    "    | Forest |  0   | 2/2  | 1/4  |        |   1/4  |\n",
    "    | Taylor |  2/3 | 2/4  | 2/4  |  1/4   |        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Based Collaborative Filtering\n",
    "\n",
    "Matrix Factorization models are based on the concept of the __Latent Variable Model__. \n",
    "\n",
    "### Latent Variable Model \n",
    "\n",
    "Latent variable models try to explain complex relationships between several variables by way of simple relationships between variables and underlying \"latent\" variables. If this sounds extremely similar to the ideas we established in Dimensionality Reduction and PCA, it's because it is very similar...it's just that the exact implementation is a bit different.\n",
    "\n",
    "\n",
    "With latent variable models, we have some number of observable variables (the features from our dataset) and a collection of unobservable latent variables. These latent variables should capable of explaining the relationships of the  to one another such that the observable variables are conditionally independent given the latent variables. \n",
    "\n",
    "\n",
    "The Matrix Factorization approach is found to be most accurate approach to reduce the problem from high levels of  sparsity in RS database as all users do not buy all products and services and our utility matrix remains highly sparse. If people had already rated every item, it would be unnecessary to recommend them anything! In the model-based recommendations,  techniques like __Latent Semantic Index (LSI)__,  and the dimensionality reduction method __Singular Value Decomposition (SVD)__ are typically combined to get rid of sparcity. Below is an example of sparse matrix , which can lead to the problems highlighted earlier in the PCA section. \n",
    "\n",
    "Let's look at how a recommendation problem can be translated into matrix decomposition context. The idea behind such models is that preferences of a users can be determined by a small number of hidden factors. We can call these factors as Embeddings.\n",
    "\n",
    "### Embeddings:\n",
    "Embeddings are __low dimensional hidden factors__ for items and users. \n",
    "\n",
    "For e.g. say we have 5 dimensional (i.e. D or n_factors = 5 in above figure) embeddings for both items and users (5 chosen randomly, this could be any number - as we saw with PCA and dim. reduction). \n",
    "\n",
    "For user-X & movie-A, we can say the those 5 numbers might represent 5 different characteristics about the movie e.g.:\n",
    "\n",
    "- How much movie-A is political\n",
    "- How recent is the movie \n",
    "- How much special effects are in movie A \n",
    "- How dialogue driven is the movie \n",
    "- How linear is the narrative in the movie\n",
    "\n",
    "In a similar way, 5 numbers in user embedding matrix might represent:\n",
    "- How much does user-X like sci-fi movie \n",
    "- How much does user-X like recent movies … and so on. You get the idea.\n",
    "\n",
    "In the above figure, a higher number from dot product of user-X and movie-A matrix means that movie-A is a good recommendation for user-X.\n",
    "\n",
    "Now let's look at one of the ways, one can factor the matrix. One of these ways we can perform matrix factorization is called Singular Value Decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Reccomendation System Using Collaborative Filtering\n",
    "    # Load the Dataset¶\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('books_data.edgelist', names=['source', 'target', 'weight'], delimiter=' ')\n",
    "    df.head()\n",
    "    #import networkx as nx\n",
    "    G = nx.Graph()\n",
    "    #Load the MetaData\n",
    "    #Next, load the metadata associated with each of the books being reviewed. The metadata is stored in the file 'books_meta.txt'.\n",
    "    meta = pd.read_csv('books_meta.txt', sep='\\t')\n",
    "    meta.head()\n",
    "    #Select a small subset of books that you are interested in generating recommendations for.\n",
    "    GOT = meta[meta.Title.str.contains('Thrones')]\n",
    "    GOT\n",
    "    #Generate Recommendations for a Few Books of Choice\n",
    "    #employ collaborative filtering to generate recommendations!\n",
    "    rec_dict = {}\n",
    "    id_name_dict = dict(zip(meta.ASIN, meta.Title))\n",
    "    for row in GOT.index:\n",
    "        book_id = GOT.ASIN[row]\n",
    "        book_name = id_name_dict[book_id]\n",
    "        most_similar = df[(df.source==book_id)\n",
    "                          | (df.target==book_id)\n",
    "                         ].sort_values(by='weight', ascending=False).head(10)\n",
    "        most_similar['source_name'] = most_similar['source'].map(id_name_dict)\n",
    "        most_similar['target_name'] = most_similar['target'].map(id_name_dict)\n",
    "        recommendations = []\n",
    "        for row in most_similar.index:\n",
    "            if most_similar.source[row] == book_id:\n",
    "                recommendations.append((most_similar.target_name[row], most_similar.weight[row]))\n",
    "            else:\n",
    "                recommendations.append((most_similar.source_name[row], most_similar.weight[row]))\n",
    "        rec_dict[book_name] = recommendations\n",
    "        print(\"Recommendations for:\", book_name)\n",
    "        for r in recommendations:\n",
    "            print(r)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD) and Recommendations\n",
    "\n",
    "With SVD, we turn the recommendation problem into an __Optimization__ problem that deals with how good we are in predicting the rating for items given a user. One common metric to achieve such optimization is __Root Mean Square Error (RMSE)__. A lower RMSE is indicative of improved performance performance and vice versa. RMSE is minimized on the known entries in the utility matrix. SVD has a great property that it has the minimal reconstruction Sum of Square Error (SSE); therefore, it is also commonly used in dimensionality reduction. Below is the formula to achieve this:\n",
    "\n",
    "$$min_{UV\\Sigma}\\sum_{i,j∈A}(A_{ij} - [UV\\Sigma^T]_{ij})^2$$\n",
    "\n",
    "\n",
    "RMSE and SSE are monotonically related. This means that the lower the SSE, the lower the RMSE. With the convenient property of SVD that it minimizes SSE, we know that it also minimizes RMSE. Thus, SVD is a great tool for this optimization problem. To predict the unseen item for a user, we simply multiply U, V, and $\\Sigma^{T}$.\n",
    "\n",
    "### SVD in Python¶\n",
    "Scipy has a straightforward implementation of SVD to help us avoid all the complex steps of SVD. We can use svds() function to decompose a matrix as shown below. We ill use csc_matrix() to create a sparse matrix object.\n",
    "\n",
    "    from scipy.sparse import csc_matrix\n",
    "    from scipy.sparse.linalg import svds\n",
    "\n",
    "    # Create a sparse matrix \n",
    "    A = csc_matrix([[1, 0, 0], [5, 0, 2], [0, 1, 0], [0, 0, 3],[4,0,9]], dtype=float)\n",
    "\n",
    "    # Apply SVD\n",
    "    u, s, vt = svds(A, k=2) # k is the number of stretching factors\n",
    "\n",
    "    print ('A:\\n', A.toarray())\n",
    "    print ('=')\n",
    "    print ('\\nU:\\n', u)\n",
    "    print ('\\nΣ:\\n', s)\n",
    "    print ('\\nV.T:\\n', vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprise\n",
    "Surprise is a Python library that creates recommendation engines.**Surprise can make predictions on ratings, but does not recommend items to users. See below for recommendation code.**\n",
    "https://surprise.readthedocs.io/en/stable/index.html\n",
    "import surprise\n",
    "\n",
    "#### Converting df to Surprise ready format\n",
    "\n",
    "    from surprise import Reader, Dataset\n",
    "    reader = Reader()\n",
    "    data = Dataset.load_from_df(new_df,reader)\n",
    "\n",
    "#### Train-Test\n",
    "\n",
    "    #split into train and test set.\n",
    "    trainset, testset = train_test_split(jokes,test_size=0.2)\n",
    "    \n",
    "Notice how there is no X_train or y_train in our values here. Our only features here are the ratings of other users and items, so we need to keep everything together. What is happening in the train test split here is that surprise is randomly selecting certain $r_{ij}$ for users $u_{i}$ and items $i_{j} $ at the rate of 80% of the ratings in the train set and 20% in the test set. Let's investigate `trainset` and `testset` further.\n",
    "\n",
    "    print('Type trainset :',type(trainset),'\\n') #\n",
    "    print('Type testset :',type(testset)) #identified as a python list\n",
    "    \n",
    "### Memory-Based Methods (Neighborhood-Based)\n",
    "    #import packages\n",
    "    from surprise.prediction_algorithms import knns\n",
    "    from surprise.similarities import cosine, msd, pearson\n",
    "    from surprise import accuracy\n",
    "    \n",
    "    #retrieve number of users and items to see which would be more feasible to model (item/item or user/user)\n",
    "    print('Number of users: ',trainset.n_users,'\\n')\n",
    "    print('Number of items: ',trainset.n_items,'\\n')\n",
    "    \n",
    "##### cosine similarity model with KNN basic\n",
    "    sim_cos = {'name':'cosine','user_based':False} #selects item based data only\n",
    "    #instantiate knn and fit\n",
    "    basic = knns.KNNBasic(sim_options=sim_cos)\n",
    "    basic.fit(trainset)\n",
    "    basic.sim  #generate an array that shows similarity data\n",
    "    #get accuracy scores\n",
    "    predictions = basic.test(testset) #generates an RMSE score rounded to four decimal places\n",
    "    print(accuracy.rmse(predictions)) #generates RMSE without being rounded off\n",
    "    \n",
    "##### pearson similarity model with KNN basic\n",
    "    sim_pearson = {'name':'pearson','user_based':False}\n",
    "    basic_pearson = knns.KNNBasic(sim_options=sim_pearson)\n",
    "    basic_pearson.fit(trainset)\n",
    "    predictions = basic_pearson.test(testset)\n",
    "    print(accuracy.rmse(predictions))\n",
    "    \n",
    "##### pearson similarity model with KNN mean\n",
    "    sim_pearson = {'name':'pearson','user_based':False}\n",
    "    knn_means = knns.KNNWithMeans(sim_options=sim_pearson)\n",
    "    knn_means.fit(trainset)\n",
    "    predictions = knn_means.test(testset)\n",
    "    print(accuracy.rmse(predictions))\n",
    "  \n",
    "##### pearson similarity model with KNN baseline\n",
    "    \n",
    "    sim_pearson = {'name':'pearson','user_based':False}\n",
    "    knn_baseline = knns.KNNBaseline(sim_options=sim_pearson)\n",
    "    knn_baseline.fit(trainset)\n",
    "    predictions = knn_baseline.test(testset)\n",
    "    print(accuracy.rmse(predictions))\n",
    "    \n",
    "### Model Based methods (Matrix Factorization)\n",
    "\n",
    "    #import packages\n",
    "    from surprise.prediction_algorithms import SVD\n",
    "    from surprise.model_selection import GridSearchCV\n",
    "    #perform Gridsearch for optimal parameters\n",
    "    param_grid = {'n_factors':[20,100],'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
    "                   'reg_all': [0.4, 0.6]}\n",
    "    gs_model = GridSearchCV(SVD,param_grid=param_grid,n_jobs = -1,joblib_verbose=5)\n",
    "    gs_model.fit(jokes)\n",
    "    \n",
    "    svd = SVD(n_factors=100,n_epochs=10,lr_all=0.005,reg_all=0.4)\n",
    "    svd.fit(trainset)\n",
    "    predictions = svd.test(testset)\n",
    "    print(accuracy.rmse(predictions))\n",
    "    \n",
    "### Rating Prediction\n",
    "\n",
    "    user_34_prediction = svd.predict('34','25') #prediction for user 35, item 24\n",
    "    user_34_prediction #produces a tuple with estimation included \n",
    "    \n",
    "### Obtaining Ratings from Specific Users\n",
    "\n",
    "    def movie_rater(movie_df,num, genre=None):\n",
    "        userID = 1000 #specific user id\n",
    "        rating_list = []\n",
    "        while num > 0:\n",
    "            if genre:\n",
    "                movie = movie_df[movie_df['genres'].str.contains(genre)].sample(1)\n",
    "            else:\n",
    "                movie = movie_df.sample(1)\n",
    "            print(movie)\n",
    "            rating = input('How do you rate this movie on a scale of 1-5, press n if you have not seen :\\n')\n",
    "            if rating == 'n':\n",
    "                continue\n",
    "            else:\n",
    "                rating_one_movie = {'userId':userID,'movieId':movie['movieId'].values[0],'rating':rating}\n",
    "                rating_list.append(rating_one_movie) \n",
    "                num -= 1\n",
    "        return rating_list\n",
    "        \n",
    "    user_rating = movie_rater(df_movies,4,'Comedy')\n",
    "    \n",
    "### Making Predictions with New Ratings\n",
    "\n",
    "    #add the new ratings to the original ratings DataFrame\n",
    "    new_ratings_df = new_df.append(user_rating,ignore_index=True)\n",
    "    new_data = Dataset.load_from_df(new_ratings_df,reader)\n",
    "    \n",
    "    # train a model using the new combined DataFrame\n",
    "    svd_ = SVD(n_factors= 50, reg_all=0.05)\n",
    "    svd_.fit(new_data.build_full_trainset())\n",
    "    \n",
    "    # make predictions for the user\n",
    "    list_of_movies = []\n",
    "    for m_id in new_df['movieId'].unique():\n",
    "        list_of_movies.append( (m_id,svd_.predict(1000,m_id)[3]))\n",
    "        \n",
    "    # order the predictions from highest to lowest rated\n",
    "    ranked_movies = sorted(list_of_movies,key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    # return the top n recommendations using the \n",
    "    def recommended_movies(user_ratings,movie_title_df,n):\n",
    "            for idx, rec in enumerate(user_ratings):\n",
    "                title = movie_title_df.loc[movie_title_df['movieId'] == int(rec[0])]['title']\n",
    "                print('Recommendation # ',idx+1,': ',title,'\\n')\n",
    "                n-= 1\n",
    "                if n == 0:\n",
    "                    break\n",
    "\n",
    "    recommended_movies(ranked_movies,df_movies,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
